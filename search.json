[
  {
    "objectID": "content/summarizing_data.html",
    "href": "content/summarizing_data.html",
    "title": "Summarizing data",
    "section": "",
    "text": "Figure 1: XKCD: Data Trap. It’s important to make sure your analysis destroys as much information as it produces.\nOnce we have some data, the next step is often to summarize it. In fact, we’ve already done that in some ways. Some statistics like the mean may be considered a summary of the data. This may be useful because we prefer large datasets (remember good sampling!), but making sense of a list of numbers can be really hard! Summaries help us describe, and eventually compare, datasets, which we are using to infer something about a population.\nThink about it this way. We want to know if several species of iris (Iris versicolor, setosa and virginica) have similarly-shaped flowers. Since we can’t measure every flower on every plant from these species, we sample several sites and come up with the following data (using R’s built-in iris dataset, a dataset we will often use).\niris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\nOverwhelming, isn’t it? And this isn’t a huge dataset! There are only 150 rows, yet some datasets have tens of thousands!\nLet’s just look at the first few rows of the data\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\nIt’s really hard (or impossible) to just look at these numbers and infer anything about the population. Summary statistics help us get a better mental image of the distribution of the sample data.\nIt’s really hard (or impossible) to just look at these numbers and infer anything about the population. Summary statistics help us get a better mental image of the distribution of the sample data."
  },
  {
    "objectID": "content/summarizing_data.html#types-of-data",
    "href": "content/summarizing_data.html#types-of-data",
    "title": "Summarizing data",
    "section": "Types of data",
    "text": "Types of data\nWe can summarize data using visual (i.e., graphs) or numerical (e.g., summary statistics like the mean) approaches. The specific way we summarize the data also depends on the type of data. Note, the trait we are collecting data on may also be called a variable (since it varies across the population and thus sample).\n\nCategorical variables\nVariables can be categorical (e.g., eye color). If categorical variables have no clear hierarchical relationship (again, like eye color - one isn’t better than the other), then they are nominal variables. If the categories imply a rank or order (e.g., freshmen, sophomore, junior, senior; egg, larvae, pupae, adult) then they are ordinal variables).\n\n\nNumeric variables\nIf data values are based on numbers instead of categories, they are numeric variables. These can be divided into those are count-based (no fractions) - we call these discrete data- and those that can take on values between whole numbers - like height. We call these continuous variables."
  },
  {
    "objectID": "content/summarizing_data.html#graphical-summaries",
    "href": "content/summarizing_data.html#graphical-summaries",
    "title": "Summarizing data",
    "section": "Graphical summaries",
    "text": "Graphical summaries\nVisual interpretations or displays of your data are an excellent way to let patterns, trends, and distributions easier to see. In this section we’ll go over a number of graphs. Consider this is a resource. I don’t expect you to know how to make each of these on your own immediately. We will actually introduce the software we are using to make these in later sections. Instead, you can return here later when you are actually making a graph for ideas (and code!). For your first read, focus on the images (not the code!)\nWhile the type of graph you should use will depend on the data (and you may have several options!) all graphs should have\n\nDescriptive title\n\nMove beyond Y vs. X. State any patterns you see in the title to help the viewer know what they are looking for! Honest interpretation of data is always paramount, but in producing a graph you will already be making visualization decisions.\n\nLabeled axes (measure and unit)\n\nWhat did you measure, and using what (e.g. Sepal length (cm)\n\nData points\n\nOther parts should only be included when needed.\n\nLegends\n\nOnly needed for graphs with multiple datasets where color, shape, or some other visual cue indicates something to the viewer.\n\nTrendlines\n\nCan be used to show the general/overall relationship between variables. If you use these, make sure to use the right ones! Don’t fit a straight line to a curved relationship!\n\n\n\nSingle variable\n\nNumerical data\n\nHistograms\nOccasionally you only want to show the distribution for a single numerical variable (or how the data themselves are distributed). For example, we could want to display sepal lengths for all the Iris virginica we sampled. We could do this using a histogram.\n\nlabel_size &lt;- 2\ntitle_size &lt;-2.5\nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"], \n     main = expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))), \n     xlab = \"Sepal Length (cm)\", \n     cex.lab=label_size, cex.axis=label_size, cex.main=title_size, \n     cex.sub=label_size, col = \"blue\")\n\n\n\n\nFigure 4: Example of approximately normal data\n\n\n\n\nThe above plot is produced using functions available in all R installs. Many plots now use ggplot2, a package you have to install (don’t worry we’ll get there!). However, since you may come back to this later, I’ll also show how to make each of these graphs using ggplot2.\n\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5: Example of approximately normal data\n\n\n\n\nHistograms put the data in bins (usually automatically set by software, but you can update!) and then show the number of samples that fell into each bin. This allows a quick estimate (look at the y, or vertical, axis) of how many samples were taken. The above images also allows us to begin to consider the bounds/range of the data (~4.5-8 cm), which gives information on the minimum and maximum values. We can also see lengths around 6-7 cm are most common.\n\n\nWhy do these graphs look slightly different? (Click the grey triangle to see the answer\n\nMost programs, including R, have autobreak functions to separate the data into bins. Notice ggplot2 uses a different algorithm to bin the data. That also impacts what you see! Users, however, can override these, so it’s worth noting that differences in bin size can influence what distributions look like.\n\nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],       main = expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),       xlab = \"Sepal Length (cm)\",       cex.lab=label_size, cex.axis=label_size, cex.main=title_size,       cex.sub=label_size, col = \"blue\") \nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],        breaks=3, main = \"Sepal length histogram, 3 breaks\", xlab = \"Sepal Length (cm)\", cex.lab=label_size, cex.axis=label_size, cex.main=title_size, cex.sub=label_size, col = \"blue\")  \nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],        breaks=10, main = \"Sepal length histogram, 10 breaks\", xlab = \"Sepal Length (cm)\", cex.lab=label_size, cex.axis=label_size, cex.main=title_size, cex.sub=label_size, col = \"blue\")\n\n\n\n\nFigure 6: ?(caption)\n\n\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\n\n\nFigure 8: ?(caption)\n\n\n\n\nA similar issue exists for qualitative data in regards to the categories that are combined/used.\n\nThis distribution of this data is approximately normal. We will define normality more later (equations!), but for now note the distribution is roughly symmetric, with tails on either side. Values near the middle of the range are more common, with the chance of getting smaller or larger values declining at an increasing rate…\nComparing the above graph to other distributions may be an easier approach. Consider these graphs.\n\ncardinals &lt;- round(rbeta(10000,70,5),3)\nhist(cardinals, main=\"Weight of Westchester cardinals\", xlab = \"\\n Weight (g)\", ylab = \"Frequency (#)\\n\", col = \"red\", cex.lab=label_size, cex.axis=1.25, cex.main=title_size, cex.sub=label_size)\n\n\n\n\nFigure 9: Example of left-skewed data (plot)\n\n\n\n\n\nggplot(data.frame(cardinals), \n       aes(x=cardinals)) +\n  geom_histogram( fill=\"red\", color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 10: Example of left-skewed data (ggplot2)\n\n\n\n\n\nparrots&lt;- round(c(rnorm(1000,.9,4)),3)\nggplot(data.frame(parrots), \n       aes(x=parrots)) +\n  geom_histogram( fill=\"green\", color=\"black\") +\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 11: Example of normal data\n\n\n\n\n\nblue_jays &lt;- round(rbeta(10000,2,10),3)\nggplot(data.frame(blue_jays), \n       aes(x=blue_jays)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=\"Weight of Westchester blue jays\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 12: Example of right-skewed data\n\n\n\n\nThe cardinal Figure 10 data has a longer left tail and is not symmetric. We call this left- or negatively-skewed data (since it’s going lower on the x-axis). Compare that to the blue jay Figure 12 data; it has a longer right-tail and is positively- or right-skewed. Again, note this is all relative to symmetric data like you see with the parrots Figure 11, which is normally-distributed data.\nAll symmetric data is not normal, however. Look at the data on robin and woodpecker weights.\n\nrochester &lt;- round(c(runif(1000,.1,8)),3)\nggplot(data.frame(rochester), \n       aes(x=rochester)) +\n  geom_histogram( fill=\"pink\", color=\"black\") +\n  labs(title=\"Weight of Rochester robins\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 13: Example of uniform data\n\n\n\n\n\nwoodpeckers &lt;- round(c(rnorm(100,20,4),rnorm(100,40,4)),3)\nggplot(data.frame(woodpeckers), \n       aes(x=woodpeckers)) +\n  geom_histogram( fill=\"orange\", color=\"black\") +\n  labs(title=\"Weight of  Westchester woodpeckers\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 14: Example of bimodal data\n\n\n\n\nBoth these are roughly symmetric but clearly different from normally-distributed data (we will return to the woodpecker data!). The robin data is what we call uniformly distributed. There are really no tails, as it appears you are just as likely to see any number within the bounds as any other. Kurtosis is the statistical term for what proportion of the data points are in the tails. High kurtosis distributions have heavy tails with multiple outliers. The uniform distibution is an example of a low kurtosis distribution (it has no tails!).\nThis figure may also help.\n\n\n\nFigure 15: English: Plot of several symmetric unimodal probability densities with unit variance. From highest to lowest peak: red, kurtosis 3, Laplace (D)ouble exponential distribution; orange, kurtosis 2, hyperbolic (S)ecant distribution; green, kurtosis 1.2, (L)ogistic distribution; black, kurtosis 0, (N)ormal distribution; cyan, kurtosis −0.593762…, raised (C)osine distribution; blue, kurtosis −1, (W)igner semicircle distribution; magenta, kurtosis −1.2, (U)niform distribution.\n\n\nIf we consider the normal distribution (shown in black) to have 0 kurtosis, the uniform (pink) has less, and the double-exponential (red) has more.\nFigure 15\nFinally, the woodpecker data is what we call bimodal. It is symmetric in this case (not always true!), but it has a two clear peaks instead of a single central or skewed high point in the distribution.\nThese distributions helps us think about what we would expect to find in future samples (remember, we assume we have good samples!). To think about future sampling, we can change our y-axis from what we saw (frequency) to a probability density.\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram(aes(y = ..density..),fill=\"blue\", color=\"black\") +\n  geom_density()+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 16: Probability density distribution\n\n\n\n\nThese probability density distributions can be calculated from data (as seen above), but they can also be developed from equations. The benefits of using a distribution derived from an equation is that it is consistent and easy to describe (standardized). This is why many common tests we will learn rely upon the data (or some derivative of it) following a known distribution. For example, many parametric tests will rely upon the data (or means of the data, or errors…we’ll get there) following a normal distribution. We can see our parrot data (which came from a normal distribution!) is very close to a “perfect” normal distribution as define by an equation.\n\nparrots_df &lt;- data.frame(parrots)\ncolors &lt;- c(\"PDF from data\" = \"black\", \"normal curve\" = \"red\", \"Petal Width\" = \"orange\")\nggplot(parrots_df, \n       aes(x=parrots)) +\n  geom_histogram(aes(y = ..density..),fill=\"green\", color=\"black\") +\n  geom_density(aes(color=\"PDF from data\"))+\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Density\",\n       color=\"Source\")+\nstat_function(fun = dnorm, args = list(mean = mean(parrots_df$parrots), sd = sd(parrots_df$parrots)), aes(color=\"normal curve\"))+\n      scale_color_manual(values = colors)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 17: Comparing the distribution of the data to a perfect normal distribution\n\n\n\n\n\n\nBonus question: Why isn’t it perfect? (Click the grey triangle to see the answer!)\n\nThis is an easy example of sampling error!\n\n\n\nBox plots (aka, box and whisker plots)\nAnother way to visualize the distribution of numerical data for a single group is using box-and-whisker plots.\n\nggplot(iris[iris$Species == \"virginica\",],\n            aes(x=Species,y=Sepal.Length)) + geom_boxplot(size = 3) +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"\",\n       y= \"Sepal Length (cm)\")+\n  theme(axis.text.x = element_text(size=0))\n\n\n\n\nFigure 18: Example of approximately normal data\n\n\n\n\nThese plots show the values of the quartiles of the data. In this way they start combining numerical summaries (more to come!) and visual summaries. More to come, but for now imagine you had a 99 data points. If you arrange the data points from smallest to largest, the median of the data would be the middle (50th data point). If you took the bottom half of the data (first data to median), the first quartile would be the middle point (or, in this case, the average of the 25th and 26th data points). Similarly, the third quartile is the middle of the top half of the data set (or, if not one number, average of 75th and 76th data point). Note the median is also the 2nd quartile of the data!\nThe box in the box-and-whisker plot shows the first, second, and third quartiles, also known as the inter-quartile range (IQR). The whiskers extend to the minimum and maximum values of the dataset or, up to values within a set range. In ggplot, whiskers by default can only be as long as 150% of the IQR. This means extreme outliers are shown as individual dots. Typically, the most extreme values (minimum and maximum) plus the first, second, and third quartiles are together called the five number summary.\n\n“Easy” examples of five number summaries\n\nAssume we have data that goes from 1 to 99. The five number summary should be\n\nx &lt;- seq(1:1:99) \nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    25.5    50.0    50.0    74.5    99.0 \n\n\nNote the 1st and 3rd quartiles are averaged!\nSimilarly, consider the numbers 1-5\n\nx &lt;- seq(1:1:5) \nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       2       3       3       4       5 \n\n\n\n\n\n\nCategorical data\nFor categorical data, a bar chart fills a very similar role. Note, however, we don’t bin the data., and there is inherent order for some examples (nominal data). For example, we could examine the colors of our I. virginica. To do this, we’ll need to add some data to our iris data (notice this produces no output…)…\n\nset.seed(19)\ncolors &lt;- c(\"blue\", \"orange\", \"purple\")\niris$Color &lt;- factor(sample(colors, size = nrow(iris),replace = T))\n\nand then summarize it…\n\nlibrary(Rmisc)\n\nLoading required package: lattice\n\n\nLoading required package: plyr\n\nI_viriginica_colors &lt;- summarySE(iris[iris$Species == \"virginica\",], measurevar = \"Sepal.Length\",\n                                 groupvars = \"Color\", na.rm = T)\n\nbefore we graph it.\n\nbarplot(I_viriginica_colors$N, \n        names.arg = I_viriginica_colors$Color, \n        xlab=\"Colors\",\n        ylab=\"Frequency\",\n        cex.lab=label_size, cex.axis=label_size, \n        cex.main=title_size, cex.sub=label_size, \n        main = expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")))\n\n\n\n\nFigure 19: Distribution of flower colors\n\n\n\n\nOr better\n\nbarplot(I_viriginica_colors$N, \n        names.arg = I_viriginica_colors$Color, \n        cex.lab=label_size, cex.axis=label_size, \n        cex.main=title_size, cex.sub=label_size, \n        main = expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n        xlab=\"Colors\",\n        ylab=\"Frequency\",\n        col = colors)\n\n\n\n\nFigure 20: Distribution of flower colors (plot)\n\n\n\n\nUsing ggplot2\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Color,fill=Color)) +\n  geom_bar()+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Colors\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 21: Distribution of flower colors (ggplot2)\n\n\n\n\nNote the legend may be superflous here (but consider accessiblity - should we add another distinguishing feature?):\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Color,fill=Color)) +\n  geom_bar()+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Colors\",\n       y= \"Frequency\")+\n  guides(fill = \"none\")\n\n\n\n\nFigure 22: Let colors match traits if possible, but note everyone can’t see colors and sometimes they are not printed.\n\n\n\n\n\n\n**Barchart issues**\n\nNote all of the bar graphs above share a similar problem. People tend to like bars, but they are actually just using a lot of ink! We could get the same information about sepal lengths focusing on just the “top” of the bar:\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_freqpoly(color=\"blue\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 23: Note you only really know the tops of the bar!\n\n\n\n\nWe can also just display the data!\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Species, y=Sepal.Length)) +\n  geom_point(color=\"blue\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")+\n  theme(axis.text.x = element_text(size=0))\n\n\n\n\nFigure 24: Displaying the data may be the easiest option for small-ish datasets.\n\n\n\n\n\n\n\n\nMultiple variables\nOften we collect multiple pieces of information instead of just one. This can occur for multiple reasons. We may want to consider differences in some variable/trait among groups. This means we have either numerical or categorical data from various groups, but note that groups themselves are now a piece of data! We can think of these analyses as impact of group (a category) on traits (numerical or categorical). We will eventually call these a t-test or ANOVA (when the trait we measure is categorical) ota \\(\\chi^2\\) test (when the trait is categorical). Either way, this is a case where we are collecting a single piece of data from multiple groups. Alternatively, we may collect data on multiple traits from a single group to see how they impact each other. We will eventually analyze this type of data using regression or correlation. Regardless of type, we can also graph this data.\n\nNumerical variables from multiple groups\nWhen we gather numerical data from various groups and wish to compare, we can extend our use bar charts and box-whisker plots by using shapes, colors, or other features to symbolize the groups. For example, we can illustrate the mean (coming up in numerical summaries) or other summary statistics using bar plots..\n\nggplot(iris, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n{#fig-bar_charts_all species width=672}\n\n\nor the distribution using stacked histograms…\n\nggplot(iris, aes(x=Sepal.Length)) +     geom_histogram(aes(fill=Species))+    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),        y= \"Sepal length (cm)\",        x= \"Species\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n{#fig-stacked_histograms_all species width=672}\n\n\nor box-and-whisker plots.\n\nggplot(iris, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_boxplot(aes(fill=Species))+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),        y= \"Sepal length (cm)\", x= \"Species\")\n\n{#fig-box_whisker_all species width=672}\n\n\nWe can also still just display the data for each group…\n\nggplot(iris, aes(y=Sepal.Length, x=Species, color=Species)) +\n  geom_jitter() +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n{#fig-point_all species width=672}\n\n\nWe also need to ensure the different groups are visible when distributions overlap. Sometimes stacked histograms (and similar graphs) make it hard to actually visualize each individual group. One option is to instead facet these graphs. Faceting means we produce different graphs for each group, treatment, etc, but they (typically) share axes. This makes it easier to compare the groups.\n\n ggplot(iris, aes(x=Sepal.Length)) + \n   geom_histogram(aes(fill=Species))+ \n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")+\n   facet_wrap(~Species, ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n{#fig-faceted_histograms_all species width=672}\n\n\nAnother option is to show the cumulative frequency distribution for each group.\n\nggplot(iris, aes(Sepal.Length, colour = Species)) + stat_ecdf()+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Cumulative frequency\")\n\n\n\n\nFigure 25: Cumulative frequency distributions can be useful in noting exactly where distributions diverge\n\n\n\n\n\n\nCategorical data from multiple groups\nFor our example, let’s return to our focus on the color of flowers for various species of iris. One option for this is to consider bar plots. These can be stacked…\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 26: Bar plots are stacked by default and count the number of rows found in each category\n\n\n\n\nor not…\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position = position_dodge(width=0.5))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 27: Bar plots can also be grouped by adding the position_dodge argument\n\n\n\n\nOther options include divergent plots, but those are best for 2 groups of data. They also require the data to be summarized and somewhat transformed. For example, we could have blue or not blue flowers.\n\nlibrary(plyr)\niris$blue &lt;- revalue(iris$Color, c(\"blue\"=\"blue\", \"purple\"=\"not blue\", \"orange\"=\"not blue\"))\n\nThen we have to summarize the data.\n\niris_summary &lt;- data.frame(table(iris$blue, iris$Species))\nnames(iris_summary) &lt;- c(\"Blue\", \"Species\", \"Frequency\")\n\nand make not blue negative\n\niris_summary[iris_summary$Blue == \"not blue\", \"Frequency\"] &lt;- iris_summary[iris_summary$Blue == \"not blue\", \"Frequency\"] * -1\n\nthen plot it.\n\nggplot(iris_summary,aes(x=Species, y=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 28: Divergent plots show how 2 categories differ among groups\n\n\n\n\nwhich we could flip by reversing all x/y arguments..\n\nggplot(iris_summary,aes(y=Species, x=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       y= \"Species\",\n       x= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 29: Reorienting graphs may help viewers better visualize differnces\n\n\n\n\nor using an additional argument (remember, a lot of this is for later reference!)\n\nggplot(iris_summary,aes(x=Species, y=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\")) +\n  coord_flip()\n\n\n\n\nFigure 30: Note we get the same results by simply adding the argument coord_flip\n\n\n\n\n\n\ngeom_bar vs geom_col\n\ngeom_bar and geom_col are very similar commands, but geom_bar assumes its needs to do something to the data (like count it) by default, whereas geom_col assumes the data are summarized/ready to plot as is. The extra argument stat=identity above can usually make geom_bar behave like geom_col.\n\nIn the above cases, each group was measured the same number of times. However, if this isn’t true, visualizations may confound sampling size with summaries. In those cases, focusing on proportion of outcomes may be more useful (and will give you the exact same visualization if all groups were measured the same number of times!). This is sometimes called a mosaic plot; another way to make them (not shown here) is using the package ggmosaic.\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position = \"fill\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Proportion\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 31: For proportion-based visualizations, stacked bar plots may be easier to read than grouped. We just add the position=fill argument to make these.\n\n\n\n\nNote we could also facet this data if we had other variables. For example, assume sampled another set of populations to the west..\n\niris_new &lt;- iris\ncolors &lt;- c(\"pink\", \"orange\", \"yellow\")\niris_new$Color &lt;- factor(sample(colors, size = nrow(iris),replace = T))\niris_both &lt;- rbind(iris,iris_new)\niris_both$Population &lt;- factor(c(rep(\"East\",nrow(iris)), rep(\"West\", nrow(iris_new))))\n\n\nggplot(iris_both,aes(x=Species)) +\n  geom_bar(aes(fill=Color))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_wrap(~Population, nrow=1)\n\n\n\n\nFigure 32: Faceting can make patterns easier to compare.\n\n\n\n\nNote we can combined these ideas!\n\nggplot(iris_both,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position=\"fill\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_wrap(~Population, nrow=1)\n\n\n\n\nFigure 33: We can add facets and proportions.\n\n\n\n\nFinally, we can end this section noting a pie chart is just a transformed bar chart.\n\niris_both$Share &lt;- \"\"\nggplot(iris_both,aes(x=Share)) +\n  geom_bar(aes(fill=Color), position=\"fill\")+\n  labs(title=expression(paste(\"Distribution of flower colors differ among populations of \",italic(\"I. species \"))),\n       y=\"\", \n       x=\"\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_grid(Population~Species) +\n coord_polar(theta=\"y\") \n\n\n\n\nFigure 34: We can add facets and proportions.\n\n\n\n\n\n\nRelationships among data from a single group\nInstead of collecting data on a single trait from multiple groups, we may collect data on multiple traits from a single group. For example, we could want to see if petal length is related to sepal width in I.virginica. This relationship could be visually summarized using a scatter plot.\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length, y=Petal.Length)) +\n  geom_point() +\n  labs(title=expression(paste(\"Larger sepals means larger petals in \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Petal Length (cm)\")\n\n\n\n\nFigure 35: Scatter plots show relationships among numerical variables.\n\n\n\n\nObviously we can (and will) combine many of the above approaches. For example, we may want to see if relationships among two numerical variables differ among groups (an ANCOVA!).\n\nggplot(iris,\n              aes(x=Sepal.Length, y=Petal.Length, color=Species)) +\n  geom_point() +\n  labs(title=expression(paste(\"Larger sepals means larger petals in \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Petal Length (cm)\")\n\n\n\n\nFigure 36: We can add facets and proportions\n\n\n\n\nWe’ll get to these later in class, but I just want to note their existence here. Finally, if you are reading this for the first time, don’t worry about the tests (just like the code!). We will explain how all these tests are related when we get there!\nFinally, note data of this type may include time or dates. We’ll use a different dataset to illustrate this.\n\nairquality$Date &lt;-as.Date(paste(airquality$Month, airquality$Day,\"1973\", sep=\"/\"), format =\"%m/%d/%Y\")\n\nggplot(airquality, aes(x=Date,y =Temp)) + \n  geom_point(col = \"orange\") + \n  labs(title=\"Temperature over time\", \n       x= \"Date\",\n       y=\"Temperature (C)\")\n\n\n\n\nFigure 37: Scatter plots can also include temporal data\n\n\n\n\nWe can also add lines…\n\nggplot(airquality, aes(x=Date,y =Temp)) + \n  geom_point(col = \"orange\") + \n  geom_line()+\n  labs(title=\"Temperature over time\", \n       x= \"Date\",\n       y=\"Temperature (C)\")\n\n\n\n\nFigure 38: Scatter plots can also include lines\n\n\n\n\nWe can even include multiple data sets!\n\nggplot(airquality, aes(x =Date,y =Temp)) + geom_point(aes(col =\"Temp\")) + geom_line(col=\"orange\") + geom_point(aes(y=Wind+50, col = \"Wind speed\")) + scale_y_continuous(sec.axis = sec_axis(~.-50, name = \"Wind (mph)\")) + geom_line(aes(y=Wind+50))+\n     labs(title=\"Environmental measurements over time\", \n       x= \"Date\",\n       y=\"Temperature (C)\")\n\n\n\n\nFigure 39: Scatter plots can also include multiple lines\n\n\n\n\n\n\nThere’s more to do and think about!\nThis just scratches the surface of potential ways to visualize data. For example, heatmaps can be used to show location specific data and we can build interactive or animated visualizations. However, the basic principles we’ve examined here should get you started.\nThe different approaches covered here also indicate there a lots way to display data! Whichever approach you use, you should ensure that you represent the data honestly and clearly. Sometimes that means you just display data (points)! You should also always consider possible ways the data/visualization could be misinterpreted and avoid them. Common mistakes include the decision about which baseline should be included. For example, should charts always include 0 on the y-axis? Not including 0 can may small differences appear large. However, including it can make important changes seem insignificant! Consider\n\nsmall_difference &lt;- data.frame(Treatment = c(\"a\",\"b\"), mean=c(37,40))\nlibrary(ggpubr)\n\n\nAttaching package: 'ggpubr'\n\n\nThe following object is masked from 'package:plyr':\n\n    mutate\n\nbp &lt;- ggplot(small_difference, aes(x=Treatment, y=mean, fill=Treatment))+\n  geom_bar(stat=\"identity\")\nsp &lt;- ggplot(small_difference, aes(x=Treatment, y=mean, color=Treatment))+\n  geom_point()\ncompare &lt;- ggarrange(bp, sp, labels = c(\"A\", \"B\"),\n          ncol = 2, nrow = 1,common.legend = TRUE, legend=\"bottom\")\nannotate_figure(compare,\n                top = text_grob(\"Including a zero point can make a big difference!\", color = \"red\", face = \"bold\", size = 14))\n\n\n\n\nFigure 40: The decision of where to start the y-axis can have major impacts on interpretation\n\n\n\n\nIf this is changes in temperature, option B may be more useful (this could be a normal temperature (37 C) compared to a fever of 104 (40 C). However, if its a difference in a metabolic rate, it may have minor impacts (and thus we choose option A)!\nSimilarly, imagine we collected this data\n\ngood_fit_x &lt;- runif(100, 1, 50) \ngood_fit_y &lt;- rnorm(100,25,2) \ngood_data &lt;- data.frame(source = \"good\", x=good_fit_x, y=good_fit_y) \nbad_fit_x &lt;- runif(10, 20, 30) \nbad_fit_y &lt;- rnorm(10,95,1) \nbad_data &lt;- data.frame(source = \"outlier\", x=bad_fit_x, y=bad_fit_y) \nall_data &lt;- rbind (good_data, bad_data)\n\npoints &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_point(aes(color=source)) + \n  labs(title=\"Raw data\")\n\npoints_plus_curve &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_point(aes(color=source)) + \n  geom_smooth(se = F) + \n    labs(title=\"Curve fit to data but points shown\")\n\ncurve &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_smooth(se = F) +\n    labs(title=\"Only curve\")\ncompare &lt;- ggarrange(points, points_plus_curve, curve, labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2, common.legend = TRUE, legend=\"bottom\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nannotate_figure(compare,\n                top = text_grob(\"Same data, three visualizations\", color = \"red\", face = \"bold\", size = 14))\n\n\n\n\nFigure 41: This would likely indicate\n\n\n\n\nWhereas the raw data (panel A) may suggest some outliers that are concerning, by panel C we have “smoothed” the data and made an interesting pattern. In general, thought must be applied to individual situations regarding visualization style and nuance. Adding information on spread in the data will also help (coming up!)."
  },
  {
    "objectID": "content/summarizing_data.html#numerical-summaries",
    "href": "content/summarizing_data.html#numerical-summaries",
    "title": "Summarizing data",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\nWhile visual summaries give us a clearer picture of the data, numerical summaries can help distill a large dataset into several components that can then be analyzed or compared. A key point is we are rarely trying to say if 2 groups are exactly the same or if a trait value is exactly equal to something. Given sampling error, we know its unlikely we would get exactly the same values, and, more importantly, its really rare for 2 groups to be exactly the same. Instead, we are often comparing characteristics of the population data among group or to set values.\n\nCentral tendency\nOne common characteristic of a population is central tendency. Central tendency considers what are common values in a dataset by focusing on the center of the distribution. Mean, or the average or \\(\\mu\\) , is one measure of central tendency. Due to sampling error, we don’t know \\(\\mu\\), but we can estimate it. In general, we use Greek letters to denote the population values and standard(Latin) letters typically denote our estimate (sometimes with added symbols). For example, if we have n data points, our estimate of the mean \\(\\mu\\) is known as \\(\\overline{Y}\\) (read as “y-bar”) is\n\\[\n\\overline{Y} = \\frac{\\sum_{i=1}^{n} n_{i}}{n} \\sim \\mu\n\\]\nwhere \\(\\sim\\) means “approximately”. Other measures of central tendency include the mode (most common data point) or median (middle data point if all data were placed in ascending order (remember box plots!)).\nWhy do we need more than one measure of central tendency? Consider our cardinal data:\n\n# function to calculate mode\nfun.mode&lt;-function(x){as.numeric(names(sort(-table(x)))[1])}\n\nggplot(data.frame(cardinals), \n       aes(x=cardinals)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(cardinals), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(cardinals), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(cardinals), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 42: Skew impacts value of and relationships among various measures of central tendency\n\n\n\n\nNote the data is left-skewed. so the mean is pulled towards these outliers. The median may offer a better summary of the actual center. We see similar outcomes with right-skewed data.\n\nggplot(data.frame(blue_jays), \n       aes(x=blue_jays)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester blue jays\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(blue_jays), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(blue_jays), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(blue_jays), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 43: Skew impacts value of and relationships among various measures of central tendency\n\n\n\n\nBut with symmetric data, we see the measures of central tendency align more\n\nggplot(data.frame(parrots), \n       aes(x=parrots)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(parrots), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(parrots), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(parrots), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 44: For symmetric data, measures of central tendency are more aligned.\n\n\n\n\n\n\nWhy is the mode not always on the highest bar?\n\nNote the mode is heavily/totally impacted by data precision and thus can lead to unusual matches with histograms. In our above example, cardinals were measured to 10-3 grams. However, the data was binned to levels of 10-2 grams. Due to this mismatch, the most common measurement of the raw data was 0.935, which occurred 159 times. However, more data points still fell in another bin!\n\nA special case where central tendency may not be the best way to describe a distribution is known as bimodal data. In this case, the distribution shows two, not one, clear peak. Remember the woodpecker Figure 14 data?\n\nggplot(data.frame(woodpeckers), \n       aes(x=woodpeckers)) +\n  geom_histogram(aes(y=..density..), color=\"black\") +\n  geom_density()+\n  labs(title=\"Weight of  Westchester woodpeckers\",\n       x= \"Weight (g)\",\n       y= \"Proportion\")+\n    theme_bw()+\n  geom_vline(aes(xintercept=mean(woodpeckers), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(woodpeckers), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(woodpeckers), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 45: Example of bimodal data with various measures\n\n\n\n\nNote the mode is (again) off, but the mean and median also represent uncommon individuals!\n\n\nSpread\nAlong with the central tendency, another set of numerical summaries focus on the spread of the data. In some ways these focus on how much of the data is in the tail (or how heavy the tail is). To illustrate this, consider the Figure 5 data.\n\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 46: Remember this example of approximately normal data?\n\n\n\n\nWe can instead plot the raw data\n\niris$sample &lt;- 1:nrow(iris)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point() +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")\n\n\n\n\nFigure 47: Note x-axis is just sample order!\n\n\n\n\nNow let’s add the mean\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point(color='blue') +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n  annotate(\"text\", label = \"mean\", x = 135, y = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") \n\n\n\n\nFigure 48: Mean shown in blue!\n\n\n\n\nNote the points differ in how far they are from the mean! For example, we could find another species with a very similar mean but different spread of the data.\n\niris_new &lt;- data.frame(Sepal.Length = runif(50,min=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])-.25, max=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.25), Species = \"uniforma\", sample=101:150)\niris_hypothetical &lt;- merge(iris, iris_new, all = T)\nggplot(iris_hypothetical[iris_hypothetical$Species %in% c(\"virginica\", \"uniforma\"),],\n              aes(y=Sepal.Length, x=sample, color=Species))+\n  geom_point() +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" and \", italic(\"uniforma \"), \"arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n    geom_hline(aes(yintercept=mean(iris_hypothetical[iris_hypothetical$Species == \"uniforma\", \"Sepal.Length\"])), color = \"red\")+\n  annotate(\"text\", label = \"I. virginica mean\", x = 135, y = mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") +\n  annotate(\"text\", label = \"I. uniforma mean\", x = 135, y = mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])-.13, color = \"blue\")+\n  scale_color_manual(values=c(\"blue\", \"red\"))\n\n\n\n\nFigure 49: Similar mean, very different distribution!\n\n\n\n\nThis spread could be quantified in multiple ways. Here we focus on the variance, which is defined as\n\\[\ns^2 = \\frac{\\sum_{i=1}^{n} (Y_{i}-\\overline{Y})^2}{n-1} \\sim \\sigma^2\n\\]\nAgain, the population parameter is \\(\\sigma^2\\), and the estimate is \\(s^2\\). This is effectively the average distance of each point from the mean squared!\n\nsegment_data &lt;- data.frame( x = 101:150, xend = 101:150, y=iris[iris$Species == \"virginica\", \"Sepal.Length\"], yend = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"]) )\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point(color='blue') +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n  annotate(\"text\", label = \"mean\", x = 135, y = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") +\n  annotate(\"text\", label = \"square each red line \\n and find average!\", x = 145, y = 7.5 , color = \"red\") + geom_segment(data = segment_data, aes(x = x, y = y, xend = xend, yend = yend), color= \"red\", size = 1.1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 50: Mean shown in blue!\n\n\n\n\nWhy \\(n-1\\) on the bottom, you might ask? Related reasons focus on degrees of freedom (we’ll get there) and bias. Our estimate of \\(s^2\\) is based on our sample mean. Since the sample mean can, at best, be the population mean, our variance estimate may be biased (too small). Dividing by \\(n-1\\) can be shown to correcthat. Alternatively, we are estimating one parameter from our data (again, \\(\\mu\\)), so we need to remove one degree of freedom from our calculation. However, some programs still \\(n\\) on the bottom. Note this will only really matter when \\(n\\) is small. In short, as another professor once told me, if the difference is based on whether you use \\(n\\) or \\(n-1\\) in your variance calculation, you likely have other problems.\nNote variance has odd (squared) units. If we take the square root of the variance, we get a value called the standard deviation.\n\\[\nstandard\\;deviation = sd= \\sqrt{variance}\n\\]\nThis metric is now in the same units as the mean (\\(\\mu\\)), so we can plot them easily on the same graph! This will become important later, especially when we discuss normal distributions.\n\n\nOthers worth knowing/remembering\nThe coefficient of variation, \\(CV\\), is found using the formula\n\\[\nCV = \\frac{s}{Y} * 100\\%\n\\]\nThis scales the variance (spread) of the data by the mean. The \\(CV\\) is unitless and can be used to compare various distributions.\nAs noted above in the discussion of five-number summaries, we can also find the assorted quartiles, minimum, and maximum of a dataset. This can be especially helpful when the mean isn’t a useful measure of central tendency (since variance and \\(CV\\) rely on the estimate of the mean!). For examples, see Figure 9. The original five number summary is\n\nsummary(cardinals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7910  0.9160  0.9370  0.9335  0.9550  0.9940 \n\n\nNote that if we shifted the bottom 10% of the values even further left (increasing the skew), the median stays the same, as does the IQR.\n\ncardinals_new &lt;- cardinals\ncardinals_new[cardinals_new &lt; quantile(cardinals_new, probs= .1)] &lt;- cardinals_new[cardinals_new &lt; quantile(cardinals_new, probs= .1)]-.4\n\nggplot(data.frame(cardinals_new), \n       aes(x=cardinals_new)) +\n  geom_histogram( fill=\"red\", color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals (shifted)\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nsummary(cardinals_new)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3910  0.9160  0.9370  0.8953  0.9550  0.9940 \n\n\n\n\n\nFigure 51: Shifted cardinal data\n\n\n\n\nFinally, if we have categorical instead of numerical data, we can find the proportion of data in each category. Producing this summary actually requires you to count the number in a given group and divided by the total number sampled."
  },
  {
    "objectID": "content/summarizing_data.html#next-steps",
    "href": "content/summarizing_data.html#next-steps",
    "title": "Summarizing data",
    "section": "Next steps",
    "text": "Next steps\nNow that we can summarize data, we can begin to connect summaries to statistics (and learn/remember some probability along the way!)."
  },
  {
    "objectID": "content/getting_started.html",
    "href": "content/getting_started.html",
    "title": "Before the first class",
    "section": "",
    "text": "Over the course of the semester/reading this book, our (ambitious) goals are to\nTo prepare for our first few lessons"
  },
  {
    "objectID": "content/getting_started.html#concept-stuff",
    "href": "content/getting_started.html#concept-stuff",
    "title": "Before the first class",
    "section": "Concept stuff",
    "text": "Concept stuff\n\nCheck out the class website\nWatch this video"
  },
  {
    "objectID": "content/getting_started.html#tech-stuff",
    "href": "content/getting_started.html#tech-stuff",
    "title": "Before the first class",
    "section": "Tech stuff",
    "text": "Tech stuff\n\nGet access to R!. You can make an account at Rstudio cloud (https://rstudio.cloud/). You can also install R (https://cran.r-project.org/) and Rstudio (https://www.rstudio.com/) on your machine, but I strongly recommend starting with Rstudio cloud.\nRstudio cloud is free for up to 25 hours/month, you don’t have to maintain it, and it gives gives a standard install (same on all machines, so your intro/ our training may be smoother). You can also do both. If you need help, videos are at :\n\nDownloading R\nDownloading Rstudio\nMaking a Rstudio cloud account\n\nJoin the github classroom we’ll be using for our sessions\n\nlook for email from Blackboard! \nWhen you visit the page it will ask you to connect or create a github repository. You can use any name (be anonymous or not) that you want. This is a free process.\n\n\n\nOptional (get a head start if you want)\nIt may be easier to open these intructions in a browser so you can follow along there while working in Rstudio!\nAfter you join the github classroom, you’ll make a clone of the repository onto your machine. First, find your copy of the repository. You can follow the github classroom link again, or log into github and then visit https://github.com/settings/repositories. Find the repository called data_science_intro_YOURGITHUBUSERNAME, and click on it. Then follow along below - find instructions for Rstudio cloud or Rstudio desktop depending on your setup.\n\nIf you are using Rstudio cloud…\nVideo at Accepting your first github repository (from github classroom) and cloning to Rstudio cloud\nLog into your Rstudio cloud account. You’ll see something like this:\n\n\n\nRstudio cloud home screen\n\n\nTo copy a repository, select New Project, New Project from Github repo. Next you’ll need to enter the url for your repository. To find this, click on the Code button from the github page for your repository (instructions above!)\n\n\n\nClick on Code to get repository url\n\n\nCopy the web url (or click the copy icon). Input that into the field asking for the URL of your github repository.\nNote you may need to enter your github username and password to create the repository.\nThe next screen will bring you to a “normal” RStudio screen. We’ll come back to this in the first class or two!\n\n\nIf you are using RStudio on your desktop (or via a server…anywhere that\nlooks like an RStudio screen)\nVideo at Accepting your first github repository (from github classroom) and cloning to Rstudio desktop\nTo start working on an assignment, open RStudio.\n\n\n\nSelect File &gt; New Project in Rstudio\n\n\nSelect file, new project, Version control. On the next screen select git. If this isn’t available, you may need to install git (free) on your system. You can download it at https://git-scm.com/download/.\nNext you’ll need to enter the url for your repository. To find this, click on the Code button from the github page for your repository (instructions above!).\n\n\n\nClick on Code to get repository url\n\n\nCopy the web url (or click the copy icon). Input that into the Rstudio Repository URL space. You can select/edit what you want the repository to be called and where its stored (its just a folder on your computer). For example, I have a Repositories folder in my main hard drive where I save all of these. Then select Create project. Whatever you choose, the project will be saved in new folder in that location using the name you chose. Note you may need to enter your github username and password to create the repository.\nYou also may get an error/warning about personal access token! this happens at different points on different machines (thus why Rstudio cloud is nice). If you see this now, don’t worry. We’ll cover it (a known issue) in class.\nThe next screen will bring you to a “normal” RStudio screen. We’ll come back to this in the first class or two!"
  },
  {
    "objectID": "content/Acquiring_data.html",
    "href": "content/Acquiring_data.html",
    "title": "Acquiring data",
    "section": "",
    "text": "Let’s start our statistics journey by thinking about the simplest scenario: We want to know something about a group. An example might be the average (or mean, we will define later if needed!) value for some trait, the minimum value, or the maximum value. We could also wish to know about the distribution of values for that trait in the group. These traits of the group are called statistics:\n\nthe numerical facts or data themselves - Dictionary.com\n\nThis means we have a target trait we are focused on, and we have defined a group of interest. We can call this group of interest a population. Note that while the term population may have specific meanings in some fields (such as ecology), here population is just the group of interest. It could be a population of Goliath grouper in Florida, a population of flowers in Virginia, or people from a certain country or demographic group. We could want to know something about all of these groups!\nAs we’ve already noted, in a perfect world we know everything (or at least our trait value) for every member of the focal population. However, we often don’t or can’t measure every member of a population. It may be too difficult or expensive to measure every member of the population. In fact, we may not even know how large the population is!\nIn the cases where we can’t measure every member of the population, we collect data on the focal trait(s) from a sample. A sample is the subset of the population of interest. Data can be collected from samples used in experimental studies, where researchers manipulate something to see how it impacts the focal trait. Researchers may expose organisms to different stimuli in a controlled lab, field, or mesocosm study to see what happens. For example, researchers interested in impacts of an invasive crayfish (Pacifastacus leniusculus) on Mazama newts (Taricha granulosa mazamae) collected newts and crayfish.; they then placed either just newts or newts and crayfish in in large tanks to observe interactions Girdner et al. (2018).\n\n\n\nFigure 1: Experimental mesocosms used to evaluate Mazama newt and signal crayfish behavior on Wizard Island, Crater Lake, Oregon. A team of NPS scientists observed the interaction between newts and crayfish in tanks designed to mimic natural habitat.\n\n\nData can also be collected from observational studies, where researchers simply measure outcomes and other traits without manipulating anything. For example, scientists interested in impacts of climate change on species ranges surveyed sites for species presence and abundance and compared it to historical data (Sagarin et al. (1999)).\nDifferent types of studies change what we can use the data for. In general, experimental studies are more commonly used to ascertain causation (something makes something happen), whereas observational studies are used to assess correlation (something happens when something else happens). However, these can be hard to disentangle, especially since studies can only be observational since experiments would be unethical or impossible to carry out. As XKCD puts it\n\n\n\nFigure 2: XKCD: Correlation. Title text (text that pops up when you hover over the comic): Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’.\n\n\n\nCorrelation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’ - XKCD #552\n\nOnce we have the sample, we can measure the trait of interest in it, and use that to estimate the statistic of interest for the actual population. This is the science of statistics, which can actually be defined as\n\nthe practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample. - Oxford English Dictionary\n\nIf the whole idea of statistics is to infer something about the population from our sample, we need to make sure the sample is representative of the population. That means it should not be biased. Bias occurs if the trait values we measure in our sample differ from the population in a consistent way. This can happen with samples of convenience, or when researchers select samples that are easy to measure but may not be representative of the population. Classic examples include estimating the amount of time students spend studying by surveying students at a campus library.\nBias may also be related to issues of independence. In a good sampling design, every member of the population has the same chance of being included in a sample. Samples of convenience violate this premise, and often the underlying issue is that the samples are not independent. A perfect solution is to randomly choose members of the population to be in the sample, but that is often not possible. Again, it requires knowing every member of the population! Indepence also means each data point is not related to any others!\n\n\n\nFigure 3: XKCD: Slope Hypothesis Testing. Don't worry, we'll come back to significance - but what is the independence issue?\n\n\nIn some cases linkages among samples are impossible to avoid. We will cover ways to address that using blocking factors or random effects later.\nNotice in discussing bias we are not directly focusing on the quality of the measurements! For that, we could discuss accuracy (how well we measure the underlying trait in regards to its true value, which we typically don’t know) and precision (how repeatable our measurement technique is). Obviously we need good data to make good estimates, but these ideas are different from our current focus on picking a good sample.\nEven if we have a proper way to measure a trait (accurate and precise) in a good sample (not biased), we will still be producing an estimate of the population statistic! This is due to sampling error. Sampling error refers to the fact that every sample will produce a slightly different estimate of the statistic. Imagine this - there a 1000 fish in a lake. We sample 50 of them, measure their length, and use it calculate the average fish length. If we took a different sample, do you think it would have exactly the same average?\nWe can demonstrate this in R - you won’t understand the code below yet, so just trust me for now, but this will let you start seeing code and thinking about how to use it.\nLet’s generate a population of fish. We’ll store their lengths in a vector called lengths.\n\nlengths &lt;- rnorm(n=1000, mean = 10, sd=1)\n\nThe average length of fish in this population is 10 cm. We can then simulate a sample from this population. In fact, let’s simulate 2 and compare the means of each.\n\nsample_1 &lt;- sample(lengths,50)\nsample_2 &lt;-sample(lengths, 50)\n\nThe mean length for fish in sample 1 is 9.94 cm, while that in sample 2 is 10 cm (Note: if you view this on the webpage you will see a number, but in the actual qmd file you see R code here - this is an example of merging code and text!). These are both close to the true value, but they are also both slightly different - this is sampling error!\nSampling error always exists, and a major part of statistics is to quantify it. One thing that reduces sampling error is to have large samples! Remember, if we measure every member of the population we don’t even need statistics, so the closer we get to that (implying larger samples) the better!"
  },
  {
    "objectID": "content/Acquiring_data.html#how-do-we-get-data",
    "href": "content/Acquiring_data.html#how-do-we-get-data",
    "title": "Acquiring data",
    "section": "",
    "text": "Let’s start our statistics journey by thinking about the simplest scenario: We want to know something about a group. An example might be the average (or mean, we will define later if needed!) value for some trait, the minimum value, or the maximum value. We could also wish to know about the distribution of values for that trait in the group. These traits of the group are called statistics:\n\nthe numerical facts or data themselves - Dictionary.com\n\nThis means we have a target trait we are focused on, and we have defined a group of interest. We can call this group of interest a population. Note that while the term population may have specific meanings in some fields (such as ecology), here population is just the group of interest. It could be a population of Goliath grouper in Florida, a population of flowers in Virginia, or people from a certain country or demographic group. We could want to know something about all of these groups!\nAs we’ve already noted, in a perfect world we know everything (or at least our trait value) for every member of the focal population. However, we often don’t or can’t measure every member of a population. It may be too difficult or expensive to measure every member of the population. In fact, we may not even know how large the population is!\nIn the cases where we can’t measure every member of the population, we collect data on the focal trait(s) from a sample. A sample is the subset of the population of interest. Data can be collected from samples used in experimental studies, where researchers manipulate something to see how it impacts the focal trait. Researchers may expose organisms to different stimuli in a controlled lab, field, or mesocosm study to see what happens. For example, researchers interested in impacts of an invasive crayfish (Pacifastacus leniusculus) on Mazama newts (Taricha granulosa mazamae) collected newts and crayfish.; they then placed either just newts or newts and crayfish in in large tanks to observe interactions Girdner et al. (2018).\n\n\n\nFigure 1: Experimental mesocosms used to evaluate Mazama newt and signal crayfish behavior on Wizard Island, Crater Lake, Oregon. A team of NPS scientists observed the interaction between newts and crayfish in tanks designed to mimic natural habitat.\n\n\nData can also be collected from observational studies, where researchers simply measure outcomes and other traits without manipulating anything. For example, scientists interested in impacts of climate change on species ranges surveyed sites for species presence and abundance and compared it to historical data (Sagarin et al. (1999)).\nDifferent types of studies change what we can use the data for. In general, experimental studies are more commonly used to ascertain causation (something makes something happen), whereas observational studies are used to assess correlation (something happens when something else happens). However, these can be hard to disentangle, especially since studies can only be observational since experiments would be unethical or impossible to carry out. As XKCD puts it\n\n\n\nFigure 2: XKCD: Correlation. Title text (text that pops up when you hover over the comic): Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’.\n\n\n\nCorrelation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’ - XKCD #552\n\nOnce we have the sample, we can measure the trait of interest in it, and use that to estimate the statistic of interest for the actual population. This is the science of statistics, which can actually be defined as\n\nthe practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample. - Oxford English Dictionary\n\nIf the whole idea of statistics is to infer something about the population from our sample, we need to make sure the sample is representative of the population. That means it should not be biased. Bias occurs if the trait values we measure in our sample differ from the population in a consistent way. This can happen with samples of convenience, or when researchers select samples that are easy to measure but may not be representative of the population. Classic examples include estimating the amount of time students spend studying by surveying students at a campus library.\nBias may also be related to issues of independence. In a good sampling design, every member of the population has the same chance of being included in a sample. Samples of convenience violate this premise, and often the underlying issue is that the samples are not independent. A perfect solution is to randomly choose members of the population to be in the sample, but that is often not possible. Again, it requires knowing every member of the population! Indepence also means each data point is not related to any others!\n\n\n\nFigure 3: XKCD: Slope Hypothesis Testing. Don't worry, we'll come back to significance - but what is the independence issue?\n\n\nIn some cases linkages among samples are impossible to avoid. We will cover ways to address that using blocking factors or random effects later.\nNotice in discussing bias we are not directly focusing on the quality of the measurements! For that, we could discuss accuracy (how well we measure the underlying trait in regards to its true value, which we typically don’t know) and precision (how repeatable our measurement technique is). Obviously we need good data to make good estimates, but these ideas are different from our current focus on picking a good sample.\nEven if we have a proper way to measure a trait (accurate and precise) in a good sample (not biased), we will still be producing an estimate of the population statistic! This is due to sampling error. Sampling error refers to the fact that every sample will produce a slightly different estimate of the statistic. Imagine this - there a 1000 fish in a lake. We sample 50 of them, measure their length, and use it calculate the average fish length. If we took a different sample, do you think it would have exactly the same average?\nWe can demonstrate this in R - you won’t understand the code below yet, so just trust me for now, but this will let you start seeing code and thinking about how to use it.\nLet’s generate a population of fish. We’ll store their lengths in a vector called lengths.\n\nlengths &lt;- rnorm(n=1000, mean = 10, sd=1)\n\nThe average length of fish in this population is 10 cm. We can then simulate a sample from this population. In fact, let’s simulate 2 and compare the means of each.\n\nsample_1 &lt;- sample(lengths,50)\nsample_2 &lt;-sample(lengths, 50)\n\nThe mean length for fish in sample 1 is 9.94 cm, while that in sample 2 is 10 cm (Note: if you view this on the webpage you will see a number, but in the actual qmd file you see R code here - this is an example of merging code and text!). These are both close to the true value, but they are also both slightly different - this is sampling error!\nSampling error always exists, and a major part of statistics is to quantify it. One thing that reduces sampling error is to have large samples! Remember, if we measure every member of the population we don’t even need statistics, so the closer we get to that (implying larger samples) the better!"
  },
  {
    "objectID": "content/Acquiring_data.html#next-steps",
    "href": "content/Acquiring_data.html#next-steps",
    "title": "Acquiring data",
    "section": "Next steps",
    "text": "Next steps\nNow that we have data, we’ll discuss summarizing it in the next section (and actually define mean and some of the other terms we’ve started to use!)."
  },
  {
    "objectID": "content/Probability.html",
    "href": "content/Probability.html",
    "title": "Probability",
    "section": "",
    "text": "We’ve already address probability. When we stated the 95% confidence interval means that if we make these intervals from 100 samples that we expect 95 of them to contain the true mean, we are discussing probability. An even easier example is flipping a coin. You probably know there is a 50% chance of a coin landing on heads. That doesn’t mean given flip will be half heads and half tails. Both of these statements refer to likely outcomes if we do something many, many times!"
  },
  {
    "objectID": "content/Probability.html#what-is-probability",
    "href": "content/Probability.html#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability?\nTo put it specifically, the probability of an outcome is its true relative frequency, the proportion of times the event would occur if we repeated the same process over and over again. We can describe the probability if an outcome by considering all the potential outcomes and how likely each is. If we describe all the outcomes, the total probability must be equal to 1 (since frequency is typically measured as a fraction!). Some probability distributions can be described mathematically, others are a list of possible outcomes, and others are almost impossible to solve. The “impossible” ones require simulations, and we will return to this for our introduction to Bayesian analysis.\nThe simplest case is when we focus on outcomes for a single trait that falls into specific categories. These outcomes are often mutually exclusive, meaning only one can happen (like our heads and tails example!), and lead to discrete probability distributions (meaning each outcome has to be a specific value). Another example is rolling a 6-sided die. The die can only land on numbers 1 to 6 (so 2.57 is not an option!).\n\ndie_roll &lt;- data.frame(Number = 1:6, Probability = rep(1/6,6))\ndie_roll$Number &lt;- factor(die_roll$Number)\n\nlibrary(ggplot2)\nggplot(die_roll, aes(x=Number, y= Probability, fill=Number)) +\n  geom_col() \n\n\n\n\nin this example, the probability of rolling a 6 is .167. You may see this written as\n\\[\nP[roll=6]= \\frac{1}{6} \\sim .167\n\\]Obviously when you roll a die you don’t roll .167 of a 6. You roll a 1, 2, 3, 4, 5, or 6. Again, probability refers to the expected outcome over multiple attempts.\nCompare this to a continuous probability distribution, where the outcome can take any value in a given range.\n\nggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = \"orange\")+\n  labs(y=\"Probability\", x=\"Outcome\")\n\n\n\n\nHere’s an odd outcome: Since the outcome can take on any value in a given range, the chance of it being a specific value is 0. Think about it this way - for any value you mention, I can zoom in more. For example, if you ask the probability of x in the above graph being equal to 0, we could zoom in to 0.0, or 0.00, or 0.000. At some limit of resolution, the area under the curve (which denotes the probability and would be found using integral calculus, which we won’t do here) would be equal to 0!\nThis may seem like an odd aside, but it is actually very important. It explains why when we will discuss probabilities the probability of an outcome being less than, more than, or between two values in upcoming chapters. For example, we can note (again) that for a normal distribution (what we see above and will (still eventually) define more appropriately) that 67% of the data falls within one standard devation for perfectly (very rare!) normally-distributed data.\n\n#function from https://dkmathstats.com/plotting-normal-distributions-in-r-using-ggplot2/\ndnorm_one_sd &lt;- function(x){\n  norm_one_sd &lt;- dnorm(x)\n  # Have NA values outside interval x in [-1, 1]:\n  norm_one_sd[x &lt;= -1 | x &gt;= 1] &lt;- NA\n  return(norm_one_sd)\n}\n\nggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = \"orange\")+\n  labs(y=\"Probability\", x=\"Outcome\") + \n  stat_function(fun = dnorm_one_sd, geom = \"area\", fill = \"purple\")\n\n\n\n\n\nWhat if more than one thing is of importance?\nSometimes we focus on the probability of more than one outcome for a given event. This requires adding or combining probabilities. The first step in doing this is deciding if the outcomes are mutually exclusive. This means they can not occur in the same unit of focus. For example, we could ask the probability of rolling a 1 or a 6, or of being &gt;2 and &lt;-2. In both cases, a single outcome can’t be both of these things, so the outcomes are mutually exclusive. When this is the case, we simply add the probabilities. This is sometimes called the union of two outcomes.\nContrast this with when we want to know the probability of two things occurring that may occur in the same unit. For example, assume our die not only had dots on it, but these dots were a different color. For example, odd numbers were blue and even numbers were red.\n\ncolors &lt;- c(\"blue\" = \"blue\", \"red\" = \"red\")\ndie_roll$Color &lt;- NA\ndie_roll[as.numeric(as.character(die_roll$Number)) %% 2 == 0, \"Color\"] &lt;- \"blue\"\ndie_roll[as.numeric(as.character(die_roll$Number)) %% 2 != 0, \"Color\"] &lt;- \"red\"\nggplot(die_roll, aes(x=Number, y= Probability, fill=Color)) +\n  geom_col() +\n  scale_fill_manual(values = colors)\n\n\n\n\nNow, the probability of rolling any given group of numbers can be found by adding probabilities since the outcomes are mutually exclusive. Same for die color. However, what about the probability of rolling a blue (even) outcome or a 6? Note we can’t simply add these. Why not?\nBecause a single roll can result in a 6 and blue dots! So adding the probability of getting blue dots (.5) and of getting a 6 (.167) will double-count the 6. In other words, there is an an intersection of the possible outcomes. So, the probability of rolling a 6 or blue is equal to\n\\[\nP[roll=blue] + P[roll=6] - P[roll=6 \\ and \\ blue]= \\frac{1}{2} + \\frac{1}{6} -\\frac{1}{6}\n\\]\nThis is sometimes called the general addition principle.\nIf we are measuring multiple outcomes (note this slightly different than the probability of two or more outcomes for a specific event), we need to consider if the events are independent. This means the outcome of one does not influence the outcome of the other. If this is true, the probability of both events occurring can be found by simply multiplying the probability of each (the multiplication rule. For example, consider the probability of flipping a coin twice and seeing a heads followed by a tails. We can write out all the options (T, HH, TH, TT); assuming independence, the probability for each is \\(\\frac{1}{4}\\). We can also say the probability of heads on the first flip is \\(\\frac{1}{2}\\) and the probability of tails on the second flip is \\(\\frac{1}{2}\\); multiplying these yields \\(\\frac{1}{2}\\).\nConsider instead that we roll 2 dice and measure the sum of the rolls. Since one roll does not influence the other, these are independent events. As noted above, we can work out the probability distribution by writing out all possible outcomes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF irst Di ce\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\nSe cond Di ce\n1\n( 1,1)\n( 1,2)\n( 1,3)\n(1 , 4)\n( 1,5)\n( 1,6)\n\n\n\n2\n( 2,1)\n( 2,2)\n(2 , 3)\n( 2,4)\n( 2,5)\n( 2,6)\n\n\n\n3\n( 3,1)\n(3 , 2)\n( 3,3)\n( 3,4)\n( 3,5)\n( 3,6)\n\n\n\n4\n(4 , 1)\n( 4,2)\n( 4,3)\n( 4,4)\n( 4,5)\n( 4,6)\n\n\n\n5\n( 5,1)\n( 5,2)\n( 5,3)\n( 5,4)\n( 5,5)\n( 5,6)\n\n\n\n6\n( 6,1)\n( 6,2)\n( 6,3)\n( 6,4)\n( 6,5)\n( 6,6)\n\n\n\nNow assume we want to know the probability of the sum of the dice being equal to 5. If we assume independence, the the probability of rolling a sum of 5 (highlighted cells above) is \\(\\frac{4}{36}\\).\nWe could also simulate the outcome\n\nlibrary(reshape2)\nnumber_of_rolls &lt;- 100000\nsum_of_rolls &lt;- data.frame(index = 1:number_of_rolls, Sum = NA)\nfor (i in 1:number_of_rolls){\n  dice_roll_trial &lt;- sample(1:6, size = 2, replace = TRUE)\n  sum_of_rolls$Sum[i] &lt;- sum(dice_roll_trial)\n}\nsum_of_rolls_df &lt;- dcast(sum_of_rolls, Sum ~ \"Probability\", length )\n\nUsing Sum as value column: use value.var to override.\n\nsum_of_rolls_df$Probability &lt;- sum_of_rolls_df$Probability/number_of_rolls\nggplot(sum_of_rolls_df, aes(x=Sum, y=Probability)) +\n  geom_col(fill=\"orange\") +\n  labs(y=\"Probability\")+\n    scale_x_continuous(breaks = c(2:12))\n\n\n\n\nNotice here we find the probability of rolling a sum of 5 is 0.11055 which is very close to \\(\\frac{4}{36}\\).\nTo find the probability using math, we have to note that the dice rolls are independent. For example, even though we only want a 4 on the second dice if we roll a 1 on the first dice, the roll of the first die does not influence the roll of the second. We should also note the desired outcomes are mutually exclusive. So we can find the probability of each happening and then add them. It’s easy to see how probability can get complicated!"
  },
  {
    "objectID": "content/Probability.html#conditional-probability",
    "href": "content/Probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nUnlike our coin example, sometimes a first event occurring does influence the probability of a second event.\n\n\n\nXKCD Conditional Risk: The annual death rate among people who know that statistics is one is six.\n\n\nIn a similar vein, although the risk of shark attack is low, it increases dramatically if you swim in the ocean.\nUnlike our previous examples, we now have 2 events (lets call them A and B), and the probability of both occurring is equal to\n\\[\nP[A \\ and \\ B] = P[A] \\ P[B|A]\n\\]\nwhich can be read as “the probability of A and B occurring is equal to the probability of A multiplied by the probability of B given A occurs”. Note if A and B are independent, this reduces to the multiplication rule.\nWe can extend this by noting\n\\[\nP[A \\ and \\ B] = P[A] \\ P[B|A] \\\\\nP[A \\ and \\ B] = P[B] \\ P[A|B] \\\\\nP[A] \\ P[B|A] = P[B] \\ P[A|B] \\\\\nP[A|B] = \\frac{P[B|A]*P[A]}{P[B]}\n\\]\nThis rule is known as Bayes’ Theorem. We will return to this when we discuss Bayesian analysis, but we can use it here for demonstration.\nFor our lightning example, we could use some (pretend) numbers to understand the risk and Bayes’ Theorem. First, let A be the probability of being outside in a lightning storm. B is then the probability of getting struck by lightning, , and P[B|A] is the probabiity of getting struck by lightning given that you are outside in a lightning storm (hint: it’s much higher than the P[B]).\nHere’s anoher similar (in concept) example. Medical trials are designed to test the effectiveness of drugs or treatments. In these trials, drug efficacy is considered by comparing outcomes in people who receive the drug or treatment compared to those who receive a placebo (such as a sugar pill). Note this only works if participants do not know which group (drug vs placebo) they are in (why?). In a given trial, people who receive the drug recover 60% of the time (or avoid some other adverse outcome). This may seem good, but it’s only relevant when compared to the placebo group. What if people receiving the placebo recovered 80% of the time? Also, if we know the probability of recovering without the drug, we can consider the total probability of recovery. For now, let’s assume that 20% of people who receive the placebo recover.\nWe could use a tree diagram to consider possible options:\n\nlibrary(DiagrammeR)\n\nbayes_probability_tree &lt;- function(prior, true_positive, true_negative, label1 = \"Prior\", \n                                   label2 = \"Complimentary Prior\", label3 = \"True Positive\",\n                                   label4 = \"False Negative\", label5 = \"False Positive\",\n                                   label6 = \"True Negative\") {\n  \n  if (!all(c(prior, true_positive, true_negative) &gt; 0) && !all(c(prior, true_positive, true_negative) &lt; 1)) {\n    stop(\"probabilities must be greater than 0 and less than 1.\",\n         call. = FALSE)\n  }\n  c_prior &lt;- 1 - prior\n  c_tp &lt;- 1 - true_positive\n  c_tn &lt;- 1 - true_negative\n  \n  round4 &lt;- purrr::partial(round, digits = 5)\n  \n  b1 &lt;- round4(prior * true_positive)\n  b2 &lt;- round4(prior * c_tp)\n  b3 &lt;- round4(c_prior * c_tn)\n  b4 &lt;- round4(c_prior * true_negative)\n  \n  bp &lt;-  round4(b1/(b1 + b3))\n  \n  labs &lt;- c(\"X\", prior, c_prior, true_positive, c_tp, true_negative, c_tn, b1, b2, b4, b3)\n  \n  tree &lt;-\n    create_graph() %&gt;%\n    add_n_nodes(\n      n = 11,\n      type = \"path\",\n      label = labs,\n      node_aes = node_aes(\n        shape = \"circle\",\n        height = 1,\n        width = 1,\n        x = c(0, 3, 3, 6, 6, 6, 6, 8, 8, 8, 8),\n        y = c(0, 2, -2, 3, 1, -3, -1, 3, 1, -3, -1))) %&gt;% \n    add_edge(\n      from = 1,\n      to = 2,\n      edge_aes = edge_aes(\n        label = label1\n      )\n    ) %&gt;% \n    add_edge(\n      from = 1, \n      to = 3,\n      edge_aes = edge_aes(\n        label = label2\n      )\n    ) %&gt;% \n    add_edge(\n      from = 2,\n      to = 4,\n      edge_aes = edge_aes(\n        label = label3\n      )\n    ) %&gt;% \n    add_edge(\n      from = 2,\n      to = 5,\n      edge_aes = edge_aes(\n        label = label4\n      )\n    ) %&gt;% \n    add_edge(\n      from = 3,\n      to = 7,\n      edge_aes = edge_aes(\n        label = label5\n      )\n    ) %&gt;% \n    add_edge(\n      from = 3,\n      to = 6,\n      edge_aes = edge_aes(\n        label = label6\n      )\n    ) %&gt;% \n    add_edge(\n      from = 4,\n      to = 8,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 5,\n      to = 9,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 7,\n      to = 11,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 6,\n      to = 10,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) \n  message(glue::glue(\"The probability of having {label1} after testing {label3} is {bp}\"))\n  print(render_graph(tree))\n  invisible(tree)\n}\n\n#first example\nbayes_probability_tree(prior = 0.5, true_positive = 0.8, true_negative = 0.8, label1 = \"medicine\", label2 = \"placebo\",\n                       label3 = \"cured\", label4 = \"not cured\",\n                       label5 = \"cured\", label6 = \"not cured\")\n\nThe probability of having medicine after testing cured is 0.8\n\n\nThis tree let’s us consider multiple things. We can see the probability of being cured is .5, but 80% of those come from the group receiving medicine (again, the P[being cured|given medicine]). WE could also derive this using Bayes Theorem.\n$$ P[being  cured|medicine] = \\ P[being  cured|medicine] = \\ P[being  cured|medicine] = .8\n$$\nOne more example. Instead of medical trials, let’s focus on medical screenings. These are used to identify patients who have a condition, but there are no perfect tests. A test may give a false positive, meaning it says a condition exists when it does not. A test can also give a false negative, meaning it finds a condition does not exist when it really does. Both of these present issues for patients and explain why series of tests are often used before more invasive procedures are employed.\nFor example, assume a procedure is used to assess skin cancer. This cancer occurs at a frequency of .00021 in the general population. The test is fairly accurate; if a patient has cancer, the screening will correctly identify if 95% of the time. However, the probability of a false positive is .01. Given these numbers, how worried should a person be about a positive test?\nAlthough the test seems to be good, note the prevalence of a false positive is far higher than the prevalence of cancer! This means most positives will likely be false. To quantify this, let A be the probability of cancer and B be the probability of a positive screening. So,\n\\[\nP[A|B] = \\frac{.95 \\ * .00021}{.95 \\ * \\ .0021+.01*.9779} \\\\\nP[A|B] = .169\n\\]\nIn other words, only 17% of people with positive screenings actually have cancer.\nTo show this in a probability tree:\n\nbayes_probability_tree(prior = 0.0021, true_positive = 0.95, true_negative = 0.99, label1 = \"cancer\", \n                       label2 = \"not cancer\",\n                       label3 = \"positive\", \n                       label4 = \"negative\",\n                       label5 = \"positive\", \n                       label6 = \"negative\")\n\nThe probability of having cancer after testing positive is 0.16625"
  },
  {
    "objectID": "content/Probability.html#related-ideas",
    "href": "content/Probability.html#related-ideas",
    "title": "Probability",
    "section": "Related Ideas",
    "text": "Related Ideas\nFalse results are integral to the ideas of test specificity and sensitivity Lalkhen and McCluskey (2008). A specific test will yield few false negatives, while a sensitive test will yield few false positives. Put another way (from Lalken & McCluskey (2008): “A test with 100% sensitivity correctly identifies all patients with the disease”, and ” a test with 100% specificity correctly identifies all patients without the disease”. True and false results also impact predictive values.\n\n\n\n\n\n\n\n\n\nCondition\n\n\n\n\n\n\nTest Outcome\nPresent\nAbsent\n\n\nPositive\nTrue Positive\nFalse positive\n\n\nNegative\nFalse negative\nTrue Negative\n\n\n\nSensitivity=\nTrue Positive/ Condition present\nSpecificity =\nTrue negative/\nCondition negative\n\n\n\nThese ideas can also be related to power. We will return to this visualization in future chapters.\n\n\n\nA 3D visualisaion of PPV, NPV, Sensitivity and Specificity. Luigi Albert Maria, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons"
  },
  {
    "objectID": "content/Probability.html#next-steps",
    "href": "content/Probability.html#next-steps",
    "title": "Probability",
    "section": "Next steps",
    "text": "Next steps\nNow that we’ve discussed probability, we can move into the wild world of p-values and discuss how they relate to estimation!"
  },
  {
    "objectID": "content/Estimation.html",
    "href": "content/Estimation.html",
    "title": "Estimation and uncertainty",
    "section": "",
    "text": "Now that we can describe data distributions, we want to start thinking about how we quantify the uncertainty in our estimates (of \\(\\mu\\), for example). Remember, we typically want to describe a population but need to rely on a sample, and we’ve already talked about sampling error. So now we just want to think about how much error we typically have (or, alternatively, how precise are our estimates).\nAnswering this question is hard. Quantifying sampling error requires you to know the “true” value for a population parameter, but we only have estimates! Statisticians solve this problem by investigating sampling error in populations they fully know because they created them."
  },
  {
    "objectID": "content/Estimation.html#lets-start-with-an-example",
    "href": "content/Estimation.html#lets-start-with-an-example",
    "title": "Estimation and uncertainty",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\nFor example, let’s assume we measure all the males in a population. Furthermore, let’s assume the distribution of heights is normal. Remember, this means the distribution is roughly symmetric, with tails on either side. Values near the middle of the range are more common, with the chance of getting smaller or larger values declining at an increasing rate. In fact, in turns out ~95% of the data lies within two standard deviations (remember those?) of the mean (so we calculate the mean and then the standard deviation. We then subtract the standard deviation from the mean to find a lower bound. We then add the standard deviation from the mean to find an upper bound. These bounds denote where 95% of the data points will be found).\nLet’s see this in action. First, lets make a population with a trait (let’s assume height, measured in cm, that follows a normal distribution. We can set the mean to 70 and the standard deviation to 3.\n\nset.seed(42)\npopulation_size &lt;- 10000\npopulation_norm &lt;- data.frame(id = 1:population_size, \n                         height = rnorm(population_size, 70, 3))\n\nNow’s let graph it.\n\nlibrary(ggplot2)\n\nggplot(population_norm, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow let’s add the mean (69.97 cm) and mark two standard deviations (sd = 3.02 in) above and below it. Remember we noted a benefit of using standard deviations to describe spread was that they were in the same units as the mean? Now we can use that!\n\ncolors &lt;- c(\"mean\" = \"black\", \"2 standard deviations below\" = \"red\", \n            \"2 standard deviations above\" = \"green\")\nggplot(population_norm, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(height), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(height)-\n                     2*sd(height), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(height)+\n                     2*sd(height), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 1200, x = mean(population_norm$height), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 1200, x = mean(population_norm$height)-\n                     2*sd(population_norm$height), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 1200, x = mean(population_norm$height)+\n                     2*sd(population_norm$height), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 1: Our imaginary population!\n\n\n\n\nThis bound captures 95.53% of the data.\nNow let’s sample the population. We’ll start by drawing a sample of 100 from the population. This is true random sampling, so any differences are due to sampling error.\n\nsample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\nggplot(sample_1, aes(height)) + \n  geom_histogram() +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of 100 random males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor this sample, we have a mean of 69.78 cm and a standard deviation of 3.11 cm.\n\nMoving to a sample of means\nHere’s the tricky part. We typically only have one sample, but we want to discuss the uncertainty in our estimate. So, let’s explore this by drawing multiple samples (each of 100 individuals) from our population and finding the mean for each sample.\n\nnumber_of_samples &lt;- 1000\nsample_outcomes_1 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes_1$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes_1$sd[i] &lt;- sd(sample_1$height)\n  \n}\n\nThen let’s plot the means.\n\nggplot(sample_outcomes_1, aes(mean)) + \n    geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 100)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_1$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_1$mean)-\n                     2*sd(sample_outcomes_1$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_1$mean)+\n                     2*sd(sample_outcomes_1$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor our sample of means (this should sound weird!), we have a mean of 69.98 in and a standard deviation of 0.3 cm\nNote this suggests the mean of our means is close to the true population value of \\(\\mu\\). But the spread of our means (their standard deviation) is much less than the spread of the actual population! How much less? Let’s consider a set of smaller samples (n = 20).\n\nsample_outcomes_2 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_2 &lt;- population_norm[sample(nrow(population_norm), 20),]\n  sample_outcomes_2$mean[i] &lt;- mean(sample_2$height)\n  sample_outcomes_2$sd[i] &lt;- sd(sample_2$height)\n  \n}\n\nggplot(sample_outcomes_2, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 20)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_2$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_2$mean)-\n                     2*sd(sample_outcomes_2$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_2$mean)+\n                     2*sd(sample_outcomes_2$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis new sample of means has a mean of 69.96 in and a standard deviation of 0.66 cm So, the estimate for \\(\\mu\\) is still close to the same, but the standard deviation of our estimates is growing.\nThis is even more clear if we sample only 5 individuals.\n\nsample_outcomes_3 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_3 &lt;- population_norm[sample(nrow(population_norm), 5),]\n  sample_outcomes_3$mean[i] &lt;- mean(sample_3$height)\n  sample_outcomes_3$sd[i] &lt;- sd(sample_3$height)\n  \n}\n\nggplot(sample_outcomes_3, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 5)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_3$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_3$mean)-\n                     2*sd(sample_outcomes_3$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_3$mean)+\n                     2*sd(sample_outcomes_3$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nwhere we find a mean of 69.97 in and a standard deviation of 1.33 cm.\nIf we facet the graphs (and let them share an x-axis) we can see this even better\n\nsample_outcomes_1$n=100\nsample_outcomes_2$n=20\nsample_outcomes_3$n=5\nsamples_all &lt;- rbind(sample_outcomes_1,sample_outcomes_2, sample_outcomes_3)\n\nggplot(samples_all, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples\",\n       color=\"Measure\") +  facet_wrap(~n, ncol=1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou can clearly see larger sample sizes lead to a more “clustered’ group of means (so there is less uncertainty in the measurements!). This is why larger estimates make us more confident in our estimates - the means we get are less likely to be far away! In other words, larger samples yield more precise estimates with lower spread (lower sampling error).\nWe call the standard deviation of our means the standard error. We can calculate this as\n\\[\n[\\sigma_{\\overline{Y}} = \\frac{\\sigma}{\\sqrt{n}}] \\sim [s_{\\overline{Y}} = \\frac{s}{\\sqrt{n}}]\n\\]\nAlso, note distribution of means was normal (which we will define even better in a few lectures!). For now, that means we can get 95% of the sample means within ~2 standard deviations of the mean of means, which is very close to the true mean. Conversely, if we use data from each sample to generate a an interval ~2 standard deviations above and below each sample mean, these intervals will contain the true mean 95% of the time. We call this range a 95% confidence interval. For example, let’s take take 20 samples of 100 individuals from our fake population, then calculate and plot their confidence intervals.\n\nnumber_of_samples &lt;- 20\nsample_outcomes &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA, se = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes$sd[i] &lt;- sd(sample_1$height)\n  sample_outcomes$se &lt;- sd(sample_1$height)/sqrt(100)\n}\nsample_outcomes$sample &lt;- as.factor(1:number_of_samples)\nggplot(sample_outcomes\n       , aes(x=sample, y=mean)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean-(2*se), ymax=mean+(2*se)))+\n  geom_hline(aes(yintercept=mean(population_norm$height))) +\n  ylab(\"Mean\")+\n  xlab(\"Sample\")+\n  ggtitle(\"Variation in error bars\")\n\n\n\n\nNotice one of samples (#2) has a range that does not include the true mean of the population!\nA few other notes about confidence intervals\n\nConfidence interval for normally-distributed samples (like those described here!) should be symmetric around the mean. This will change slightly for non-normal data (which we may address with generalized linear models that use a binomial, Poisson, or gamma distribution).\nThe “~2” is based on sample size. The value actually trends towards 1.96 at large sample sizes, but at sample sizes over ten 2 is a good estimate. You may also here this total (the 2 multiplied by the standard error) referred to as the margin of error. We could also have other numbers. For example, we could have a 90% confidence interval.\n\n\n\nWould it be wider or narrower compared to a 95% interval?\n\nIf you are less confident in the interval (90% vs 95%), the interval itself will get smaller (only 90% of samples need to have the true mean!)\n\n\nConfidence bounds also exist. These are slightly different - we’ll explain them in a few chapters.\n(more complicated) Note these calculations assumes we have lots of samples, but we typically only have one. The average probability of the first 95% CI capturing the true sample mean is only around 83%\n\n\n\n\n\n\nNeed to see this another way?\n\nThese two simulations (produced by UBC) will allow you to see this another way!\n\nRelationship between sample size and distribution of sample means for samples from a normally distributed population\nConfidence intervals\n\n\nFinally, it turns out the underlying distribution of data doesn’t matter; only that of the trait we are focused on does. For example, the means of the data will be normally distributed as long as you have a large sample size. This is know as the central limit theorem.\nTo prove this, let’s instead consider a uniform distribution:\n\npopulation_unif &lt;- data.frame(id = 1:population_size, \n                         height = runif(population_size, 60, 80))\nggplot(population_unif, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow, let’s do what we did above. First, draw a sample of 100\n\nsample_unif &lt;- population_unif[sample(nrow(population_unif), 100),]\nggplot(sample_unif, aes(height)) + \n  geom_histogram() +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of 100 random males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote the sample is still relatively uniformly distributed. This makes sense. In general, a good sample should resemble the underlying population, so this makes sense.\nNow let’s sample a 100 of these numerous times and plot the means of each sample.\n\nnumber_of_samples &lt;- 1000\nsample_outcomes_unif &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_unif &lt;- population_norm[sample(nrow(population_unif), 100),]\n  sample_outcomes_unif$mean[i] &lt;- mean(sample_unif$height)\n  sample_outcomes_unif$sd[i] &lt;- sd(sample_unif$height)\n}\nggplot(sample_outcomes_unif, aes(mean)) + \n    geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 100)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_unif$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_unif$mean)-\n                     2*sd(sample_outcomes_unif$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_unif$mean)+\n                     2*sd(sample_outcomes_unif$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice we are back to a normal distribution!\n\n\nNeed to see this another way?\n\nAnother UBC visualization will allow you to see this another way!\n\nCentral limit theorem: Sampling from non-normal distributions.\n\n\nAs you can see above, 95% confidence intervals are commonly graphed to show the potential spread of mean values. This distinction is important, as one could plot the standard deviation of the raw the data, the standard error of the related means, or the 95% confidence interval. These can be very different. As an example,let’s return to our normal population.\n\nnumber_of_samples &lt;- 1\nsample_outcomes &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA, se = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes$sd[i] &lt;- sd(sample_1$height)\n  sample_outcomes$se &lt;- sd(sample_1$height)/sqrt(100)\n}\nsample_outcomes$sample &lt;- as.factor(1:number_of_samples)\n\nsample_1$sample &lt;- \"Data\"\nsample_1$data &lt;- sample_1$height\nonese &lt;- sample_outcomes\nonese$sample &lt;- \"+- 1 standard error\"\nonese$data &lt;- onese$mean\nonese$bar_length &lt;-  onese$se\ntwosd &lt;- sample_outcomes\ntwosd$sample &lt;- \"+- 2 standard error ~ \\n 95% confidence interval\"\ntwosd$data &lt;- twosd$mean\ntwosd$bar_length &lt;-  onese$se * 2\nonesd &lt;- sample_outcomes\nonesd$sample &lt;- \"+- 1 standard deviation\"\nonesd$data &lt;- onesd$mean\nonesd$bar_length &lt;-  onese$sd\n\nexample_clarity &lt;- merge(sample_1, onese, all.x =T, all.y = T)\nexample_clarity &lt;- merge(example_clarity, twosd, all.x =T, all.y = T)\nexample_clarity &lt;- merge(example_clarity, onesd, all.x =T, all.y = T)\n\nlibrary(plyr)\nexample_clarity$sample &lt;- relevel(as.factor(example_clarity$sample), \"Data\")\n\nggplot(example_clarity\n       , aes(x=sample, y=data)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean-bar_length, ymax=mean+bar_length))+\n  labs(y =\"Height (cm)\", x= \"Frequency\",\n       title = \"Variation in error bar display\")\n\n\n\n\nNotice the differences! We will typically use 95% confidence intervals in class, but you should always specify in your captions and note when you read other papers!\nVisualizations of spread are commonly used with bar graphs (despite the earlier issues we noted with bar graphs!). For example, we can return to our iris data and add\n\nlibrary(Rmisc)\n\nLoading required package: lattice\n\nfunction_output &lt;- summarySE(iris, measurevar=\"Sepal.Length\", groupvars =\n                               c(\"Species\"))\n\nggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_col(aes(fill=Species)) +\n    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n\n\n\nIn general, presenting the estimate for a parameter and measures of sampling error (or uncertainty) allow you to state the magnitude of a statistic. This is a different but related approach to the more commonly observed p-values (which we’ll get to!). For example, for a single population we can ask if the confidence interval includes a relevant value (like 0!). For multiple groups,we can consider if the true means are in the same range by looking at overlap in confidence intervals among the groups."
  },
  {
    "objectID": "content/Estimation.html#next-steps",
    "href": "content/Estimation.html#next-steps",
    "title": "Estimation and uncertainty",
    "section": "Next steps",
    "text": "Next steps\nIn this chapter we’ve started to talk about probability. In the next we will review some probability basics before we move onto testing hypotheses."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site is a work in progress! Original .R and .rmd files from are being migrated into a new book using quarto."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Welcome",
    "section": "Welcome",
    "text": "Welcome\nThis book is meant to accompany BIO/ENV 2100:Biostatistics at Baruch College, but it should offer another perspective to anyone trying to learn statistics, R, or some combination. The class now includes\n\na website housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!\n\nAll of these resources may prove useful in learning the material.\nI say another perspective because an immediate question should be why the world needs another self-published statistics book, especially one focused on introducing R. There are already many, many good ones (some of which are shared at the end of each of relevant chapter and in the list of additional resources).To this I offer a few responses\n\nAs already noted, this book was designed to accompany courses I teach. Having the material presented in the same order, but with additional context, should help students learn the material.\nThe courses I teach focus on introducing statistics from a biological perspective, so examples, papers, and problems focus on natural systems when possible. Having examples, including from published papers, that introduce the need and use of various tests should aid in helping students learn to understand why various tests exist and when they should be used."
  },
  {
    "objectID": "index.html#here-there-be-monsters-but-also-opportunities",
    "href": "index.html#here-there-be-monsters-but-also-opportunities",
    "title": "Welcome",
    "section": "Here there be monsters, but also opportunities!",
    "text": "Here there be monsters, but also opportunities!\nStatistics is a complex field that is unfortunately often stuffed into the curriculum of other majors (see above). However, my goal is to teach the concepts while also giving students the tools to actually address questions. Given these goals,\n\nwe’ll learn how to use tools and applications including R (through Rstudio), git, and markdown. If this is your first time using any (or all) of these tools, don’t worry. We will start at the very beginning.\n\nThere are many, many ways to do any task in R. I will show you one (sometimes two) for a given concept, but note you may find other approaches online or in other material\nI typically use verbose coding (more words and lines, but easier to read and understand). I know much of what we do could be done in fewer steps, but speed is commonly less of an issue than readability (which is connected to repoducibility) for our fields\n\nGiven our focus on concepts, we will not dwell on the proofs or other mathematical components of statistics. I’m happy to point you towards texts to help with those, or discuss them.\nWe will use easy examples to illustrate concepts and applications (toy datasets), which make it easier for you to update in the future (real data are often messy!) while also connecting our class to real-life papers and ideas as much as possible\n\n\n\n\nFigure 1: Old maps rarely stated “Here there be monsters”, but mythical animals did appear on maps!\n\n\nBe warned: You may encounter some questions as we introduce new material. For example, we’ll talk about normality before fully explaining it. We’ll also use code (to make figures, for example) before you understand it. Feel free to ask questions, but you can also be sure we’ll cycle back (and expand) on many topics. I’ll also add asides/tangents throughout the book to help answer some common questions that pop up.\nHopefully this will open the door to careers in data science (a related term) and statistics to some students who haven’t considered that path before. Jobs in these fields are some of the fastest growing in the country, and the skills you learn in this class, including\n\nCritical thinking\nCoding\n\nR\nmarkdown\ngit\n\nData wrangling\nVisualization and stats\nWriting and communication\n\nwill be some of the most transferable you acquire as an undergraduate.\n\n\n\nFigure 2: Chart from Occupational Outlook Handbook showing fastest growing occupations and median pay. Data from 9.8.22. Screenshot taken 7.26.23\n\n\nI hope you find the book useful and learn to see statistics as more than something you do to finish a project or a course that you are required to take. The book is written in quarto, a derivative/extension of rmarkdown, which allows R code and prose to be easily created and published together. You can see the code for all the material on github, and you will learn early on how to make a copy of the material that you can work on yourself."
  },
  {
    "objectID": "content/additional_resources.html",
    "href": "content/additional_resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "The class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!"
  },
  {
    "objectID": "content/additional_resources.html#class-related-materials",
    "href": "content/additional_resources.html#class-related-materials",
    "title": "Additional resources",
    "section": "",
    "text": "The class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!"
  },
  {
    "objectID": "content/additional_resources.html#other-resources",
    "href": "content/additional_resources.html#other-resources",
    "title": "Additional resources",
    "section": "Other resources",
    "text": "Other resources\nAs noted in the introduction, there are many, many resources that may assist you in your quest to learn statistics and R. Relevant ones are noted throughout the book and listed here.\n“Introduction to Data Science” (n.d.)\n“Welcome | r for Data Science” (n.d.)"
  },
  {
    "objectID": "content/1b_intro_to_Rmd.html",
    "href": "content/1b_intro_to_Rmd.html",
    "title": "1b. Intro to Rmd files and literate programming",
    "section": "",
    "text": "Rmd files differ from R files in that they combine regular text with code chunks. This is a code chunk\n\nprint(\"this is a chunk\")\n\n[1] \"this is a chunk\"\n\n\nCode chunks combine code with output. When combined with regular text/prose, this makes it easier to produce a range of documents. You set the output in the YAML header (the stuff between the 3 dashes you see at top of this document).\nAfter you write the file, you Knit it to turn the Rmd file into the selected output. Try it now. Note the first time you do this in a project you may be prompted to install a number of packages! If you are using a webservice you may also need to allow pop-ups in your browser. Don’t be surprised if a new window pops up (it should).\n\n\n\nThe knit button turns your .rmd file into other products\n\n\nThe Knit button saves the .Rmd file and renders a new version whose output depends on what you selected in the header. Here we have html_document, so if everything works a preview of a webpage like document should appear. The file also produces a github friendly .md file. This means you should only edit the Rmd file (leave the md and output files alone! They are automatically produced any changes you make there will be overwritten by your next knit).\nWhen you Knit a file, it runs in a totally new R instance. this means anything you only added in your instance (like working in the console) won’t be available. In other words, its the best way to see what a “new” user gets when they use your code.\nhowever, you don’t have to knit the file every time. if you just want to see output, note you can press the green button next to an R chunk.\n\n\n\nThe green arrows just runs the chunk in the console and shows the output\n\n\n\nprint(\"this is a chunk\")\n\n[1] \"this is a chunk\"\n\n\nNow we’ll start changing the file to show you how rmarkdown works. First, amend the file by replacing the NAME and DATE spots in the header (top of the file between the — markers) with your name and the real date. Then Knit the file again. You should see your name in the new preview.\nRstudio has a Markdown Quick Reference guide (look under the help tab), but some general notes.\n\nPound/Hashtag signs denote headers\nyou can surround something double asterisks for bold or single asterisks for italics\nlists are denoted by numbers or asterisks at beginning of line (followed by space!)\n\nand can be indented for sublevels\n\nR code can be done inline, but is generally placed in stand-alone chunks\n\nthese will, by default, show the code and output\n\nlots of other options exist!\n\nThe main idea is Rmd files allow you to combine code, text, graphs, etc into multiple outputs that you can share (including with coding illiterate colleagues who just want output)."
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Many thanks to Bill Rice and Steve Gaines at UCSB for encouraging me to continue my interests in statistics.\nMy department at Baruch also supported me when I proposed the Biostatistics (ENV/BIO 2100) course in 2017 and taught for the first time in 2018.\nBaruch College’s Center for Teaching and Learning, as a channel for a statewide funding effort focuse on developed OER (open-educational resources) at CUNY and SUNY campuses, have supported the continued development of the class.\nThe class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!\n\nThis repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/Introduction.html",
    "href": "content/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "“Why is statistics a required course for someone who wants to be a dentist/doctor/ nurse?”\nThis is a common question (or at least thought) for many students. I hope to convince you this semester you at least need to understand statistics as part of the scientific method (and you should realize the scientific process informs all those jobs - in fact it can inform any job or task where you are searching for an answer or better method).\nFor example, doctors prescribe medicine to patients, but how do they know these medicines work? Some doctors carry out research, but many rely on published guidelines, which themselves rely on research. So a new drug or treatment is proposed- but who decides it’s worth using? Researchers carry out trials to determine the efficacy of the treatment. In doing this they have to consider how to design an experiment (what do they collect? from whom?) and analyze the resulting data so they can trust the results.\n\n\n\nFigure 1: XKCD: Control Group\n\n\nOther students in our class may be interested in a more environment- or resource management- focused career (e.g., wildlife rehabilitation, carbon mitigation expert, researcher). Regardless of your goal, any question should be informed by this approach. For example,\n\nDoes an environmental factor cause cancer?\nDo potential toxins really harm the enviroment?\nIs organic food really healthier?\nDoes exposing organisms reared in captivity to predator cues lead to more successful releases?\n\nZhu et al. (2023)\n\n\n\nAt its heart, statistics is about turning data into information that we can use to make decisions or better understand the world around us. Data can come from experiments we are running. This offers a clear connection to field and lab science, and its what we will focus on for most of this class. Data and theories can also be used to develop models that produce output ; this isn’t real-world data, but it offers very useful insight on what we think will happen if something occurs (and something we can test with other field data!). For example, restoration projects may focus on small-scale plots that undergo different restoration protocols. Data produced from monitoring these plots may be used to develop models to predict large-scale impacts (and maybe benefits and costs) of different restoration scenarios for larger regions.\nAbove I used words like know (how do they know these medicines work? )and predict (develop models to predict large-scale impacts (and maybe benefits and costs) of different restoration scenarios for larger regions). While we may use words like these that are related our findings interchangeably at times, its important to note the different. Statistics (and related models) generally give us estimates about how the real world works. Put another way, if we knew everything about the world, we wouldn’t need to use statistics because we wouldn’t need estimates.\nThe reasons we don’t usually know everything include\n\nthe world is complicated (some questions can’t be directly tested)\nit’s not possible to measure everything\n\nBecause of this, statistics is also focused on trying to describe populations of interest or find signals (impacts of treatments, medicines, or restoration practices, for example) amidst the noise (variation in outcomes that are always common!). When considering relationships among variables, noise may occur because there are lots of things impacting the outcome of interest. For example, restoration protocol may impact the trajectory of an oyster reef, but so too may local factors like temperature an and salinity. Noise can also occur because of sampling error - since we don’t measure everything, our estimate of relationship or population traits may be imperfect.\nIn the next session we’ll start to discuss how we can use data to make estimates about a population (and answer questions like what is a population and what are we trying to estimate). However, a final aside to finish this section - we often think about statistics happening after an experiment, survey, or other thing we get data from is finished. However, part of statistics is experimental design! Statistics should inform how you setup an experiment. In fact, the best idea (which seldom happens!) is that you simulate the type of data you expect to get from your experiment and then analyze that before you actually run the experiment. This ensures you are measuring what you need to measure and setting things up correctly! As the famous quote (to statisticians) states,\n\nTo consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of. -Ronald Fisher\n\n\n\n\n\nReferences\n\nZhu, Jennifer, J. Stephen Gosnell, Laila Akallal, and Micah Goltsman. 2023. “Fear Changes Traits and Increases Survival: A Meta-Analysis Evaluating the Efficacy of Antipredator Training in Captive-Rearing Programs.” Restoration Ecology 31 (3): e13674. https://doi.org/10.1111/rec.13674."
  }
]