[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site is a work in progress! Original .R and .rmd files from are being migrated into a new book using quarto."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Welcome",
    "section": "Welcome",
    "text": "Welcome\nThis book is meant to accompany BIO/ENV 2100:Biostatistics at Baruch College, but it should offer another perspective to anyone trying to learn statistics, R, or some combination. The class now includes\n\na website housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!\n\nAll of these resources may prove useful in learning the material.\nI say another perspective because an immediate question should be why the world needs another self-published statistics book, especially one focused on introducing R. There are already many, many good ones (some of which are shared at the end of each of relevant chapter and in the list of additional resources).To this I offer a few responses\n\nAs already noted, this book was designed to accompany courses I teach. Having the material presented in the same order, but with additional context, should help students learn the material.\nThe courses I teach focus on introducing statistics from a biological perspective, so examples, papers, and problems focus on natural systems when possible. Having examples, including from published papers, that introduce the need and use of various tests should aid in helping students learn\n\nwhy various tests exist\nhow they relate to each other\nwhen one should be used as opposed to another\nhow to defend the choices you made or evaluate those of others!"
  },
  {
    "objectID": "index.html#here-there-be-monsters-but-also-opportunities",
    "href": "index.html#here-there-be-monsters-but-also-opportunities",
    "title": "Welcome",
    "section": "Here there be monsters, but also opportunities!",
    "text": "Here there be monsters, but also opportunities!\nStatistics is a complex field that is unfortunately often stuffed into the curriculum of other majors (see above). However, my goal is to teach the concepts while also giving students the tools to actually address questions. Given these goals,\n\nwe’ll learn how to use tools and applications including R (through Rstudio), git, and markdown. If this is your first time using any (or all) of these tools, don’t worry. We will start at the very beginning.\n\nThere are many, many ways to do any task in R. I will show you one (sometimes two) for a given concept, but note you may find other approaches online or in other material\nI typically use verbose coding (more words and lines, but easier to read and understand). I know much of what we do could be done in fewer steps, but speed is commonly less of an issue than readability (which is connected to repoducibility) for our fields\n\nGiven our focus on concepts, we will not dwell on the proofs or other mathematical components of statistics. I’m happy to point you towards texts to help with those, or discuss them.\nWe will use easy examples to illustrate concepts and applications (toy datasets), which make it easier for you to update in the future (real data are often messy!) while also connecting our class to real-life papers and ideas as much as possible\n\n\n\n\nFigure 1: Old maps rarely stated “Here there be monsters”, but mythical animals did appear on maps!\n\n\nBe warned: You may encounter some questions as we introduce new material. For example, we’ll talk about normality before fully explaining it. We’ll also use code (to make figures, for example) before you understand it. Feel free to ask questions, but you can also be sure we’ll cycle back (and expand) on many topics. I’ll also add asides/tangents throughout the book to help answer some common questions that pop up.\nHopefully this will open the door to careers in data science (a related term) and statistics to some students who haven’t considered that path before. Jobs in these fields are some of the fastest growing in the country, and the skills you learn in this class, including\n\nCritical thinking\nCoding\n\nR\nmarkdown\ngit\n\nData wrangling\nVisualization and stats\nWriting and communication\n\nwill be some of the most transferable you acquire as an undergraduate.\n\n\n\nFigure 2: Chart from Occupational Outlook Handbook showing fastest growing occupations and median pay. Data from 9.8.22. Screenshot taken 7.26.23\n\n\nI hope you find the book useful and learn to see statistics as more than something you do to finish a project or a course that you are required to take. The book is written in quarto, a derivative/extension of rmarkdown, which allows R code and prose to be easily created and published together. You can see the code for all the material on github, and you will learn early on how to make a copy of the material that you can work on yourself."
  },
  {
    "objectID": "content/summarizing_data.html",
    "href": "content/summarizing_data.html",
    "title": "Summarizing data",
    "section": "",
    "text": "Figure 1: XKCD: Data Trap. It’s important to make sure your analysis destroys as much information as it produces.\nOnce we have some data, the next step is often to summarize it. In fact, we’ve already done that in some ways. Some statistics like the mean may be considered a summary of the data. This may be useful because we prefer large datasets (remember good sampling!), but making sense of a list of numbers can be really hard! Summaries help us describe, and eventually compare, datasets, which we are using to infer something about a population.\nThink about it this way. We want to know if several species of iris (Iris versicolor, setosa and virginica) have similarly-shaped flowers. Since we can’t measure every flower on every plant from these species, we sample several sites and come up with the following data (using R’s built-in iris dataset, a dataset we will often use).\niris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\nOverwhelming, isn’t it? And this isn’t a huge dataset! There are only 150 rows, yet some datasets have tens of thousands!\nLet’s just look at the first few rows of the data\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\nIt’s really hard (or impossible) to just look at these numbers and infer anything about the population. Summary statistics help us get a better mental image of the distribution of the sample data.\nIt’s really hard (or impossible) to just look at these numbers and infer anything about the population. Summary statistics help us get a better mental image of the distribution of the sample data."
  },
  {
    "objectID": "content/summarizing_data.html#types-of-data",
    "href": "content/summarizing_data.html#types-of-data",
    "title": "Summarizing data",
    "section": "Types of data",
    "text": "Types of data\nWe can summarize data using visual (i.e., graphs) or numerical (e.g., summary statistics like the mean) approaches. The specific way we summarize the data also depends on the type of data. Note, the trait we are collecting data on may also be called a variable (since it varies across the population and thus sample).\n\nCategorical variables\nVariables can be categorical (e.g., eye color). If categorical variables have no clear hierarchical relationship (again, like eye color - one isn’t better than the other), then they are nominal variables. If the categories imply a rank or order (e.g., freshmen, sophomore, junior, senior; egg, larvae, pupae, adult) then they are ordinal variables).\n\n\nNumeric variables\nIf data values are based on numbers instead of categories, they are numeric variables. These can be divided into those are count-based (no fractions) - we call these discrete data- and those that can take on any values in a given range - like height. We call these continuous variables."
  },
  {
    "objectID": "content/summarizing_data.html#graphical-summaries",
    "href": "content/summarizing_data.html#graphical-summaries",
    "title": "Summarizing data",
    "section": "Graphical summaries",
    "text": "Graphical summaries\nVisual interpretations or displays of your data are an excellent way to let patterns, trends, and distributions easier to see. In this section we’ll go over a number of graphs. Consider this is a resource. I don’t expect you to know how to make each of these on your own immediately. We will actually introduce the software we are using to make these in later sections. Instead, you can return here later when you are actually making a graph for ideas (and code!). For your first read, focus on the images (not the code!)\nWhile the type of graph you should use will depend on the data (and you may have several options!) all graphs should have\n\nDescriptive title\n\nMove beyond Y vs. X. State any patterns you see in the title to help the viewer know what they are looking for! Honest interpretation of data is always paramount, but in producing a graph you will already be making visualization decisions.\n\nLabeled axes (measure and unit)\n\nWhat did you measure, and using what (e.g. Sepal length (cm)\n\nData points\n\nOther parts should only be included when needed.\n\nLegends\n\nOnly needed for graphs with multiple datasets where color, shape, or some other visual cue indicates something to the viewer.\n\nTrendlines\n\nCan be used to show the general/overall relationship between variables. If you use these, make sure to use the right ones! Don’t fit a straight line to a curved relationship!\n\n\n\nSingle variable\n\nNumerical data\n\nHistograms\nOccasionally you only want to show the distribution for a single numerical variable (or how the data themselves are distributed). For example, we could want to display sepal lengths for all the Iris virginica we sampled. We could do this using a histogram.\n\nlabel_size &lt;- 2\ntitle_size &lt;-2.5\nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"], \n     main = expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))), \n     xlab = \"Sepal Length (cm)\", \n     cex.lab=label_size, cex.axis=label_size, cex.main=title_size, \n     cex.sub=label_size, col = \"blue\")\n\n\n\n\nFigure 4: Example of approximately normal data\n\n\n\n\nThe above plot is produced using functions available in all R installs. Many plots now use ggplot2, a package you have to install (don’t worry we’ll get there!). However, since you may come back to this later, I’ll also show how to make each of these graphs using ggplot2.\n\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5: Example of approximately normal data\n\n\n\n\nHistograms put the data in bins (usually automatically set by software, but you can update!) and then show the number of samples that fell into each bin. This allows a quick estimate (look at the y, or vertical, axis) of how many samples were taken. The above images also allows us to begin to consider the bounds/range of the data (~4.5-8 cm), which gives information on the minimum and maximum values. We can also see lengths around 6-7 cm are most common.\n\n\nWhy do these graphs look slightly different? (Click the grey triangle to see the answer\n\nMost programs, including R, have autobreak functions to separate the data into bins. Notice ggplot2 uses a different algorithm to bin the data. That also impacts what you see! Users, however, can override these, so it’s worth noting that differences in bin size can influence what distributions look like.\n\nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],       main = expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),       xlab = \"Sepal Length (cm)\",       cex.lab=label_size, cex.axis=label_size, cex.main=title_size,       cex.sub=label_size, col = \"blue\") \nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],        breaks=3, main = \"Sepal length histogram, 3 breaks\", xlab = \"Sepal Length (cm)\", cex.lab=label_size, cex.axis=label_size, cex.main=title_size, cex.sub=label_size, col = \"blue\")  \nhist(iris[iris$Species == \"virginica\", \"Sepal.Length\"],        breaks=10, main = \"Sepal length histogram, 10 breaks\", xlab = \"Sepal Length (cm)\", cex.lab=label_size, cex.axis=label_size, cex.main=title_size, cex.sub=label_size, col = \"blue\")\n\n\n\n\nFigure 6: ?(caption)\n\n\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\n\n\nFigure 8: ?(caption)\n\n\n\n\nA similar issue exists for qualitative data in regards to the categories that are combined/used.\n\nThis distribution of this data is approximately normal. We will define normality more later (equations!), but for now note the distribution is roughly symmetric, with tails on either side. Values near the middle of the range are more common, with the chance of getting smaller or larger values declining at an increasing rate…\nComparing the above graph to other distributions may be an easier approach. Consider these graphs.\n\ncardinals &lt;- round(rbeta(10000,10,2)*50+runif(10000,5,10),3)\nhist(cardinals, main=\"Weight of Westchester cardinals\", xlab = \"\\n Weight (g)\", ylab = \"Frequency (#)\\n\", col = \"red\", cex.lab=label_size, cex.axis=1.25, cex.main=title_size, cex.sub=label_size)\n\n\n\n\nFigure 9: Example of left-skewed data (plot)\n\n\n\n\n\nggplot(data.frame(cardinals), \n       aes(x=cardinals)) +\n  geom_histogram( fill=\"red\", color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 10: Example of left-skewed data (ggplot2)\n\n\n\n\n\nparrots&lt;- round(c(rnorm(1000,400,10)),3)\nggplot(data.frame(parrots), \n       aes(x=parrots)) +\n  geom_histogram( fill=\"green\", color=\"black\") +\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 11: Example of normal data\n\n\n\n\n\nblue_jays &lt;- round(rbeta(10000,2,8)*100+runif(10000,60,80),3)\nggplot(data.frame(blue_jays), \n       aes(x=blue_jays)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=\"Weight of Westchester blue jays\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 12: Example of right-skewed data\n\n\n\n\nThe cardinal Figure 10 data has a longer left tail and is not symmetric. We call this left- or negatively-skewed data (since it’s going lower on the x-axis). Compare that to the blue jay Figure 12 data; it has a longer right-tail and is positively- or right-skewed. Again, note this is all relative to symmetric data like you see with the parrots Figure 11, which is normally-distributed data.\nAll symmetric data is not normal, however. Look at the data on robin and woodpecker weights.\n\nrochester &lt;- round(c(runif(1000,75,85)),3)\nggplot(data.frame(rochester), \n       aes(x=rochester)) +\n  geom_histogram( fill=\"pink\", color=\"black\") +\n  labs(title=\"Weight of Rochester robins\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 13: Example of uniform data\n\n\n\n\n\nwoodpeckers &lt;- round(c(rnorm(100,60,4),rnorm(100,80,4)),3)\nggplot(data.frame(woodpeckers), \n       aes(x=woodpeckers)) +\n  geom_histogram( fill=\"orange\", color=\"black\") +\n  labs(title=\"Weight of  Westchester woodpeckers\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 14: Example of bimodal data\n\n\n\n\nBoth these are roughly symmetric but clearly different from normally-distributed data (we will return to the woodpecker data!). The robin data is what we call uniformly distributed. There are really no tails, as it appears you are just as likely to see any number within the bounds as any other. Kurtosis is the statistical term for what proportion of the data points are in the tails. High kurtosis distributions have heavy tails with multiple outliers. The uniform distibution is an example of a low kurtosis distribution (it has no tails!).\nThis figure may also help.\n\n\n\nFigure 15: English: Plot of several symmetric unimodal probability densities with unit variance. From highest to lowest peak: red, kurtosis 3, Laplace (D)ouble exponential distribution; orange, kurtosis 2, hyperbolic (S)ecant distribution; green, kurtosis 1.2, (L)ogistic distribution; black, kurtosis 0, (N)ormal distribution; cyan, kurtosis −0.593762…, raised (C)osine distribution; blue, kurtosis −1, (W)igner semicircle distribution; magenta, kurtosis −1.2, (U)niform distribution.\n\n\nIf we consider the normal distribution (shown in black) to have 0 kurtosis, the uniform (pink) has less, and the double-exponential (red) has more.\nFigure 15\nFinally, the woodpecker data is what we call bimodal. It is symmetric in this case (not always true!), but it has a two clear peaks instead of a single central or skewed high point in the distribution.\nThese distributions helps us think about what we would expect to find in future samples (remember, we assume we have good samples!). To think about future sampling, we can change our y-axis from what we saw (frequency) to a probability density.\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram(aes(y = ..density..),fill=\"blue\", color=\"black\") +\n  geom_density()+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 16: Probability density distribution\n\n\n\n\nThese probability density distributions can be calculated from data (as seen above), but they can also be developed from equations. The benefits of using a distribution derived from an equation is that it is consistent and easy to describe (standardized). This is why many common tests we will learn rely upon the data (or some derivative of it) following a known distribution. For example, many parametric tests will rely upon the data (or means of the data, or errors…we’ll get there) following a normal distribution. We can see our parrot data (which came from a normal distribution!) is very close to a “perfect” normal distribution as define by an equation.\n\nparrots_df &lt;- data.frame(parrots)\ncolors &lt;- c(\"PDF from data\" = \"black\", \"normal curve\" = \"red\")\nggplot(parrots_df, \n       aes(x=parrots)) +\n  geom_histogram(aes(y = ..density..),fill=\"green\", color=\"black\") +\n  geom_density(aes(color=\"PDF from data\"))+\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Density\",\n       color=\"Source\")+\nstat_function(fun = dnorm, args = list(mean = mean(parrots_df$parrots), sd = sd(parrots_df$parrots)), aes(color=\"normal curve\"))+\n      scale_color_manual(values = colors)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 17: Comparing the distribution of the data to a perfect normal distribution\n\n\n\n\n\n\nBonus question: Why isn’t it perfect? (Click the grey triangle to see the answer!)\n\nThis is an easy example of sampling error!\n\n\n\nBox plots (aka, box and whisker plots)\nAnother way to visualize the distribution of numerical data for a single group is using box-and-whisker plots.\n\nggplot(iris[iris$Species == \"virginica\",],\n            aes(x=Species,y=Sepal.Length)) + geom_boxplot(size = 3) +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"\",\n       y= \"Sepal Length (cm)\")+\n  theme(axis.text.x = element_text(size=0))\n\n\n\n\nFigure 18: Example of approximately normal data\n\n\n\n\nThese plots show the values of the quartiles of the data. In this way they start combining numerical summaries (more to come!) and visual summaries. More to come, but for now imagine you had a 99 data points. If you arrange the data points from smallest to largest, the median of the data would be the middle (50th data point). If you took the bottom half of the data (first data to median), the first quartile would be the middle point (or, in this case, the average of the 25th and 26th data points). Similarly, the third quartile is the middle of the top half of the data set (or, if not one number, average of 75th and 76th data point). Note the median is also the 2nd quartile of the data!\nThe box in the box-and-whisker plot shows the first, second, and third quartiles, also known as the inter-quartile range (IQR). The whiskers extend to the minimum and maximum values of the dataset or, up to values within a set range. In ggplot, whiskers by default can only be as long as 150% of the IQR. This means extreme outliers are shown as individual dots. Typically, the most extreme values (minimum and maximum) plus the first, second, and third quartiles are together called the five number summary.\n\n“Easy” examples of five number summaries\n\nAssume we have data that goes from 1 to 99. The five number summary should be\n\nx &lt;- seq(1:1:99) \nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    25.5    50.0    50.0    74.5    99.0 \n\n\nNote the 1st and 3rd quartiles are averaged!\nSimilarly, consider the numbers 1-5\n\nx &lt;- seq(1:1:5) \nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       2       3       3       4       5 \n\n\n\n\n\n\nCategorical data\nFor categorical data, a bar chart fills a very similar role. Note, however, we don’t bin the data., and there is inherent order for some examples (nominal data). For example, we could examine the colors of our I. virginica. To do this, we’ll need to add some data to our iris data (notice this produces no output…)…\n\nset.seed(19)\ncolors &lt;- c(\"blue\", \"orange\", \"purple\")\niris$Color &lt;- factor(sample(colors, size = nrow(iris),replace = T))\n\nand then summarize it…\n\nlibrary(Rmisc)\n\nLoading required package: lattice\n\n\nLoading required package: plyr\n\nI_viriginica_colors &lt;- summarySE(iris[iris$Species == \"virginica\",], measurevar = \"Sepal.Length\",\n                                 groupvars = \"Color\", na.rm = T)\n\nbefore we graph it.\n\nbarplot(I_viriginica_colors$N, \n        names.arg = I_viriginica_colors$Color, \n        xlab=\"Colors\",\n        ylab=\"Frequency\",\n        cex.lab=label_size, cex.axis=label_size, \n        cex.main=title_size, cex.sub=label_size, \n        main = expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")))\n\n\n\n\nFigure 19: Distribution of flower colors\n\n\n\n\nOr better\n\nbarplot(I_viriginica_colors$N, \n        names.arg = I_viriginica_colors$Color, \n        cex.lab=label_size, cex.axis=label_size, \n        cex.main=title_size, cex.sub=label_size, \n        main = expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n        xlab=\"Colors\",\n        ylab=\"Frequency\",\n        col = colors)\n\n\n\n\nFigure 20: Distribution of flower colors (plot)\n\n\n\n\nUsing ggplot2\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Color,fill=Color)) +\n  geom_bar()+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Colors\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 21: Distribution of flower colors (ggplot2)\n\n\n\n\nNote the legend may be superflous here (but consider accessiblity - should we add another distinguishing feature?):\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Color,fill=Color)) +\n  geom_bar()+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Colors\",\n       y= \"Frequency\")+\n  guides(fill = \"none\")\n\n\n\n\nFigure 22: Let colors match traits if possible, but note everyone can’t see colors and sometimes they are not printed.\n\n\n\n\n\n\n**Barchart issues**\n\nNote all of the bar graphs above share a similar problem. People tend to like bars, but they are actually just using a lot of ink! We could get the same information about sepal lengths focusing on just the “top” of the bar:\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_freqpoly(color=\"blue\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 23: Note you only really know the tops of the bar!\n\n\n\n\nWe can also just display the data!\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Species, y=Sepal.Length)) +\n  geom_point(color=\"blue\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")+\n  theme(axis.text.x = element_text(size=0))\n\n\n\n\nFigure 24: Displaying the data may be the easiest option for small-ish datasets.\n\n\n\n\n\n\n\n\nMultiple variables\nOften we collect multiple pieces of information instead of just one. This can occur for multiple reasons. We may want to consider differences in some variable/trait among groups. This means we have either numerical or categorical data from various groups, but note that groups themselves are now a piece of data! We can think of these analyses as impact of group (a category) on traits (numerical or categorical). We will eventually call these a t-test or ANOVA (when the trait we measure is categorical) ota \\(\\chi^2\\) test (when the trait is categorical). Either way, this is a case where we are collecting a single piece of data from multiple groups. Alternatively, we may collect data on multiple traits from a single group to see how they impact each other. We will eventually analyze this type of data using regression or correlation. Regardless of type, we can also graph this data.\n\nNumerical variables from multiple groups\nWhen we gather numerical data from various groups and wish to compare, we can extend our use bar charts and box-whisker plots by using shapes, colors, or other features to symbolize the groups. For example, we can illustrate the mean (coming up in numerical summaries) or other summary statistics using bar plots..\n\nggplot(iris, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n{#fig-bar_charts_all species width=672}\n\n\nor the distribution using stacked histograms…\n\nggplot(iris, aes(x=Sepal.Length)) +     geom_histogram(aes(fill=Species))+    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),        y= \"Sepal length (cm)\",        x= \"Species\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n{#fig-stacked_histograms_all species width=672}\n\n\nor box-and-whisker plots.\n\nggplot(iris, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_boxplot(aes(fill=Species))+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),        y= \"Sepal length (cm)\", x= \"Species\")\n\n{#fig-box_whisker_all species width=672}\n\n\nWe can also still just display the data for each group…\n\nggplot(iris, aes(y=Sepal.Length, x=Species, color=Species)) +\n  geom_jitter() +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n{#fig-point_all species width=672}\n\n\nWe also need to ensure the different groups are visible when distributions overlap. Sometimes stacked histograms (and similar graphs) make it hard to actually visualize each individual group. One option is to instead facet these graphs. Faceting means we produce different graphs for each group, treatment, etc, but they (typically) share axes. This makes it easier to compare the groups.\n\n ggplot(iris, aes(x=Sepal.Length)) + \n   geom_histogram(aes(fill=Species))+ \n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")+\n   facet_wrap(~Species, ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n{#fig-faceted_histograms_all species width=672}\n\n\nAnother option is to show the cumulative frequency distribution for each group.\n\nggplot(iris, aes(Sepal.Length, colour = Species)) + stat_ecdf()+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Cumulative frequency\")\n\n\n\n\nFigure 25: Cumulative frequency distributions can be useful in noting exactly where distributions diverge\n\n\n\n\n\n\nCategorical data from multiple groups\nFor our example, let’s return to our focus on the color of flowers for various species of iris. One option for this is to consider bar plots. These can be stacked…\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 26: Bar plots are stacked by default and count the number of rows found in each category\n\n\n\n\nor not…\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position = position_dodge(width=0.5))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 27: Bar plots can also be grouped by adding the position_dodge argument\n\n\n\n\nOther options include divergent plots, but those are best for 2 groups of data. They also require the data to be summarized and somewhat transformed. For example, we could have blue or not blue flowers.\n\nlibrary(plyr)\niris$blue &lt;- revalue(iris$Color, c(\"blue\"=\"blue\", \"purple\"=\"not blue\", \"orange\"=\"not blue\"))\n\nThen we have to summarize the data.\n\niris_summary &lt;- data.frame(table(iris$blue, iris$Species))\nnames(iris_summary) &lt;- c(\"Blue\", \"Species\", \"Frequency\")\n\nand make not blue negative\n\niris_summary[iris_summary$Blue == \"not blue\", \"Frequency\"] &lt;- iris_summary[iris_summary$Blue == \"not blue\", \"Frequency\"] * -1\n\nthen plot it.\n\nggplot(iris_summary,aes(x=Species, y=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 28: Divergent plots show how 2 categories differ among groups\n\n\n\n\nwhich we could flip by reversing all x/y arguments..\n\nggplot(iris_summary,aes(y=Species, x=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       y= \"Species\",\n       x= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\"))\n\n\n\n\nFigure 29: Reorienting graphs may help viewers better visualize differnces\n\n\n\n\nor using an additional argument (remember, a lot of this is for later reference!)\n\nggplot(iris_summary,aes(x=Species, y=Frequency)) +\n  geom_bar(aes(fill=Blue), stat=\"identity\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"not blue\" = \"orange\", \"purple\" = \"purple\")) +\n  coord_flip()\n\n\n\n\nFigure 30: Note we get the same results by simply adding the argument coord_flip\n\n\n\n\n\n\ngeom_bar vs geom_col\n\ngeom_bar and geom_col are very similar commands, but geom_bar assumes its needs to do something to the data (like count it) by default, whereas geom_col assumes the data are summarized/ready to plot as is. The extra argument stat=identity above can usually make geom_bar behave like geom_col.\n\nIn the above cases, each group was measured the same number of times. However, if this isn’t true, visualizations may confound sampling size with summaries. In those cases, focusing on proportion (explained below!) of outcomes may be more useful (and will give you the exact same visualization if all groups were measured the same number of times!). This is sometimes called a mosaic plot; another way to make them (not shown here) is using the package ggmosaic.\n\nggplot(iris,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position = \"fill\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Proportion\")+\n  scale_fill_manual(\"legend\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\"))+\n  guides(fill = \"none\")\n\n\n\n\nFigure 31: For proportion-based visualizations, stacked bar plots may be easier to read than grouped. We just add the position=fill argument to make these.\n\n\n\n\nNote we could also facet this data if we had other variables. For example, assume sampled another set of populations to the west..\n\niris_new &lt;- iris\ncolors &lt;- c(\"pink\", \"orange\", \"yellow\")\niris_new$Color &lt;- factor(sample(colors, size = nrow(iris),replace = T))\niris_both &lt;- rbind(iris,iris_new)\niris_both$Population &lt;- factor(c(rep(\"East\",nrow(iris)), rep(\"West\", nrow(iris_new))))\n\n\nggplot(iris_both,aes(x=Species)) +\n  geom_bar(aes(fill=Color))+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_wrap(~Population, nrow=1)\n\n\n\n\nFigure 32: Faceting can make patterns easier to compare.\n\n\n\n\nNote we can combined these ideas!\n\nggplot(iris_both,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position=\"fill\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_wrap(~Population, nrow=1)\n\n\n\n\nFigure 33: We can add facets and proportions.\n\n\n\n\nFinally, we can end this section noting a pie chart is just a transformed bar chart.\n\niris_both$Share &lt;- \"\"\nggplot(iris_both,aes(x=Share)) +\n  geom_bar(aes(fill=Color), position=\"fill\")+\n  labs(title=expression(paste(\"Distribution of flower colors differ among populations of \",italic(\"I. species \"))),\n       y=\"\", \n       x=\"\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_grid(Population~Species) +\n coord_polar(theta=\"y\") \n\n\n\n\nFigure 34: We can add facets and proportions.\n\n\n\n\n\n\nRelationships among data from a single group\nInstead of collecting data on a single trait from multiple groups, we may collect data on multiple traits from a single group. For example, we could want to see if petal length is related to sepal width in I.virginica. This relationship could be visually summarized using a scatter plot.\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length, y=Petal.Length)) +\n  geom_point() +\n  labs(title=expression(paste(\"Larger sepals means larger petals in \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Petal Length (cm)\")\n\n\n\n\nFigure 35: Scatter plots show relationships among numerical variables.\n\n\n\n\nObviously we can (and will) combine many of the above approaches. For example, we may want to see if relationships among two numerical variables differ among groups (an ANCOVA!).\n\nggplot(iris,\n              aes(x=Sepal.Length, y=Petal.Length, color=Species)) +\n  geom_point() +\n  labs(title=expression(paste(\"Larger sepals means larger petals in \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Petal Length (cm)\")\n\n\n\n\nFigure 36: We can add facets and proportions\n\n\n\n\nWe’ll get to these later in class, but I just want to note their existence here. Finally, if you are reading this for the first time, don’t worry about the tests (just like the code!). We will explain how all these tests are related when we get there!\nFinally, note data of this type may include time or dates. We’ll use a different dataset to illustrate this.\n\nairquality$Date &lt;-as.Date(paste(airquality$Month, airquality$Day,\"1973\", sep=\"/\"), format =\"%m/%d/%Y\")\n\nggplot(airquality, aes(x=Date,y =Temp)) + \n  geom_point(col = \"orange\") + \n  labs(title=\"Temperature over time\", \n       x= \"Date\",\n       y= expression(\"Temperature \" ( degree*F)))\n\n\n\n\nFigure 37: Scatter plots can also include temporal data\n\n\n\n\nWe can also add lines…\n\nggplot(airquality, aes(x=Date,y =Temp)) + \n  geom_point(col = \"orange\") + \n  geom_line()+\n  labs(title=\"Temperature over time\", \n       x= \"Date\",\n       y= expression(\"Temperature \" ( degree*F)))\n\n\n\n\nFigure 38: Scatter plots can also include lines\n\n\n\n\nWe can even include multiple data sets!\n\nggplot(airquality, aes(x =Date,y =Temp)) + geom_point(aes(col =\"Temp\")) + geom_line(col=\"orange\") + geom_point(aes(y=Wind+50, col = \"Wind speed\")) + scale_y_continuous(sec.axis = sec_axis(~.-50, name = \"Wind (mph)\")) + geom_line(aes(y=Wind+50))+\n     labs(title=\"Environmental measurements over time\", \n       x= \"Date\",\n       y= expression(\"Temperature \" ( degree*F)))\n\n\n\n\nFigure 39: Scatter plots can also include multiple lines\n\n\n\n\n\n\nThere’s more to do and think about!\nThis just scratches the surface of potential ways to visualize data. For example, heatmaps can be used to show location specific data and we can build interactive or animated visualizations. However, the basic principles we’ve examined here should get you started.\nThe different approaches covered here also indicate there a lots way to display data! Whichever approach you use, you should ensure that you represent the data honestly and clearly. Sometimes that means you just display data (points)! You should also always consider possible ways the data/visualization could be misinterpreted and avoid them. Common mistakes include the decision about which baseline should be included. For example, should charts always include 0 on the y-axis? Not including 0 can may small differences appear large. However, including it can make important changes seem insignificant! Consider\n\nsmall_difference &lt;- data.frame(Treatment = c(\"a\",\"b\"), mean=c(37,40))\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.2.3\n\n\n\nAttaching package: 'ggpubr'\n\n\nThe following object is masked from 'package:plyr':\n\n    mutate\n\nbp &lt;- ggplot(small_difference, aes(x=Treatment, y=mean, fill=Treatment))+\n  geom_bar(stat=\"identity\")\nsp &lt;- ggplot(small_difference, aes(x=Treatment, y=mean, color=Treatment))+\n  geom_point()\ncompare &lt;- ggarrange(bp, sp, labels = c(\"A\", \"B\"),\n          ncol = 2, nrow = 1,common.legend = TRUE, legend=\"bottom\")\nannotate_figure(compare,\n                top = text_grob(\"Including a zero point can make a big difference!\", color = \"red\", face = \"bold\", size = 14))\n\n\n\n\nFigure 40: The decision of where to start the y-axis can have major impacts on interpretation\n\n\n\n\nIf this is changes in temperature, option B may be more useful (this could be a normal temperature (37 C) compared to a fever of 104 (40 C). However, if its a difference in a metabolic rate, it may have minor impacts (and thus we choose option A)!\nSimilarly, imagine we collected this data\n\ngood_fit_x &lt;- runif(100, 1, 50) \ngood_fit_y &lt;- rnorm(100,25,2) \ngood_data &lt;- data.frame(source = \"good\", x=good_fit_x, y=good_fit_y) \nbad_fit_x &lt;- runif(10, 20, 30) \nbad_fit_y &lt;- rnorm(10,95,1) \nbad_data &lt;- data.frame(source = \"outlier\", x=bad_fit_x, y=bad_fit_y) \nall_data &lt;- rbind (good_data, bad_data)\n\npoints &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_point(aes(color=source)) + \n  labs(title=\"Raw data\")\n\npoints_plus_curve &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_point(aes(color=source)) + \n  geom_smooth(se = F) + \n    labs(title=\"Curve fit to data but points shown\")\n\ncurve &lt;- ggplot(all_data, aes(x =x,y =y)) + geom_smooth(se = F) +\n    labs(title=\"Only curve\")\ncompare &lt;- ggarrange(points, points_plus_curve, curve, labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2, common.legend = TRUE, legend=\"bottom\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nannotate_figure(compare,\n                top = text_grob(\"Same data, three visualizations\", color = \"red\", face = \"bold\", size = 14))\n\n\n\n\nFigure 41: This would likely indicate\n\n\n\n\nWhereas the raw data (panel A) may suggest some outliers that are concerning, by panel C we have “smoothed” the data and made an interesting pattern. In general, thought must be applied to individual situations regarding visualization style and nuance. Adding information on spread in the data will also help (coming up!)."
  },
  {
    "objectID": "content/summarizing_data.html#numerical-summaries",
    "href": "content/summarizing_data.html#numerical-summaries",
    "title": "Summarizing data",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\nWhile visual summaries give us a clearer picture of the data, numerical summaries can help distill a large dataset into several components that can then be analyzed or compared. A key point is we are rarely trying to say if 2 groups are exactly the same or if a trait value is exactly equal to something. Given sampling error, we know its unlikely we would get exactly the same values, and, more importantly, its really rare for 2 groups to be exactly the same. Instead, we are often comparing characteristics of the population data among group or to set values.\n\nCentral tendency\nOne common characteristic of a population is central tendency. Central tendency considers what are common values in a dataset by focusing on the center of the distribution. Mean, or the average or \\(\\mu\\) , is one measure of central tendency. Due to sampling error, we don’t know \\(\\mu\\), but we can estimate it. In general, we use Greek letters to denote the population values and standard(Latin) letters typically denote our estimate (sometimes with added symbols). For example, if we have n data points, our estimate of the mean \\(\\mu\\) is known as \\(\\overline{Y}\\) (read as “y-bar”) is\n\\[\n\\overline{Y} = \\frac{\\sum_{i=1}^{n} n_{i}}{n} \\sim \\mu\n\\]\nwhere \\(\\sim\\) means “approximately”. Other measures of central tendency include the mode (most common data point) or median (middle data point if all data were placed in ascending order (remember box plots!)).\nWhy do we need more than one measure of central tendency? Consider our cardinal data:\n\n# function to calculate mode\nfun.mode&lt;-function(x){as.numeric(names(sort(-table(x)))[1])}\n\nggplot(data.frame(cardinals), \n       aes(x=cardinals)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(cardinals), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(cardinals), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(cardinals), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 42: Skew impacts value of and relationships among various measures of central tendency\n\n\n\n\nNote the data is left-skewed. so the mean is pulled towards these outliers. The median may offer a better summary of the actual center. We see similar outcomes with right-skewed data.\n\nggplot(data.frame(blue_jays), \n       aes(x=blue_jays)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester blue jays\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(blue_jays), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(blue_jays), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(blue_jays), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 43: Skew impacts value of and relationships among various measures of central tendency\n\n\n\n\nBut with symmetric data, we see the measures of central tendency align more\n\nggplot(data.frame(parrots), \n       aes(x=parrots)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Weight of Westchester parrots\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=mean(parrots), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(parrots), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(parrots), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 44: For symmetric data, measures of central tendency are more aligned.\n\n\n\n\n\n\nWhy is the mode not always on the highest bar?\n\nNote the mode is heavily/totally impacted by data precision and thus can lead to unusual matches with histograms. In our above example, cardinals were measured to 10-3 grams. However, the data was binned to levels of 10-2 grams. Due to this mismatch, the most common measurement of the raw data was 51.155, which occurred 6 times. However, more data points still fell in another bin!\n\nA special case where central tendency may not be the best way to describe a distribution is known as bimodal data. In this case, the distribution shows two, not one, clear peak. Remember the woodpecker Figure 14 data?\n\nggplot(data.frame(woodpeckers), \n       aes(x=woodpeckers)) +\n  geom_histogram(aes(y=..density..), color=\"black\") +\n  geom_density()+\n  labs(title=\"Weight of  Westchester woodpeckers\",\n       x= \"Weight (g)\",\n       y= \"Proportion\")+\n    theme_bw()+\n  geom_vline(aes(xintercept=mean(woodpeckers), color=\"mean\"))+\n  geom_vline(aes(xintercept=median(woodpeckers), color= \"median\"))+\n  geom_vline(aes(xintercept=fun.mode(woodpeckers), color = \"mode\")) +\n  theme_bw()+theme(legend.position=\"bottom\")+\n    guides(color = guide_legend(title = \"Measure\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 45: Example of bimodal data with various measures\n\n\n\n\nNote the mode is (again) off, but the mean and median also represent uncommon individuals!\n\n\nSpread\nAlong with the central tendency, another set of numerical summaries focus on the spread of the data. In some ways these focus on how much of the data is in the tail (or how heavy the tail is). To illustrate this, consider the Figure 5 data.\n\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 46: Remember this example of approximately normal data?\n\n\n\n\nWe can instead plot the raw data\n\niris$sample &lt;- 1:nrow(iris)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point() +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")\n\n\n\n\nFigure 47: Note x-axis is just sample order!\n\n\n\n\nNow let’s add the mean\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point(color='blue') +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n  annotate(\"text\", label = \"mean\", x = 135, y = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") \n\n\n\n\nFigure 48: Mean shown in blue!\n\n\n\n\nNote the points differ in how far they are from the mean! For example, we could find another species with a very similar mean but different spread of the data.\n\niris_new &lt;- data.frame(Sepal.Length = runif(50,min=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])-.25, max=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.25), Species = \"uniforma\", sample=101:150)\niris_hypothetical &lt;- merge(iris, iris_new, all = T)\nggplot(iris_hypothetical[iris_hypothetical$Species %in% c(\"virginica\", \"uniforma\"),],\n              aes(y=Sepal.Length, x=sample, color=Species))+\n  geom_point() +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" and \", italic(\"uniforma \"), \"arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n    geom_hline(aes(yintercept=mean(iris_hypothetical[iris_hypothetical$Species == \"uniforma\", \"Sepal.Length\"])), color = \"red\")+\n  annotate(\"text\", label = \"I. virginica mean\", x = 135, y = mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") +\n  annotate(\"text\", label = \"I. uniforma mean\", x = 135, y = mean(iris_hypothetical[iris_hypothetical$Species == \"virginica\", \"Sepal.Length\"])-.13, color = \"blue\")+\n  scale_color_manual(values=c(\"blue\", \"red\"))\n\n\n\n\nFigure 49: Similar mean, very different distribution!\n\n\n\n\nThis spread could be quantified in multiple ways. Here we focus on the variance, which is defined as\n\\[\ns^2 = \\frac{\\sum_{i=1}^{n} (Y_{i}-\\overline{Y})^2}{n-1} \\sim \\sigma^2\n\\]\nAgain, the population parameter is \\(\\sigma^2\\), and the estimate is \\(s^2\\). This is effectively the average distance of each point from the mean squared!\n\nsegment_data &lt;- data.frame( x = 101:150, xend = 101:150, y=iris[iris$Species == \"virginica\", \"Sepal.Length\"], yend = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"]) )\nggplot(iris[iris$Species == \"virginica\",],\n              aes(y=Sepal.Length, x=sample))+\n  geom_point(color='blue') +\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"), \" arranged by collection order\")),\n       y= \"Sepal length (cm)\",\n       x= \"Collection #\")+\n  geom_hline(aes(yintercept=mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])), color = \"blue\")+\n  annotate(\"text\", label = \"mean\", x = 135, y = mean(iris[iris$Species == \"virginica\", \"Sepal.Length\"])+.13, color = \"blue\") +\n  annotate(\"text\", label = \"square each red line \\n and find average!\", x = 145, y = 7.5 , color = \"red\") + geom_segment(data = segment_data, aes(x = x, y = y, xend = xend, yend = yend), color= \"red\", size = 1.1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 50: Mean shown in blue!\n\n\n\n\nWhy \\(n-1\\) on the bottom, you might ask? Related reasons focus on degrees of freedom (we’ll get there) and bias. Our estimate of \\(s^2\\) is based on our sample mean. Since the sample mean can, at best, be the population mean, our variance estimate may be biased (too small). Dividing by \\(n-1\\) can be shown to correcthat. Alternatively, we are estimating one parameter from our data (again, \\(\\mu\\)), so we need to remove one degree of freedom from our calculation. However, some programs still \\(n\\) on the bottom. Note this will only really matter when \\(n\\) is small. In short, as another professor once told me, if the difference is based on whether you use \\(n\\) or \\(n-1\\) in your variance calculation, you likely have other problems.\nNote variance has odd (squared) units. If we take the square root of the variance, we get a value called the standard deviation.\n\\[\nstandard\\;deviation = sd= \\sqrt{variance}\n\\]\nThis metric is now in the same units as the mean (\\(\\mu\\)), so we can plot them easily on the same graph! This will become important later, especially when we discuss normal distributions.\n\n\nCoefficient of variation\nThe coefficient of variation, \\(CV\\), is found using the formula\n\\[\nCV = \\frac{s}{Y} * 100\\%\n\\]\nThis scales the variance (spread) of the data by the mean. The \\(CV\\) is unitless and can be used to compare various distributions.\n\n\nOdds and Ends: Quartiles, percentiles, other splits, maximum and minimum\nAs noted above in the discussion of five-number summaries, we can also find the assorted quartiles, minimum, and maximum of a dataset. This can be especially helpful when the mean isn’t a useful measure of central tendency (since variance and \\(CV\\) rely on the estimate of the mean!). For examples, see Figure 9. The original five number summary is\n\nsummary(cardinals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.58   45.97   49.97   49.13   53.13   59.81 \n\n\nNote that if we shifted the bottom 10% of the values even further left (increasing the skew), the median stays the same, as does the IQR.\n\ncardinals_new &lt;- cardinals\ncardinals_new[cardinals_new &lt; quantile(cardinals_new, probs= .1)] &lt;- cardinals_new[cardinals_new &lt; quantile(cardinals_new, probs= .1)]-.4\n\nggplot(data.frame(cardinals_new), \n       aes(x=cardinals_new)) +\n  geom_histogram( fill=\"red\", color=\"black\") +\n  labs(title=\"Weight of Westchester cardinals (shifted)\",\n       x= \"Weight (g)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nsummary(cardinals_new)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.18   45.97   49.97   49.09   53.13   59.81 \n\n\n\n\n\nFigure 51: Shifted cardinal data\n\n\n\n\n\n\nProportions for categorical data\nFinally, if we have categorical instead of numerical data, we can find the proportion of data in each category. We mentioned this approach when producing mosaic plots. To find a proportion, you count the number of samples that fall in a given group and divided that by the total number sampled. Alternatively, you can assign a score of 0 for values that are not in the focal group and a score of 1 to samples that are - the average of these scores will give you the proportion.\nFor example, earlier we plotted how flower color differed among species and location.\n\nggplot(iris_both,aes(x=Species)) +\n  geom_bar(aes(fill=Color), position=\"fill\")+\n  labs(title=expression(paste(\"Color of \",italic(\"I. virginica \"), \"flowers\")),\n       x= \"Species\",\n       y= \"Frequency\")+\n  scale_fill_manual(\"Flower color\", values = c(\"blue\" = \"blue\", \"orange\" = \"orange\", \"purple\" = \"purple\", \"pink\"=\"pink\", \"yellow\"=\"yellow\"))+\n  facet_wrap(~Population, nrow=1)\n\n\n\n\nLet’s see where this came from. We can count the number of each color in each species in each population:\n\n\n\n\n  \n\n\n\nThen divide them by the number sampled for each species in each population:\n\nflower_prop_df &lt;- data.frame(prop.table(flower_table, margin = c(\"Population\", \"Species\")))\nnames(flower_prop_df)[4] &lt;- \"Proportion\"\nflower_prop_df$Population_species &lt;- paste(flower_prop_df$Population,flower_prop_df$Species)\nflower_prop_df &lt;- flower_prop_df[order(flower_prop_df$Population_species),]\nrow.names(flower_prop_df) &lt;- NULL\npaged_table(flower_prop_df[,1:4])"
  },
  {
    "objectID": "content/summarizing_data.html#next-steps",
    "href": "content/summarizing_data.html#next-steps",
    "title": "Summarizing data",
    "section": "Next steps",
    "text": "Next steps\nNow that we can summarize data, we can begin to connect summaries to statistics (and learn/remember some probability along the way!)."
  },
  {
    "objectID": "content/practice_problems/6_Comparing_means.html",
    "href": "content/practice_problems/6_Comparing_means.html",
    "title": "6. Comparing means",
    "section": "",
    "text": "Before doing this, review the 6. Comparing means lecture set slides from https://sites.google.com/view/biostats/lessons/comparing-means-among-groups and the 7_ANOVAs.R script in the lecture files folder of the CUNY-BioStats github repository. Make sure you are comfortable with null and alternative hypotheses and approiate plots for all examples.\nRemember you should"
  },
  {
    "objectID": "content/practice_problems/6_Comparing_means.html#examples",
    "href": "content/practice_problems/6_Comparing_means.html#examples",
    "title": "6. Comparing means",
    "section": "Examples",
    "text": "Examples\nWe will run ANOVA’s using the lm function to connect them to other test. First, build the model\n\niris_anova &lt;- lm(Sepal.Length~Species, iris)\n\nThen use the object it created to test assumptions\n\npar(mfrow = c(2,2))\nplot(iris_anova)\n\n\n\n\nIf assumptions are met, check the p-value using the summary or Anova function.\n\nsummary(iris_anova)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nAnova(iris_anova, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Sepal.Length\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept) 1253.00   1 4728.16 &lt; 2.2e-16 ***\nSpecies       63.21   2  119.26 &lt; 2.2e-16 ***\nResiduals     38.96 147                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf the overall test is significant, carry out post hoc tests (Tukey shown here for all pairs, as most common)\n\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\ncompare_cont_tukey &lt;- glht(iris_anova, linfct = mcp(Species = \"Tukey\"))\nsummary(compare_cont_tukey)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Sepal.Length ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nversicolor - setosa == 0       0.930      0.103   9.033   &lt;1e-08 ***\nvirginica - setosa == 0        1.582      0.103  15.366   &lt;1e-08 ***\nvirginica - versicolor == 0    0.652      0.103   6.333   &lt;1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nIf assumptions are not met, we can use the Kruskal Wallis non-parametric test and associated post hoc tests.\n\nkruskal.test(Sepal.Length ~ Species, data = iris)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Sepal.Length by Species\nKruskal-Wallis chi-squared = 96.937, df = 2, p-value &lt; 2.2e-16\n\npairwise.wilcox.test(iris$Sepal.Length, \n                          iris$Species, \n                          p.adjust.method=\"holm\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  iris$Sepal.Length and iris$Species \n\n           setosa  versicolor\nversicolor 1.7e-13 -         \nvirginica  &lt; 2e-16 5.9e-07   \n\nP value adjustment method: holm \n\n\nor a bootstrap alternative\n\nlibrary(WRS2)\nt1waybt(Sepal.Length~Species, iris)\n\nCall:\nt1waybt(formula = Sepal.Length ~ Species, data = iris)\n\nEffective number of bootstrap samples was 599.\n\nTest statistic: 111.9502 \np-value: 0 \nVariance explained: 0.716 \nEffect size: 0.846 \n\nbootstrap_post_hoc &lt;- mcppb20(Sepal.Length~Species, iris)\np.adjust(as.numeric(bootstrap_post_hoc$comp[,6]), \"holm\")\n\n[1] 0 0 0"
  },
  {
    "objectID": "content/practice_problems/6_Comparing_means.html#just-for-practice",
    "href": "content/practice_problems/6_Comparing_means.html#just-for-practice",
    "title": "6. Comparing means",
    "section": "Just for practice",
    "text": "Just for practice\n\n1\n\nUse the iris dataset in R to determine if petal length differs among species. Do this problems using ANOVA, Kruskal-Wallis, and bootstrapping methods. Make sure you can plot the data and carry out multiple comparison methods as needed. Also be sure to understand the use of coefficients and adjusted R2 values and where to find them.\n\n\n\n2\n\nData on plant heights (in cm) for plants grown with a new and old formulation of fertilizer can be found at\n\nhttps://docs.google.com/spreadsheets/d/e/2PACX-1vSUVowOKlmTic4ekL7LSbwDcqrsDSXv5K_c4Qyfcvz1lLE1_iINmGzy0zMGxY7z5DImlUErK4S2wY7Y/pub?gid=0&single=true&output=csv.\nAnalyze this data using the t.test function and the lm function to convince yourself that t-tests are special cases of ANOVAs, which are special cases of linear models!"
  },
  {
    "objectID": "content/practice_problems/6_Comparing_means.html#for-the-following-questions-pick-the-appropriate-method-for-analyzing-the-question.-use-a-plot-of-the-data-andor-model-analysis-to-justify-your-decision.-make-sure-you-can-carry-out-multiple-comparison-methods-as-needed.-also-be-sure-to-understand-the-use-of-coefficients-and-adjusted-r2-values-and-where-to-find-them.",
    "href": "content/practice_problems/6_Comparing_means.html#for-the-following-questions-pick-the-appropriate-method-for-analyzing-the-question.-use-a-plot-of-the-data-andor-model-analysis-to-justify-your-decision.-make-sure-you-can-carry-out-multiple-comparison-methods-as-needed.-also-be-sure-to-understand-the-use-of-coefficients-and-adjusted-r2-values-and-where-to-find-them.",
    "title": "6. Comparing means",
    "section": "For the following questions, pick the appropriate method for analyzing the question. Use a plot of the data and/or model analysis to justify your decision. Make sure you can carry out multiple comparison methods as needed. Also be sure to understand the use of coefficients and adjusted R2 values and where to find them.",
    "text": "For the following questions, pick the appropriate method for analyzing the question. Use a plot of the data and/or model analysis to justify your decision. Make sure you can carry out multiple comparison methods as needed. Also be sure to understand the use of coefficients and adjusted R2 values and where to find them.\n\n3\n\nData on sugar cane yield for multiple fields is available using\n\nread.table(“https://docs.google.com/spreadsheets/d/e/2PACX-1vRjstKreIM6UknyKFQCtw2_Q6itY9iOAVWO1hUNZkBFL8mwVssvTevqgzV22YDKCUeJq0HBDrsBrf5O/pub?gid=971470377&single=true&output=tsv”, header = T, stringsAsFactors = T)\nMore info on the data can be found at http://www.statsci.org/data/oz/cane.html. Is there evidence that location (DistrictPosition column) impacts yield (Tonn.Hect column)? If so, which areas are driving this distance?\n\n\n4\n\nData on FEV (forced expiratory volume), a measure of lung function, can be found at\n\nhttp://www.statsci.org/data/general/fev.txt\nMore information on the dataset is available at\nhttp://www.statsci.org/data/general/fev.html.\nIs there evidence that FEV depends on gender? If so, which gender has the higher FEV score? How much variance does gender explain?\n\n\n5\n\nThe following data are human blood clotting times (in minutes) of individuals given one of two different drugs.\n\n\n\n\nDrug B\nDrug G\n\n\n\n\n8.8\n9.9\n\n\n8.4\n9.0\n\n\n7.9\n11.1\n\n\n8.7\n9.6\n\n\n9.1\n8.7\n\n\n9.6\n10.4\n\n\n\n9.5\n\n\n\nTest the hypothesis that the mean clotting times are equal for the two groups\n\nEstimating the variance from the data\nUsing rank transform analysis\nUsing a permutation test\nUsing a bootstrap test\n\n\n\n6\n\n(Example from Handbook on Biological Statistics) Odd (stunted, short, new) feathers were compared in color to typical feathers in Northern Flickers (Colaptes auratus) (Wiebe and Bortolotti 2002) . Data is at\n\nhttps://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/wiebe_2002_example.csv\nTest the hypothesis that odd and typical feathers did not differ using\n\na Student’s t test and/or lm\na rank test\nbootstrapping\n\nNote we will return to this question next week!"
  },
  {
    "objectID": "content/Intro_to_R.html",
    "href": "content/Intro_to_R.html",
    "title": "Intro to R!",
    "section": "",
    "text": "##INTRODUCTION TO R#### ##JSG, 7/2/22\n#this is a broad introduction to how R works and what it can do #we’ll return to individual topics in later sessions (both theory and practice)\n#Things to know about R (General information):####\n\nR is a programming language, and as such it can be annoying to use at first (steep\n#learning curve). However,it’s free, extremely powerful, and replicable (you can #send code (scripts, like this file, see below) to a colleague or student or use it again later. #Compare this to walking someone through JMP steps). or updating analysis. #It’s also very useful to be able to return to code months (or years) later #and run it again (as the program will load data, analyze, and graph it in one step)\n\n\nYou can enter stuff directly into R (command line), or you can use a script. A\n\n\nscript is just a text file (like this), usually a .R file but any editor can open it.\n\n\nUsing a script allows you to save what you did (generally a good idea). To\n\n\nrun what you have in a script, simply select it, right click, and select “Run\n\n\nline or selection”. You can also add comments to a script to guide others (and you)\n\n\non what you were trying to do. A # sign in front of a line (like all these)\n\n\nmeans its not read by the program; its a note. #### will automate a “table of contents”\n\n\nin Rstudio. Think of this as your lab notebook; write down what you are doing and why!\n\n\n\n\n\nif you are writing scripts, its good practice (and will make your life easier) to develop\n#and implement good habits (not always practiced in these scripts… a classic do as I #say, not as I do comment). As Hadley William’s states, “Good coding style is like #using correct punctuation. You can manage without it, but it sure makes things easier # to read.”. His guide is @ # http://adv-r.had.co.nz/Style.html # and I recommend using it\n\n\nWe’ll only use scripts this once. I’ll introduce you to Rmd files soon, which\n\n\nare a much better way of combining text (prose) and code in literate programming.\n\n\nR is an object-based language. All that means is you can define objects (x=2,\n\n\nor a list, or a matrix) and then use x for your calculations. Sometimes it\n\n\nhelpful to understand what’s happening (is the object being changed, evaluated,\n\n\nor modified?)\n#Every object in R has a class (list, matrix, dataframe, timeseries). You can #manipulate different classes in different ways using different commands (functions #, see next). We’ll work on manipulating on these throughout the semester\n\n\nAs a language, R runs commands (normally function) from different libraries. When you install R\n\n\na number of base packages are installed automatically. Howevever, you may find\n\n\nyou eventually need to download a new package. We’ll walk through this during\n\n\ntutorial.\n\n\nEvery function (including ones you can make yourself) have to be fed certain inputs.\n\n\nThis normally looks something like functionname ( part1,part2,part3). R also\n\n\nrecognizes names of the different parts, so you can technically do\n\n\nfunctionname(part3 = , part2 = , part1 = ). It will also guess what you mean if you\n\n\ndidn’t enter something. Sometimes this is bad. If you forget what a function\n\n\nneeds, you can type ?functionname or ??functionname for help. For example, lm\n#is the linear model # function. ?lm #for odd symbols, put them in single quotes ?‘?’ # under usage, the help file shows you what the function needs to be told. Any # time it says part = something, thats the default that you don’t have to fill in\n\n\nGraphical user interfaces (GUI’s) also exists for R. We’ll use Rstudio in class,\n\n\nwhich presents data, objects, plots, and other things you create in various windows\n\n\nand gives you good options for saving and manipulating things. It also color codes your\n\n\nscripts and help you find missing brackets, parentheses, and other errors.\n\n\nOther options include Revolution R\n\n\n\n\n\nRstudio also uses sections. Any line marked by at least 4 trailing #,-, or =\n\n\nsigns is labelled a section that you can collapse and that is added to the\n\n\n“label area” below\n\n\n\n\n\nUsing this exercise (in R) assumes you have a computer with R (and maybe Rstudio)\n\n\non it. A practice guide similar to this is available in the first three chapters of\n\n\nYaRrr! The Pirate’s Guide to R\n\n\nhttps://bookdown.org/ndphillips/YaRrr/\n\n\nLEARNING BY DOING: SOME R EXAMPLES\n#remember #a # sign in front of a line means its not read by the program; its a note #think of this as your lab notebook; write down what you are doing\n#first, lets play with some simple stuff. highlight this line and the two below it, #and hit run 2\n\n\nsee, it pops up in your console. do it again for the next line\n2 + 2\n\n\na rather complicated calculator, but it works. keep going\n(2 + 2) 2\n\n\nfirst error! Realize that you have to put in operators, like\n(2 + 2) * 2\n\n\nthat should be better.\n\n\n\n\n\nINTRO TO OBJECTS\n\n\nNow let’s make some objects\nx &lt;- 2\n#note here: &lt;- is equivalent in most cases to =, but style guides recommend #using &lt;- (and = can supposedly #cause trouble in some instances, though I’ve never seen them. if you are using Rstudio, #x is also now shown in your global environment pane, and you can call it in the console\nx\n#to see its value. A few notes on naming things: follow styleguides by keeping it short, #simple, but descriptive. Also try not to use an existing function name!\n#YOUR FIRST FUNCTION#### #now lets go back to the class thing and use our first function# class(x) #class (above) is the first recognizable function you’ve used. A function is a set of code #that has been given a name (saved as an object). The code states what type of input #(parameters or values, x in the case above) the code needs to run and produce some type of #output. You can recognize functions by their function_name() setup. Some functions # (like class) operate on the data in the variables as a data set and produce #a summary. Others apply themselves to each value in the data set (like log(x) #note this doesn’t change x! You can name function output and save it, like log_x &lt;- log(x)\n#let’s assign some more variables y &lt;- 2 y x + y\n#simple enough, but larger lists or objects work the same way\nx &lt;- c(2, 2, 2, 2) class(x) x # now you’ve made a list (The c stands for concatenate and you’ll see it used often)\nx &lt;- matrix(c(2, 2, 2, 2), 2, 2) class(x) x\n#now x and a matrix. note we’ve named x three times, and thus replaced it!\n#If you need to enter text, you can put it in quotes location &lt;- “alpine” zones &lt;- c(“alpine”, “valley”, “meadow”) #entering or manipulating text is important when we want R to think about factors #or grouping variables, instead of numbers(example to come). R tries to figure #out what you want, but you can specify by making someting a factor location &lt;- factor(location) location\n#Just a note, if you are doing matrix calculations directly, you #need to use different operators: Example) y &lt;- matrix(data=c(1, 1, 1, 1), nrow=2, ncol=2) #note here: I am usually bad about not specifying parameter inside a function #call (compare x and y assignmnets above). R will guess what you want, but if #you don’t specify things can go wrong\ny\nx + y\nx * y # this is wrong\nx %*% y # this is right\n#now back to lists. You can also call out certain parts (elements) #of a list or vector. #making a random number set for illustration x &lt;- rnorm(10, 1, 1) x x[2] #this calls out the 2nd element (and also shows you can comment in-line)\n#now drop an element x [-2]\n#note here: this isn’t replacing x (just using or showing it) unless you assign it, like #x &lt;- x[-2]\n#you can also call out a subset or specific options x[c(1,2)] x[x &lt; 1]\n\n\nabove is an example of comparing objects or elements: R compares objects and\n#returns TRUE or FALSE\nx &lt;- matrix(c(2, 2, 2, 2), 2, 2) y &lt;- matrix(data=c(1, 1, 1, 1), nrow=2, ncol=2) #re-assigned here in case skipped above x y x == y #Notice the question is asked for each value in the variable, so the return value #has the same length (3). x != y x &lt; y x &gt;= y\n\n\nall of the above is relatively useless (you would never use R for these simple\n\n\nactions), but it will help you understand later parts\n\n\nsome useful functions\n#making a random number set for illustration x &lt;- rnorm( 1000, 1, 1) #remember above notation is lazy (not specifying what every parameter is actually being #used for in the function). you can also write x &lt;- rnorm (n = 1000, mean = 1, sd = 1)\n#BASIC WAYS OF SUMMARING DATA####\nmean(x) var(x) sd(x) x &gt; 100 #this returns a logical output #but this returns the actual data subset(x, x &gt; 1) #or use brackets (which I prefer because they are tedious but clear and handle compound #statements better) x[x&gt;1] subset(x, x &gt; 102)\n#now, lets play with real data and show how you might actually use this.\n#GETTING DATA IN #### #One of the trickiest parts of R is getting the data in. We’ll use some of the built-in #R datasets throughout class to show you how functions work, which also allows these #scripts be self contained, but here are a few ways to get data in. # #first, let’s make a dataset. open excel, fill in the top row with 3 names (e.g., #x,y,z) and then add a few rows of “data” #save this as a .csv file, then run # file &lt;- file.choose() #this should open a popup window. select the file you created. note this is just making a path #to the file\nfile\n#which is then read by the read.csv function\nmy_dataset &lt;- read.csv(file, stringsAsFactors = T)\n\n\nSince ~2020 you need to as StringsAsFactors = T to read in characters as factors\n\n\n(what we typically want)\n#you can also read in data from a website holding a csv (we’ll do this often in class) australia_athlete_data &lt;- read.table(“http://www.statsci.org/data/oz/ais.txt”, header = T, stringsAsFactors = T)\n\n\nand thats it. however, this is not a great way to grab data (or automate code).\n#you can also put your path in directly, e.g., #my_dataset &lt;- read.csv(“C:/Users/SGosnell/Desktop/Example data set.csv”) # #Note here: R is always “looking”somewhere for files - we call this place the working #directory. It’s also where R will eventually save output. You can check the working directory using getwd() #Files and folders in that location can be called directly. You can change the #working directory using the setwd() function. For example, I could also use #setwd(“C:/Users/SGosnell/Desktop”) #to change the working directory, then use #my_dataset &lt;- read.csv(“Example data set.csv”) #to call the file. It’s useful to set the working directory at the beginning #of a script so you know where files R going and coming from.\n#you can try to call the file you made by specifying the location and/or working #directory\n#NOTES ON COLUMN NAMES#### #for your own datasets, note you want to use short column titles and avoid spaces #(uses _ or SnakeCase). You should also use “long format” (one observation per row) # and use .csv or .txt formats. You should also remember R won’t mix groups, #i.e. if you enter some numbers and some characters, it will coerce everything to a character #Later in class we’ll discuss using plyr and reshape packages to reformat data\n#you can also make a dataset directly in R, though its tedious for larger datasets greenness &lt;- c(13766, 50513, 25084) habitat &lt;- c(“forest”, “forest”, “grassland”) date &lt;- c(“2009-12-25”, “2010-01-01”, “2010-01-15”) #dates can be handled multiple ways in R, but the key idea is to make sure you #put them in so they are more than factors date &lt;- as.Date(date) #as.Date represents time relative 1970-01-01 (earlier dates are negative) date[1] &lt; date[2] birds &lt;- data.frame(greenness = greenness, habitat = habitat, date = date)\n#if your data sets don’t have headers, you’ll need to adjust the code, but you normally # do. remember, to check what function requires, defaults to, or does, type\n?read.csv\n#NOTE: AS OF EARLY 2020, R NO LONGER READS IN STRINGS AS FACTORS BY DEFAULT! SO #TO GET BAR GRAPHS, ETC, YOU MAY HAVE TO MANUALLY CHANGE A COLUMN TO A FACTOR. #YOU CAN DO THIS FOR SINGLE INSTANCE OR CHANGE THE ACTUAL OBJECT # #EXAMPLE OF DATA ANALYSIS STEPS####\n#LOAD THE DATA#### # its always good to make sure the data are in correctly. for today we’ll be using the built in #airquality dataset\nairquality # if this is too big, try # #CHECK THE DATA####\nhead(airquality)\n#by default this shows you the first 10 lines. note R stores data in vector formats, so we’ll #try to use commands that take advantage of that to speed up analysis str(airquality) #shows yous the structure of the dataframe #also useful for checking for what type of variable R thinks you have put in (factor,integer, # or numeric). lets assume you need to change a column (a common example is Trial is a # factor, not an integer). if you don’t fix this R may run the wrong analyses (e.g., # a regression when you want an ANOVA).\nairquality\\(Month &lt;- as.factor(airquality\\)Month) #similar commands exist (as.integer, as.numeric) for other classes.note how you use #this as it may give unusual results. For example, if you turn factor levels into numbers #they will go fro 1 up g (total # of levels) based on the order R had them in (often #alphabetical). If you need to turn numbers from factors to numbers, #as.numeric(as.character(x)). the easier lesson is to name things rights to begin with\nsummary(airquality) #gives you basic summary statistics\n\n\n\n\n\nYou can call up a column using the dollar sign and change it\n\n\nto a transformed version of itself\n\n\nNOTE NONE OF THIS IS CHANGING THE FILE YOU READ IN TO R!\n\n\nEXPLORATORY DATA ANALYSIS - PLOTTING AND NUMERICAL SUMMARIES\n\n\nonce your classes are set, let’s do some basic plotting. there are multiple\n\n\nplotting commands in r. the simplest is plot. You can plot by specifying x and\n\n\ny coordinates or by using a the formula setup. In r, formulas are marked as\n\n\nsomething ~ (explained by) something else)\nplot(airquality\\(Ozone, airquality\\)Temp) plot(Ozone~Solar.R, data = airquality) plot(Ozone~Solar.R, airquality, type = “l”) plot(Ozone~Month, airquality)\n#in Rstudio, note plots are automatically saved each session and you can scroll through, #zoom, plot, and save as needed\n#a useful function for visually exploring relationships pairs(airquality)\n#if you check out plot, you’ll also notice you can change type and a thousand #other things. plot(Ozone~Temp, airquality, col=airquality$Month)\n#We’ll be introducing another graphing package, ggplot2, later this semester\n#FORMAL ANALYSIS#### # Now lets do some basic analyses. the easiest way to do this is using linear models (a more general description # for regression, t-tests, ANOVAs, ANCOVAs. If you prefer there are commands that specifically do these only\nozone_month_relationship &lt;- lm(Ozone~Month+Temp+Solar.R+Wind, airquality) #the above code creates a model object\n#now lets look at what we found summary(ozone_month_relationship)\n#or we can get traditional p values (don’t worry if you don’t know/remember what #these are). we need to install a package to do this.\n#FUNCTIONS FROM NON-BASIC PACKAGES#### #Illustrating help function: Let’s say you need to use a function but when you run it you get an error like this\nAnova(ozone_month_relationship, type = “III”) #could not find function\n#then use can use ??Anova to figure out which package its in\n??Anova\n\n\nand install the package. To install the car package in Rstudio, in the top row,\n\n\nclick packages, install packages. then find a close mirror. Then find the car\n\n\npackage. once the package is installed it will stay on your computer forever.\n\n\nHowever, you’ll have to load it when you need to use it (unless you mess with\n\n\nstart up files). just type\nlibrary(car) #library is better than require function. Packages are “stored” in your library #location. You can set and see this using the .libPaths() function .libPaths() #Adding a file location in the parentheses makes that a library location. This #isn’t necessary on school computers, but on your own machine you may want different #locations for versions of R if you stick with this.\nAnova(ozone_month_relationship, type = “III”) #always specify type 3, else order #matters for the model. we’ll get to this later\n#if we do single factors, we can also add lines to plots ozone_month_relationship &lt;- lm(Ozone~Temp, airquality) plot(Ozone~Temp, airquality) abline(ozone_month_relationship) #other commands like points() and lines() can also add to existing plots\n#we can also consider interactions ozone_month_relationshipinteractions &lt;- lm(Ozone ~ Month * Temp, airquality) summary(ozone_month_relationshipinteractions) Anova(ozone_month_relationshipinteractions, type = “III”)\n\n\nRENAMING, SUBSETTING, AND SORTING DATA\n\n\nthere are lots of ways to do this\n\n\nWhat if we wanted to rename months to their normal names\n\n\nfactors have levels that you can change\nlevels(airquality\\(Month) levels(airquality\\)Month) &lt;- c(“May”,“June”, “July”, “August”, “September”) #above is a built-in way to do this, but we’ll use relevel in class (later) as i #find its less prone to mistakes levels(airquality$Month)\n#data frames have names(headers) you can change similarly #you can also specifically change indiviudal names or levels with names(airquality) #gives you column names names(airquality)[names(airquality) %in% “Month”] = “example_change” # %in% looks for matches names(airquality) names(airquality)[names(airquality) %in% “example_change”] = “Month”\n#you could also call by index names(airquality)[3] &lt;- “example_change” names(airquality) names(airquality)[3] &lt;- “Wind”\n#or for levels levels(airquality\\(Month)[levels(airquality\\)Month) %in% “August”] = “test_change” levels(airquality\\(Month)[levels(airquality\\)Month) %in% “test_change”] = “August” #other commands will do this easier (relevel) but you can use brackets to specify most changes\n#this is also useful if you want to combine months/treatments. alternatively, #you could have duplicated the column and changed the copy. the main idea here, #however, is you are never impacting the actual data file (unless you specifically #save over it). for example, we could add airquality\\(log_Temp &lt;- log(airquality\\)Temp)\n#now look at the plot again plot(Temp ~ Month, airquality) #note this is a box-whisker plot by default #remember, check out ?plot to see more advanced commands #par lets you set options to multiple plots at once #we’ll get into this more in depth later, but the par command sets graphic details for #upcoming plots. mrfow says to set a 2x2 matrix for table, oma and mar set margins #don’t worry too much about this now par(mfrow = c(2,2), oma = c(2,0,0,0), mar = c(5,4,2,2)) #notice we are subsetting data by month for the graph plot(Ozone ~ Temp, subset(airquality, Month == “June”),ylab = “Temperature”, xlab = “Ozone level (ppm))”, main = “June”, xlim = c(60,100), ylim = c(0, 175)) plot(Ozone ~ Temp, subset(airquality, Month == “July”), xlab = ““, ylab =”“, main =”July”, xlim = c(60,100), ylim = c(0, 175)) plot(Ozone ~ Temp, subset(airquality, Month == “August”), xlab = ““, ylab =”“, main =”August”, xlim = c(60,100), ylim = c(0, 175)) plot(Ozone ~ Temp, subset(airquality, Month == “September”), xlab = ““, ylab =”“, main =”September”, xlim = c(60,100), ylim = c(0, 175)) mtext(“Figure 1: Ozone Levels for the Summer Months of 2001”, 1, outer=T, cex=1.5, line=1) #later in the class we’ll transition to using the ggplot2 package for plotting\n\n\nwhat if we only cared about the impact of temperature on Ozone in July for models\njuly &lt;- lm(Ozone~Temp, subset(airquality, Month == “July”)) summary(july) Anova(july, type = “III”)\n#in general, you can subset dataframes using the subset command (above), #where the subset argument specifies rows and the select argument specifies columns, #or by using brackets like we did for vectors. The generla format is [rows, columns], like airquality[airquality\\(Month == \"July\",] # or you combine commands, with & (and) or | (or) statements airquality[airquality\\)Month == “July” & airquality\\(Temp &gt; 85,] #returns only days in June &gt; 85 airquality[airquality\\)Month == “July” | airquality$Temp &gt; 85,] #returns days in June and days &gt; 85\n#an empty space before or afer the comma implies “all”, so this is all columns #for all rows where the month is July. you can also query by column name airquality[,“Month”]\n#note on sorting data #you can also use this to sort data airquality[order(airquality$Temp),] #order (used here to order rows) puts the dataframe in ascending order of temps. #in general, order returns the index (row number) need to put the dataset in ascending order, #while sort returns the value itself"
  },
  {
    "objectID": "content/Introduction.html",
    "href": "content/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "“Why is statistics a required course for someone who wants to be a dentist/doctor/ nurse?”\nThis is a common question (or at least thought) for many students. I hope to convince you this semester you at least need to understand statistics as part of the scientific method (and you should realize the scientific process informs all those jobs - in fact it can inform any job or task where you are searching for an answer or better method).\nFor example, doctors prescribe medicine to patients, but how do they know these medicines work? Some doctors carry out research, but many rely on published guidelines, which themselves rely on research. So a new drug or treatment is proposed- but who decides it’s worth using? Researchers carry out trials to determine the efficacy of the treatment. In doing this they have to consider how to design an experiment (what do they collect? from whom?) and analyze the resulting data so they can trust the results.\n\n\n\nFigure 1: XKCD: Control Group\n\n\nOther students in our class may be interested in a more environment- or resource management- focused career (e.g., wildlife rehabilitation, carbon mitigation expert, researcher). Regardless of your goal, any question should be informed by this approach. For example,\n\nDoes an environmental factor cause cancer?\nDo potential toxins really harm the enviroment?\nIs organic food really healthier?\nDoes exposing organisms reared in captivity to predator cues lead to more successful releases?\n\nZhu et al. (2023)\n\n\nAt its heart, statistics is about turning data into information that we can use to make decisions or better understand the world around us. Data can come from experiments we are running. This offers a clear connection to field and lab science, and its what we will focus on for most of this class. Data and theories can also be used to develop models that produce output ; this isn’t real-world data, but it offers very useful insight on what we think will happen if something occurs (and something we can test with other field data!). For example, restoration projects may focus on small-scale plots that undergo different restoration protocols. Data produced from monitoring these plots may be used to develop models to predict large-scale impacts (and maybe benefits and costs) of different restoration scenarios for larger regions.\nAbove I used words like know (how do they know these medicines work? )and predict (develop models to predict large-scale impacts (and maybe benefits and costs) of different restoration scenarios for larger regions). While we may use words like these that are related our findings interchangeably at times, its important to note the different. Statistics (and related models) generally give us estimates about how the real world works. Put another way, if we knew everything about the world, we wouldn’t need to use statistics because we wouldn’t need estimates.\nThe reasons we don’t usually know everything include\n\nthe world is complicated (some questions can’t be directly tested)\nit’s not possible to measure everything\n\nBecause of this, statistics is also focused on trying to describe populations of interest or find signals (impacts of treatments, medicines, or restoration practices, for example) amidst the noise (variation in outcomes that are always common!). When considering relationships among variables, noise may occur because there are lots of things impacting the outcome of interest. For example, restoration protocol may impact the trajectory of an oyster reef, but so too may local factors like temperature an and salinity. Noise can also occur because of sampling error - since we don’t measure everything, our estimate of relationship or population traits may be imperfect.\nIn the next session we’ll start to discuss how we can use data to make estimates about a population (and answer questions like what is a population and what are we trying to estimate). However, a final aside to finish this section - we often think about statistics happening after an experiment, survey, or other thing we get data from is finished. However, part of statistics is experimental design! Statistics should inform how you setup an experiment. In fact, the best idea (which seldom happens!) is that you simulate the type of data you expect to get from your experiment and then analyze that before you actually run the experiment. This ensures you are measuring what you need to measure and setting things up correctly! As the famous quote (to statisticians) states,\n\nTo consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of. -Ronald Fisher\n\n\n\n\n\nReferences\n\nZhu, Jennifer, J. Stephen Gosnell, Laila Akallal, and Micah Goltsman. 2023. “Fear Changes Traits and Increases Survival: A Meta-Analysis Evaluating the Efficacy of Antipredator Training in Captive-Rearing Programs.” Restoration Ecology 31 (3): e13674. https://doi.org/10.1111/rec.13674."
  },
  {
    "objectID": "content/Estimation.html",
    "href": "content/Estimation.html",
    "title": "Estimation and uncertainty",
    "section": "",
    "text": "Now that we can describe data distributions, we want to start thinking about how we quantify the uncertainty in our estimates (of \\(\\mu\\), for example). Remember, we typically want to describe a population but need to rely on a sample, and we’ve already talked about sampling error. So now we just want to think about how much error we typically have (or, alternatively, how precise are our estimates).\nAnswering this question is hard. Quantifying sampling error requires you to know the “true” value for a population parameter, but we only have estimates! Statisticians solve this problem by investigating sampling error in populations they fully know because they created them."
  },
  {
    "objectID": "content/Estimation.html#lets-start-with-an-example",
    "href": "content/Estimation.html#lets-start-with-an-example",
    "title": "Estimation and uncertainty",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\nFor example, let’s assume we measure all the males in a population. Furthermore, let’s assume the distribution of heights is normal. Remember, this means the distribution is roughly symmetric, with tails on either side. Values near the middle of the range are more common, with the chance of getting smaller or larger values declining at an increasing rate. In fact, in turns out ~95% of the data lies within two standard deviations (remember those?) of the mean (so we calculate the mean and then the standard deviation. We then subtract the standard deviation from the mean to find a lower bound. We then add the standard deviation from the mean to find an upper bound. These bounds denote where 95% of the data points will be found).\nLet’s see this in action. First, lets make a population with a trait (let’s assume height, measured in cm, that follows a normal distribution. We can set the mean to 70 and the standard deviation to 3.\n\nset.seed(42)\npopulation_size &lt;- 10000\npopulation_norm &lt;- data.frame(id = 1:population_size, \n                         height = rnorm(population_size, 70, 3))\n\nNow’s let graph it.\n\nlibrary(ggplot2)\n\nggplot(population_norm, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow let’s add the mean (69.97 cm) and mark two standard deviations (sd = 3.02 in) above and below it. Remember we noted a benefit of using standard deviations to describe spread was that they were in the same units as the mean? Now we can use that!\n\ncolors &lt;- c(\"mean\" = \"black\", \"2 standard deviations below\" = \"red\", \n            \"2 standard deviations above\" = \"green\")\nggplot(population_norm, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(height), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(height)-\n                     2*sd(height), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(height)+\n                     2*sd(height), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 1200, x = mean(population_norm$height), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 1200, x = mean(population_norm$height)-\n                     2*sd(population_norm$height), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 1200, x = mean(population_norm$height)+\n                     2*sd(population_norm$height), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 1: Our imaginary population!\n\n\n\n\nThis bound captures 95.53% of the data.\nNow let’s sample the population. We’ll start by drawing a sample of 100 from the population. This is true random sampling, so any differences are due to sampling error.\n\nsample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\nggplot(sample_1, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of 100 random males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor this sample, we have a mean of 69.78 cm and a standard deviation of 3.11 cm."
  },
  {
    "objectID": "content/Estimation.html#moving-to-a-sample-of-means",
    "href": "content/Estimation.html#moving-to-a-sample-of-means",
    "title": "Estimation and uncertainty",
    "section": "Moving to a sample of means",
    "text": "Moving to a sample of means\nHere’s the tricky part. We typically only have one sample, but we want to discuss the uncertainty in our estimate. So, let’s explore this by drawing multiple samples (each of 100 individuals) from our population and finding the mean for each sample.\n\nnumber_of_samples &lt;- 1000\nsample_outcomes_1 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes_1$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes_1$sd[i] &lt;- sd(sample_1$height)\n  \n}\n\nThen let’s plot the means.\n\nggplot(sample_outcomes_1, aes(mean)) + \n    geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 100)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_1$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_1$mean)-\n                     2*sd(sample_outcomes_1$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_1$mean)+\n                     2*sd(sample_outcomes_1$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor our sample of means (this should sound weird!), we have a mean of 69.98 in and a standard deviation of 0.3 cm\nNote this suggests the mean of our means is close to the true population value of \\(\\mu\\). But the spread of our means (their standard deviation) is much less than the spread of the actual population! How much less? Let’s consider a set of smaller samples (n = 20).\n\nsample_outcomes_2 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_2 &lt;- population_norm[sample(nrow(population_norm), 20),]\n  sample_outcomes_2$mean[i] &lt;- mean(sample_2$height)\n  sample_outcomes_2$sd[i] &lt;- sd(sample_2$height)\n  \n}\n\nggplot(sample_outcomes_2, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 20)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_2$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_2$mean)-\n                     2*sd(sample_outcomes_2$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_2$mean)+\n                     2*sd(sample_outcomes_2$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis new sample of means has a mean of 69.96 in and a standard deviation of 0.66 cm So, the estimate for \\(\\mu\\) is still close to the same, but the standard deviation of our estimates is growing.\nThis is even more clear if we sample only 5 individuals.\n\nsample_outcomes_3 &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_3 &lt;- population_norm[sample(nrow(population_norm), 5),]\n  sample_outcomes_3$mean[i] &lt;- mean(sample_3$height)\n  sample_outcomes_3$sd[i] &lt;- sd(sample_3$height)\n  \n}\n\nggplot(sample_outcomes_3, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 5)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_3$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_3$mean)-\n                     2*sd(sample_outcomes_3$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_3$mean)+\n                     2*sd(sample_outcomes_3$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nwhere we find a mean of 69.97 in and a standard deviation of 1.33 cm.\nIf we facet the graphs (and let them share an x-axis) we can see this even better\n\nsample_outcomes_1$n=100\nsample_outcomes_2$n=20\nsample_outcomes_3$n=5\nsamples_all &lt;- rbind(sample_outcomes_1,sample_outcomes_2, sample_outcomes_3)\n\nggplot(samples_all, aes(mean)) + \n   geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples\",\n       color=\"Measure\") +  facet_wrap(~n, ncol=1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou can clearly see larger sample sizes lead to a more “clustered’ group of means (so there is less uncertainty in the measurements!). This is why larger estimates make us more confident in our estimates - the means we get are less likely to be far away! In other words, larger samples yield more precise estimates with lower spread (lower sampling error).\nWe call the standard deviation of our means the standard error. We can calculate this as\n\\[\n[\\sigma_{\\overline{Y}} = \\frac{\\sigma}{\\sqrt{n}}] \\sim [s_{\\overline{Y}} = \\frac{s}{\\sqrt{n}}]\n\\]\nAlso, note distribution of means was normal (which we will define even better in a few lectures!). For now, that means we can get 95% of the sample means within ~2 standard deviations of the mean of means, which is very close to the true mean. Conversely, if we use data from each sample to generate a an interval ~2 standard deviations above and below each sample mean, these intervals will contain the true mean 95% of the time. We call this range a 95% confidence interval. For example, let’s take take 20 samples of 100 individuals from our fake population, then calculate and plot their confidence intervals.\n\nnumber_of_samples &lt;- 20\nsample_outcomes &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA, se = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes$sd[i] &lt;- sd(sample_1$height)\n  sample_outcomes$se &lt;- sd(sample_1$height)/sqrt(100)\n}\nsample_outcomes$sample &lt;- as.factor(1:number_of_samples)\nggplot(sample_outcomes\n       , aes(x=sample, y=mean)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean-(2*se), ymax=mean+(2*se)))+\n  geom_hline(aes(yintercept=mean(population_norm$height))) +\n  ylab(\"Mean\")+\n  xlab(\"Sample\")+\n  ggtitle(\"Variation in error bars\")\n\n\n\n\nNotice one of samples (#2) has a range that does not include the true mean of the population!\nA few other notes about confidence intervals\n\nConfidence interval for normally-distributed samples (like those described here!) should be symmetric around the mean. This will change slightly for non-normal data (which we may address with generalized linear models that use a binomial, Poisson, or gamma distribution).\nThe “~2” is based on sample size. The value actually trends towards 1.96 at large sample sizes, but at sample sizes over ten 2 is a good estimate. You may also here this total (the 2 multiplied by the standard error) referred to as the margin of error. We could also have other numbers. For example, we could have a 90% confidence interval.\n\n\n\nWould it be wider or narrower compared to a 95% interval?\n\nIf you are less confident in the interval (90% vs 95%), the interval itself will get smaller (only 90% of samples need to have the true mean!)\n\n\nConfidence bounds also exist. These are slightly different - we’ll explain them in a few chapters.\n(more complicated) Note these calculations assumes we have lots of samples, but we typically only have one. The average probability of the first 95% CI capturing the true sample mean is only around 83%\n\n\n\n\n\n\nNeed to see this another way?\n\nThese two simulations (produced by UBC) will allow you to see this another way!\n\nRelationship between sample size and distribution of sample means for samples from a normally distributed population\nConfidence intervals"
  },
  {
    "objectID": "content/Estimation.html#what-if-the-population-isnt-normal",
    "href": "content/Estimation.html#what-if-the-population-isnt-normal",
    "title": "Estimation and uncertainty",
    "section": "What if the population isn’t normal?",
    "text": "What if the population isn’t normal?\nFinally, it turns out the underlying distribution of data doesn’t matter; only that of the trait we are focused on does. For example, the means of the data will be normally distributed as long as you have a large sample size. This is know as the central limit theorem.\nTo prove this, let’s instead consider a uniform distribution:\n\npopulation_unif &lt;- data.frame(id = 1:population_size, \n                         height = runif(population_size, 60, 80))\nggplot(population_unif, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of all males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow, let’s do what we did above. First, draw a sample of 100\n\nsample_unif &lt;- population_unif[sample(nrow(population_unif), 100),]\nggplot(sample_unif, aes(height)) + \n  geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Height of 100 random males in our fake population\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote the sample is still relatively uniformly distributed. This makes sense. In general, a good sample should resemble the underlying population, so this makes sense.\nNow let’s sample a 100 of these numerous times and plot the means of each sample.\n\nnumber_of_samples &lt;- 1000\nsample_outcomes_unif &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA)\n\nfor (i in 1:number_of_samples){\n  sample_unif &lt;- population_norm[sample(nrow(population_unif), 100),]\n  sample_outcomes_unif$mean[i] &lt;- mean(sample_unif$height)\n  sample_outcomes_unif$sd[i] &lt;- sd(sample_unif$height)\n}\nggplot(sample_outcomes_unif, aes(mean)) + \n    geom_histogram(color=\"black\") +\n  labs(x=\"Height (cm)\", y= \"Frequency\",\n       title = \"Mean heights from our samples (n = 100)\",\n       color=\"Measure\") +\n    geom_vline(aes(xintercept=mean(mean), color=\"mean\"))+\n    geom_vline(aes(xintercept=mean(mean)-\n                     2*sd(mean), color=\"2 standard deviations below\"))+\n  geom_vline(aes(xintercept=mean(mean)+\n                     2*sd(mean), color=\"2 standard deviations above\")) +\n        scale_color_manual(values = colors)+\n   annotate(\"text\", label = \"mean\", y = 150, x = mean(sample_outcomes_unif$mean), color = \"black\") +\n     annotate(\"text\", label = \"2 standard deviations \\n below\", y = 150, x = mean(sample_outcomes_unif$mean)-\n                     2*sd(sample_outcomes_unif$mean), color = \"red\")+\n     annotate(\"text\", label = \"2 standard deviations \\n above\", y = 150, x = mean(sample_outcomes_unif$mean)+\n                     2*sd(sample_outcomes_unif$mean), color = \"green\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice we are back to a normal distribution!\n\n\nNeed to see this another way?\n\nAnother UBC visualization will allow you to see this another way!\n\nCentral limit theorem: Sampling from non-normal distributions."
  },
  {
    "objectID": "content/Estimation.html#visualization-issues",
    "href": "content/Estimation.html#visualization-issues",
    "title": "Estimation and uncertainty",
    "section": "Visualization issues",
    "text": "Visualization issues\nAs you can see above, 95% confidence intervals are commonly graphed to show the potential spread of mean values. This distinction is important, as one could plot the standard deviation of the raw the data, the standard error of the related means, or the 95% confidence interval. These can be very different. As an example,let’s return to our normal population.\n\nnumber_of_samples &lt;- 1\nsample_outcomes &lt;- data.frame(mean = rep(NA, number_of_samples), sd = NA, se = NA)\n\nfor (i in 1:number_of_samples){\n  sample_1 &lt;- population_norm[sample(nrow(population_norm), 100),]\n  sample_outcomes$mean[i] &lt;- mean(sample_1$height)\n  sample_outcomes$sd[i] &lt;- sd(sample_1$height)\n  sample_outcomes$se &lt;- sd(sample_1$height)/sqrt(100)\n}\nsample_outcomes$sample &lt;- as.factor(1:number_of_samples)\n\nsample_1$sample &lt;- \"Data\"\nsample_1$data &lt;- sample_1$height\nonese &lt;- sample_outcomes\nonese$sample &lt;- \"+- 1 standard error\"\nonese$data &lt;- onese$mean\nonese$bar_length &lt;-  onese$se\ntwosd &lt;- sample_outcomes\ntwosd$sample &lt;- \"+- 2 standard error ~ \\n 95% confidence interval\"\ntwosd$data &lt;- twosd$mean\ntwosd$bar_length &lt;-  onese$se * 2\nonesd &lt;- sample_outcomes\nonesd$sample &lt;- \"+- 1 standard deviation\"\nonesd$data &lt;- onesd$mean\nonesd$bar_length &lt;-  onese$sd\n\nexample_clarity &lt;- merge(sample_1, onese, all.x =T, all.y = T)\nexample_clarity &lt;- merge(example_clarity, twosd, all.x =T, all.y = T)\nexample_clarity &lt;- merge(example_clarity, onesd, all.x =T, all.y = T)\n\nlibrary(plyr)\nexample_clarity$sample &lt;- relevel(as.factor(example_clarity$sample), \"Data\")\n\nggplot(example_clarity\n       , aes(x=sample, y=data)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=mean-bar_length, ymax=mean+bar_length))+\n  labs(y =\"Height (cm)\", x= \"Frequency\",\n       title = \"Variation in error bar display\")\n\n\n\n\nNotice the differences! We will typically use 95% confidence intervals in class, but you should always specify in your captions and note when you read other papers!\nVisualizations of spread are commonly used with bar graphs (despite the earlier issues we noted with bar graphs!). For example, we can return to our iris data and add\n\nlibrary(Rmisc)\n\nLoading required package: lattice\n\nfunction_output &lt;- summarySE(iris, measurevar=\"Sepal.Length\", groupvars =\n                               c(\"Species\"))\n\nggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_col(aes(fill=Species)) +\n    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n\n\n\nIn general, presenting the estimate for a parameter and measures of sampling error (or uncertainty) allow you to state the magnitude of a statistic. This is a different but related approach to the more commonly observed p-values (which we’ll get to!). For example, for a single population we can ask if the confidence interval includes a relevant value (like 0!). For multiple groups,we can consider if the true means are in the same range by looking at overlap in confidence intervals among the groups."
  },
  {
    "objectID": "content/Estimation.html#next-steps",
    "href": "content/Estimation.html#next-steps",
    "title": "Estimation and uncertainty",
    "section": "Next steps",
    "text": "Next steps\nIn this chapter we’ve started to talk about probability. In the next we will review some probability basics before we move onto testing hypotheses."
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html",
    "href": "content/chapters/More_ANOVAs.html",
    "title": "More ANOVAs",
    "section": "",
    "text": "In the last chapter we introduced the idea of comparing means among populations (one-way ANOVAs, our first linear models). However, the units that we measure may belong to multiple groups. We will extend our analysis of variance to consider multiple group membership and interactions in this chapter. As a starting point, consider that group membership may be an inherent property of the unit we measure or we may assign it."
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#example-back-to-the-birds",
    "href": "content/chapters/More_ANOVAs.html#example-back-to-the-birds",
    "title": "More ANOVAs",
    "section": "Example: Back to the birds",
    "text": "Example: Back to the birds\nOne of the last chapters practice problems focused bird feathers. While studying feather color in Northern flickers (Colaptes auratus), Wiebe and Bortolotti (2002) noted that ~25% of birds had one or more “odd” tail feathers. They decided to compare the color of these odd and “typical” feathers.\n\n\n\nNorthern Flicker. Mike’s Birds, CC BY-SA 2.0 &lt;https://creativecommons.org/licenses/by-sa/2.0&gt;, via Wikimedia Commons\n\n\nExample and data provided by McDonald (2014).\n\nlibrary(rmarkdown)\npaged_table(feather)"
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#how-do-we-analyze-this-data",
    "href": "content/chapters/More_ANOVAs.html#how-do-we-analyze-this-data",
    "title": "More ANOVAs",
    "section": "How do we analyze this data?",
    "text": "How do we analyze this data?\nWe may first note that we have a continuous measurement (feather color, measured using color hues from a digital camera and another statistical technique that we will not go into here) and a categorical variable (feather type, with levels “typical” and “odd”). This hopefully reminds you of an ANOVA/t-test!\nWe could plot the data\n\nlibrary(ggplot2)\nggplot(feather, aes(x=Feather_type, y= Color_index, color=Feather_type))+\n  geom_jitter()+\n  labs(y= \"Color index\",\n       x= \"Feather type\",\n       title=\"Comparing odd and typical feathers in Northern flickers\")+\n  guides(color=F)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\nDevelop a set of hypotheses:\n\\[\n\\begin{split}\nH_O: \\mu_{\\textrm{odd feather color}} = \\mu_{\\textrm{typical feather color}}\\\\\nH_A: \\mu_{\\textrm{odd feather color}} \\neq \\mu_{\\textrm{typical feather color}}\\\\\n\\end{split}\n\\]\nand test them using a t-test:\n\nt.test(Color_index ~ Feather_type, data=feather)\n\n\n    Welch Two Sample t-test\n\ndata:  Color_index by Feather_type\nt = -3.56, df = 29.971, p-value = 0.00126\nalternative hypothesis: true difference in means between group Odd and group Typical is not equal to 0\n95 percent confidence interval:\n -0.21579254 -0.05845746\nsample estimates:\n    mean in group Odd mean in group Typical \n            -0.176125             -0.039000 \n\n\nor, using more generalizable functions, a linear model:\n\nlibrary(car)\n\nLoading required package: carData\n\nAnova(lm(Color_index ~ Feather_type, data=feather), type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Color_index\n              Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  0.49632  1  41.816 3.814e-07 ***\nFeather_type 0.15043  1  12.674  0.001259 ** \nResiduals    0.35607 30                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe find a significant p value, but we did not check assumptions. For linear models (remember, $i.i.d. N(,)$, we could use our visual checks\n\nplot(lm(Color_index ~ Feather_type, data=feather))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich appears ok, but there is a problem.\nOur data are not independent!"
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#lack-of-independence",
    "href": "content/chapters/More_ANOVAs.html#lack-of-independence",
    "title": "More ANOVAs",
    "section": "Lack of Independence",
    "text": "Lack of Independence\nOdd and typical feathers were measured on a single bird (note the Bird column) in the dataset. We might assume feathers on a given bird are more closely related in color than feathers on different birds. This could be due to diet or other factors making all feathers on a given bird brighter or darker than those on another. Regardless of reason (and “good” p value), we know the measurements are linked in some way. Note we could “connect” individual observations.\n\nggplot(feather, aes(x=Feather_type, y= Color_index, color=Feather_type, group=Bird))+\n  geom_line(position = position_dodge(0.4), color=\"black\") +\n  geom_point(position = position_dodge(0.4)) +  \n  labs(y= \"Color index\",\n       x= \"Feather type\",\n       title=\"Comparing odd and typical feathers in Northern flickers\")+\n  guides(color=F)\n\n\n\n\nWhen this is true, we need to consider these connections."
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#blocking-two-way-anovasand-paired-t-tests",
    "href": "content/chapters/More_ANOVAs.html#blocking-two-way-anovasand-paired-t-tests",
    "title": "More ANOVAs",
    "section": "Blocking, two-way ANOVAs,and paired t-tests",
    "text": "Blocking, two-way ANOVAs,and paired t-tests\nIn this case the connections may be considered artifacts of the data. We didn’t assign birds. We also made a choice to compare odd and typical feathers from the same bird - why? In general, accounting for extra variation in the data will give you a better answer about how a given variable influences outcomes. This may be called blocking. Although the motivation might therefore be to get a “better” p value, it should be driven by experimental design (and thus we started with an example where we didn’t “need” to account for it to achieve significance).\nIn order to consider how color differs by bird and feather type, we need to add both variables to our model. This is possible because, as we noted earlier, we can subdivide variance among multiple levels. For each variable we add, we also add a null (and corresponding alternative) hypothesis. So we retain our focus on feather type:\n\\[\n\\begin{split}\nH_O: \\mu_{\\textrm{odd feather color}} = \\mu_{\\textrm{typical feather color}}\\\\\nH_A: \\mu_{\\textrm{odd feather color}} \\neq \\mu_{\\textrm{typical feather color}}\\\\\n\\end{split}\n\\]\nbut also add a set of hypotheses focused on birds:\n\\[\n\\begin{split}\nH_O: \\mu_{\\textrm{color of bird A}} = \\mu_{\\textrm{color of bird B}}....\\textrm{for all birds}\\\\\nH_A: \\mu_{\\textrm{color of bird A}} \\neq \\mu_{\\textrm{color of bird B}}....\\textrm{for all birds}\\\\\n\\end{split}\n\\]\nWe can analyze this using our linear model approach. Since both variables are categorical, this is often called a two-way ANOVA. First, let’s make the object\n\ntwo_way_anova_example &lt;- lm(Color_index ~ Feather_type + Bird, data=feather)\n\nThen check the assumptions\n\nplot(two_way_anova_example)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote, visually speaking, the residuals do appear to be closer to normal now. Since assumptions look ok, we can analyze the outcome\n\nsummary(two_way_anova_example)\n\n\nCall:\nlm(formula = Color_index ~ Feather_type + Bird, data = feather)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.12444 -0.05209  0.00000  0.05209  0.12444 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -0.35806    0.06955  -5.148 0.000119 ***\nFeather_typeTypical  0.13712    0.03374   4.065 0.001017 ** \nBirdB                0.09050    0.09542   0.948 0.357936    \nBirdC                0.04500    0.09542   0.472 0.643998    \nBirdD                0.12500    0.09542   1.310 0.209903    \nBirdE                0.25350    0.09542   2.657 0.017950 *  \nBirdF                0.25750    0.09542   2.699 0.016505 *  \nBirdG                0.15000    0.09542   1.572 0.136802    \nBirdH                0.25250    0.09542   2.646 0.018330 *  \nBirdI                0.28850    0.09542   3.023 0.008554 ** \nBirdJ                0.21500    0.09542   2.253 0.039643 *  \nBirdK                0.25300    0.09542   2.651 0.018139 *  \nBirdL                0.14450    0.09542   1.514 0.150719    \nBirdM                0.13650    0.09542   1.431 0.173069    \nBirdN                0.21900    0.09542   2.295 0.036567 *  \nBirdO                0.25300    0.09542   2.651 0.018139 *  \nBirdP                0.22750    0.09542   2.384 0.030759 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09542 on 15 degrees of freedom\nMultiple R-squared:  0.7304,    Adjusted R-squared:  0.4427 \nF-statistic: 2.539 on 16 and 15 DF,  p-value: 0.03923\n\nAnova(two_way_anova_example, type= \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Color_index\n              Sum Sq Df F value   Pr(&gt;F)    \n(Intercept)  0.24133  1 26.5059 0.000119 ***\nFeather_type 0.15043  1 16.5214 0.001017 ** \nBird         0.21950 15  1.6072 0.184180    \nResiduals    0.13657 15                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote we see a significant difference in color among birds and feather type. Although we may be tempted to (and could) use post-hoc tests to consider which birds are different than which others, this is typically not done for blocked variables. We did not assign these pairings and it is not the focus of our efforts.\nSince we only had 2 types of feathers, we also don’t need post-hoc tests. A significant p value means they differ from each other, and the estimates provided by the summary command indicate the typical feathers have a higher color index.\n\nt-test connections\nWhen we have only two measurements per group (e.g., odd and typical feathers from each bird), we can use a t-test approach to achieve similar goals. This approach is known as a paired t-test. Instead of focusing on the difference in means (like a 2-sample t-test), the test focuses on the mean difference between paired measurements (which would be 0 under the null hypothesis!). In this way, it is effectively a one-sample test that is pairing the data to reduce variation (blocking). We can do carry out the test:\n\nt.test(Color_index ~ Feather_type, data=feather, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  Color_index by Feather_type\nt = -4.0647, df = 15, p-value = 0.001017\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.20903152 -0.06521848\nsample estimates:\nmean difference \n      -0.137125 \n\n\nand get the same results as above (note we don’t even have to consider corrections like the Welch approach since this a one-sample test)\nIn an earlier chapters we considered options for one- and two-sample tests when t-tests assumptions were not met. For two-sample tests, one of these approaches, the sign or binary test, is only valid for paired data. The differences in paired observations are compared to a set value (typically 0). Under the null hypothesis, half should be below the proposed median and half should be above. Differences matching the proposed value are ignored, thus reducing the sample size and making it harder to reject the null hypothesis; this is actually an odd way of accounting for them. The proportion of values below the proposed median is then evaluated using a binomial test. For two sample, the SIGN.test function in the BSDA package requires 2 columns of data and assumes the order of the column represents paired data.\n\nlibrary(BSDA)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'BSDA'\n\n\nThe following objects are masked from 'package:carData':\n\n    Vocab, Wool\n\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\nSIGN.test(feather[feather$Feather_type == \"Odd\", \"Color_index\"], \n          feather[feather$Feather_type == \"Typical\", \"Color_index\"],\n          md = 0)\n\n\n    Dependent-samples Sign-Test\n\ndata:  feather[feather$Feather_type == \"Odd\", \"Color_index\"] and feather[feather$Feather_type == \"Typical\", \"Color_index\"]\nS = 3, p-value = 0.02127\nalternative hypothesis: true median difference is not equal to 0\n95 percent confidence interval:\n -0.24048275 -0.02331055\nsample estimates:\nmedian of x-y \n       -0.114 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level  L.E.pt  U.E.pt\nLower Achieved CI     0.9232 -0.2400 -0.0320\nInterpolated CI       0.9500 -0.2405 -0.0233\nUpper Achieved CI     0.9787 -0.2410 -0.0140\n\n\n\n\nMore than 2 measurements? Back to the linear model\nWe can also block for variation when we take more than 2 measurements per unit. For example, imagine if these birds also had a special, long tail feather.\n\nset.seed(25)\nspecial &lt;- data.frame(Bird = LETTERS[1:16], Feather_type = \"Special\", \n                      Color_index= feather[feather$Feather_type == \"Typical\", \"Color_index\"] +\n                        .3 +runif(16,1,1)*.01)\nfeather_extra &lt;- merge(feather, special, all = T)\nfeather_extra$Feather_type &lt;- factor(feather_extra$Feather_type)\n\nWe could still block for variation using the linear model/ANOVA, but not the t-test, approach. As another review, we create the model\n\nmore_blocks &lt;-lm(Color_index ~ Feather_type + Bird, data=feather_extra)\n\nCheck assumptions\n\nplot(more_blocks)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck outcome (this time focusing on Anova output)\n\nAnova(more_blocks, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Color_index\n              Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept)  0.36392  1  59.9538 1.224e-08 ***\nFeather_type 1.67906  2 138.3093 7.208e-16 ***\nBird         0.34649 15   3.8055 0.0008969 ***\nResiduals    0.18210 30                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe still see feather type has a significant impact on color, but since we have more than 2 groups we need to follow up this finding with a post-hoc test.\n\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\ncompare &lt;- glht(more_blocks, linfct = mcp(Feather_type = \"Tukey\"))\nsummary(compare)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Color_index ~ Feather_type + Bird, data = feather_extra)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nSpecial - Odd == 0      0.44713    0.02755  16.232   &lt;1e-04 ***\nTypical - Odd == 0      0.13713    0.02755   4.978   &lt;1e-04 ***\nTypical - Special == 0 -0.31000    0.02755 -11.254   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#other-ways-to-be-in-multiple-groups",
    "href": "content/chapters/More_ANOVAs.html#other-ways-to-be-in-multiple-groups",
    "title": "More ANOVAs",
    "section": "Other ways to be in multiple groups",
    "text": "Other ways to be in multiple groups"
  },
  {
    "objectID": "content/chapters/More_ANOVAs.html#next-steps",
    "href": "content/chapters/More_ANOVAs.html#next-steps",
    "title": "More ANOVAs",
    "section": "Next steps",
    "text": "Next steps\nOur following chapters will extend ANOVAs to consider the impact of multiple measured categories. In doing so, we will also explain paired t-tests and sign tests for paired data. For this test, the sign of the differences among paired observations is considered (negative or not) and a binomial test is then used to compare the now binary data to .5 (expectation under the null hypothesis)."
  },
  {
    "objectID": "content/Binomial.html",
    "href": "content/Binomial.html",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "",
    "text": "In this chapter will build on our previous exploration of estimation by considering the world of hypothesis testing. These are different but related ideas, and we’ll end the section showing why. Along the way we will introduce the p-value. We will do all this while considering binomial tests, which are some of the simplest data we will see."
  },
  {
    "objectID": "content/Binomial.html#example-does-age-of-birds-impact-their-likelihood-of-colliding-with-glass",
    "href": "content/Binomial.html#example-does-age-of-birds-impact-their-likelihood-of-colliding-with-glass",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Example: Does age of birds impact their likelihood of colliding with glass?",
    "text": "Example: Does age of birds impact their likelihood of colliding with glass?\nLet’s start with an example. Klem Klem (1989) wanted to know if various factors (e.g., age, sex) of birds impacted the probability they would collide with glass windows. He collected data from several areas. In one of his samples, he found 18 purple finches collided with glass windows. 9 of these were in their hatching year (we’ll call them younger), and 9 were older. Is there any evidence that age impacts the probability of a purple finch colliding with the glass?\n\n\n\nCephas, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\n\n\nWhat have we done with data like this so far? You should know to calculate the proportion of each category impacting the probability of either category being represented in the sample. For example, since there were 9 older birds and 18 total, the proportion of older birds in the sample was:\n\n9/18\n\n[1] 0.5\n\n\nWe could also graph this, but it wouldn’t be very interesting:\n\nlibrary(ggplot2)\nfinch_data &lt;- data.frame(age = c(\"younger\", \"older\"), collisions = c(9,9))\nggplot(finch_data, aes (x=age, y = collisions))+\n  geom_col()+\n  labs(x=\"Age\", y= \"Collisions\", title = \"No apparent difference in sample based on age\")\n\n\n\n\nAnd although we haven’t discussed it, you should understand we could develop a confidence interval fr this type of data (we’ll do so below). That would tell us the range of proportions we might typically expect. But does that really answer the question of whether age impacts likelihood of colliding with glass?"
  },
  {
    "objectID": "content/Binomial.html#welcome-to-hypothesis-testing",
    "href": "content/Binomial.html#welcome-to-hypothesis-testing",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Welcome to hypothesis testing",
    "text": "Welcome to hypothesis testing\nTo answer that question, we have to move to hypothesis testing. This approach focuses on if a given value we found in the data (we’ll call it a signal) is really different (or significantly different) from what we would expect to see for a a given set of circumstances given the sampling error we now know to expect when we sample.\nThe given set of circumstances are described by a null hypothesis (this is why this approach is sometimes called NHST, or null hypothesis significance testing). We often abbreviate this as Ho. Let’s start by comparing this to estimation. Given our data, we could develop a 95% confidence interval (theoretically) that you should now understand will capture the true signal of the data about 95% of the time (here we are using proportion as opposed to mean, but it works). That’s a slightly different approach than asking if the true mean is equal to a given number, which is what hypothesis testing asks. Both deal with sampling error and explain why we can’t simply say an estimate being different than a given value proves there is a difference (make sure you understand why!).\nFor hypothesis testing in general, we again generate a known population that we draw from multiple times (should sound familiar), but this time the population parameters are set by a null hypothesis. Then we compare the spread of signals from those multiple draws (which exist due to sampling error!) to what we actually observed to determine how likely our draw was given the null hypothesis was true. If it’s unlikley to have occured by chance under the null hypothesis, we consider that evidence the null hypothesis is not correct and (eventually) reject it.\nYou can typically think of a null hypothesis as a hypothesis of no difference, affect, or relationship. Let’s walk through this with our bird example, where our null hypothesis would state age (measured as a category here!) has no impact on collisions. Given that, what would we expect to see in our sample?\nThis is a tricky question (that I chose on purpose!). Many approaches to this question start with a 50/50 expectation (like flipping a coin), but I’ve found that confuses students into thinking that is always the answer. Instead, think about what we would expect to see if age had no impact on collisions. We would not necessarily expect a 50/50 split in older and younger birds because that may not be what the population looks like. In fact, previous research has suggested the population is split closer to 3:1, with 3 older birds for every younger bird. This means if age has no impact on collisions, we should see about (due to sampling error!) 3 older birds for every younger bird in our samples of birds that hit glass.\nWhat did we actually see? We saw 5 old birds and 5 young birds. That is not a 3:1 split, but its also a small sample size. If we had a population with a 3:1 split and randomly selected 10 birds from it, how rare would it be to get 5 younger and 5 older birds? That’ (close) to what we are asking.\nIn this case, our null hypothesis is comparing our signal to a set value. This is common when we measure a single group and want to compare it to something. So our null hypothesis could be written as conceptually as age does not impact the probabilty a bird collides with glass. However, its often better (in order to connect it to tests!) to write it using numbers. In this case, we could write\n\\[\nH_O: p=.75\n\\]\nwhere p is the probability of a bird in our sample being old (for a single draw), or what we expect on average over larger samples (a proportion!). Remember, to find a proportion, you count the number of samples that fall in a given group and divided that by the total number sampled. Alternatively, you can assign a score of 0 for values that are not in the focal group and a score of 1 to samples that are - the average of these scores will give you the proportion.\nNote we could instead focus on young birds and get:\n\\[\nH_O: p=.25\n\\]\nWe also have alternative hypotheses (abbreviated HA)to accompany each of these. Our alternative is just the opposite of the null. Together, they encompass all the probability space. It is usually just as simple as switching signs. For example, if we focus on older birds, we get\n\\[\n\\begin{split}\nH_O: p=.75 \\\\  \nH_A: p \\neq .75\n\\end{split}\n\\]\nThe above ideas stay the same for all NHST approaches! We always use the null hypothesis to generate a “known” population (sometimes called the null population, draw samples from it, and then compare it to what we actually observed. What changes based on data type is how we generate the sample and multiple draws."
  },
  {
    "objectID": "content/Binomial.html#binomial-data",
    "href": "content/Binomial.html#binomial-data",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Binomial data",
    "text": "Binomial data\nThis example focuses on independent data points (one does not impact any others) that can be divided into 2 outcomes (young and old in our example). That is known as binomial data (so if data can be divided into two categories, we call it binomial data). A special case of binomial data exists when we only get one organism in our sample (e.g., one bird, one coin flip). We call this Bernoulli data.\nFor any type of data, we can simulate a distribution under the null hypothesis. For this example, we could put 4 pieces of paper in a hat, 3 labelled older and 1 labelled younger. We can then draw a sample of 18 (the number we actually observed) by drawing a piece of paper, writing down what it says, returning it to the hat, and repeating the process 9 more times to get single sample. For each sample, you could then calculate the observed proportion of older birds. You could visualize the spread of those results using a histogram. It’s important to realize this is doable without a computer (think it through), but it would take a lot of time because you need a lot of samples (we’ll come back to this).\nFor now, let’s do it with the computer. Let’s also take a shortcut: Instead of younger and older, let’s label the pieces of paper 0 and 1. We will also call the 0’s failures and the 1’s successes. Then we can sum the draws and divide by 10 to get the proportion of successes (make sure you understand why!). For now, let’s do a 1000 random draws of 18.\n\nset.seed(42)\nchoices &lt;- c(rep(0,1),rep(1,3))\nnumber_of_draws &lt;- 18\nnumber_of_simulations &lt;- 1000\n\nsampling_experiment&lt;- data.frame(\"observed_proportion\" = rep(NA, number_of_simulations))\nfor(i in 1:number_of_simulations){\nsampling_experiment$observed_proportion[i] = sum(sample(choices,number_of_draws, replace=T))/number_of_draws\n}\n\nLet’ s take a look at the first few draw\n\nhead(sampling_experiment$observed_proportion)\n\n[1] 0.6111111 0.7222222 0.8333333 0.8333333 0.6111111 0.7222222\n\n\nNote we see some variation. Also note it is impossible to get a proportion of .75. Why? We only sampled 18 individuals, so we can’t get any outcomes that aren’t some form of a whole number less than 18 divided by 18. This seems simple, but it’s a reminder that your signal being different than your hypothesized value is not sufficient to reject the null hypothesis!\nNow let’s plot the proportions from our sampling experiment:\n\nggplot(sampling_experiment,\n              aes(x=observed_proportion)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed proportions from 1000 random draws\",\n       x= \"Proportion\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nJust looking at this, it seems getting a proportion of .5 is unlikely. It only occurred 14 times. However, we also need to note how often more extreme outcomes occurred. Why?\nMore extreme values (the same or further distance away from the hypothesized value as our observed signal were) are also useful in considering if the null hypothesis is valid. When we move to continuous distributions, it’s also impossible to get a certain value (as mentioned in the probability section).\nIn this example, our observed proportion was .5. That’s .25 away from the value under the null hypothesis (.75), so we should all simulations that were . 5 or less or 1 or more. That only happened 22 times. So, in taking 1000 random draws from our null population, we only saw what we actually observed (or something more extreme) 0.022% of the time.\n\nsampling_experiment$Proportion = ifelse(sampling_experiment$observed_proportion &lt;= .5, \n                                  '&lt;= .5', ifelse(sampling_experiment$observed_proportion &gt;= 1, '&gt;.5 & &lt; 1', '&gt;= 1'))\n\n\n\n\nggplot(sampling_experiment,\n              aes(x=observed_proportion, fill=Proportion)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed proportions from 1000 random draws\",\n       x= \"Proportion\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOr to think about as or more extreme..\n\nsampling_experiment$Proportion2 = ifelse(abs(sampling_experiment$observed_proportion-.75) &gt;= abs(9/18-.75), 'as or more extreme', 'not as or more extreme')\n\nggplot(sampling_experiment,\n              aes(x=observed_proportion, fill=Proportion2)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed proportions from 1000 random draws\",\n       x= \"Proportion\",\n       y= \"Frequency\", \n       fill = \"Proportion\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/Binomial.html#welcome-to-the-p-value",
    "href": "content/Binomial.html#welcome-to-the-p-value",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Welcome to the p-value",
    "text": "Welcome to the p-value\nThis is a p-value. Don’t get confused! We will get p-values from multiple tests, but the binomial distribution also has a p parameter (the proportion). They are not the same.\nExplaining p-values is hard! You can see some statisticians try to explain the concept here.\nA smaller p-value therefore means it is less likely to obtain your observed signal, or something more extreme, by chance when the null hypothesis is true. Traditionally, a p-value of less than .05 is thought to be sufficient evidence to reject the null hypothesis. This comparison value is sometimes called the \\(\\alpha\\) (alpha), or significance, level. So if our p-value is &lt; .05, we often say we have significant evidence against HO. While we now often get specific p-values from software, historically people used tables to find ranges (less than .05, for example)."
  },
  {
    "objectID": "content/Binomial.html#understand-what-this-implies",
    "href": "content/Binomial.html#understand-what-this-implies",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Understand what this implies!",
    "text": "Understand what this implies!\nNote our p-value is the probability we would get a signal like we observed by chance if the null hypothesis was true. This means for an \\(\\alpha\\) of .05,we would expect to see something this extreme by chance 1 time out of 20! In other words, we can have errors. Think about it this way:\n\n\n\n\n\n\n\n\n\nBiological reality\n\n\n\n\n\nDecision (based on analysis of sample data)\nHO True\nHO False\n\n\nReject HO\nType I error (P[ɑ])\nPower (1- \\(\\beta\\) )\n\n\nDo not reject HO\nCorrect (P[1- ɑ])\nType II Error ( Pr\n[\\(\\beta\\)])\n\n\n\n\\(\\alpha\\) sets the limit we are ok with for rejecting HO when it is true (a Type 1 error). Alternatively, a Type II error is when we do not reject HO even when it’s is false. Importantly, \\(\\alpha + \\beta \\neq 1\\)! Instead, \\(\\alpha + (1-\\alpha)\\) is the probability space for HO, and \\(\\beta + (1-\\beta)\\) (or \\(\\beta + power\\)) is the probability space for HA. How they overlap depends on the signal, as the the distribution of signals under HA is close to what we already estimated for confidence intervals! You can visualize the relationship using the image below. Note the code is hidden given it’s length!\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `show_guide` argument of `layer()` is deprecated as of ggplot2 2.0.0.\nℹ Please use the `show.legend` argument instead.\n\n\n\n\n\nThe key point is the experimenter sets HO and \\(\\alpha\\). Here we clearly see that in a typical test (like what we illustrated above) \\(\\alpha\\) is split among the top and bottom of the distribution of signals under HO to create rejection regions. Note if we decrease \\(\\alpha\\), which we can, we also decrease the power of the test! On a related note, we can return to an earlier image\n\n\n\nA 3D visualisaion of PPV, NPV, Sensitivity and Specificity. Luigi Albert Maria, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\n\n\nand note that power is equal to sensitivity!\nThese issues come up in power analysis, which is way of using prior estimates of the distribution of signals to determine appropriate sample sizes needed to detect significant results. Another form of power analysis occurs after a test is carried out, but this basically rehashing the p-value Levine and Ensom (2001) Heckman, Davis, and Crowson (2022).\nThis all points to a central idea of NHST. Larger sample sizes let you pick up smaller differences among groups! We will develop this below and consider relationships among significance and importance!"
  },
  {
    "objectID": "content/Binomial.html#can-we-do-this-without-running-a-sampling-experiment-every-time",
    "href": "content/Binomial.html#can-we-do-this-without-running-a-sampling-experiment-every-time",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Can we do this without running a sampling experiment every time?",
    "text": "Can we do this without running a sampling experiment every time?\nAs shown above, we can always use simulations to obtain a p-value. However, without a computer (and even with) it’s cumbersome. We also have to redo it for every change (for example, what if our sample contained 19 instead of 18 birds?). Another option is to find an algorithm that can be used to calculate a distribution that is very close to what we saw with the simulation.\nIn the case of our binomial data, very close actually means exact. The binomial data is an example of data where we can fully describe the probability outcomes a sample may take. The binomial distribution allows calculations of how often you would expect s successes for a set number of trials (n-s) if a population had a proportion of p for the focal trait. This means we use the binomial distribution to calculate our probabilities.\nLet’s not derive this fully, but just think about it. We have a proportion of success p, so (1 - p) is equal to the probability of failure (since we only have 2 options). For variance, we noted we find the average squared distance from the focal parameter value (in this case, a proportion).\nFor a single draw (what we call Bernoulli data), if we assume a success is equal to 1 and a failure to zero, we could “simply” multiply the likelihood of each our outcomes by their average squared distance from the mean\n\\[ \\begin{split} \\sigma^2 = (0-p)^2(1-p)+(1-p)^2(p)\\\\ which \\ eventually \\ reduces \\  to \\\\ \\sigma^2 =p(1-p) \\end{split} \\]\nSince we are assuming each data point is independent ( remember the multiplication rule?), the probability distribution of getting S successes from N draws will be\n\\[ Pr[S] =p^S(1-p)^{N-S} \\]\nand the variance will be\n\\[\n\\sigma_\\mu^2 =Np(1-p)\n\\]\nsince when you add independent events, you multiply the variances.\nSince we don’t care about the order of successes and failures in the sample, we have to think about combinations (not developed here), or how many ways one can arrange s successes in n draws. Putting it together, we can write the binomial distribution as\n\\[\nP[n \\ successes] = {n\\choose s}p^s(1-p)^{n-s}\n\\]\nUsing this distribution we can ask for the probability of obtaining any given number of successes for a given sample size. We can then find how likely we were to see a signal as more extreme than what we actually observed in the data by chance if the null hypothesis is true (p-value!). The dbinom function in R uses this distribution.\n\nsum(dbinom(0:9,18,.75))+dbinom(18,18,.75)\n\n[1] 0.02498549\n\n\nThis distributional assumptions also powers the binomial test (also called the exact binomial test). In R, we can use the binom.test function to carry it, with the arguments\n\nx=number of successes\n\nnow you see why we called them successes!\n\nn = total number of trials\np= expected proportion under the null hypothesis\n\n\nbinom.test(x=9, n=18, p=.75)\n\n\n    Exact binomial test\n\ndata:  9 and 18\nnumber of successes = 9, number of trials = 18, p-value = 0.02499\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.2601906 0.7398094\nsample estimates:\nprobability of success \n                   0.5 \n\n\nNote for this test the default value for p is .5 (equal chance), so if you don’t enter it that’s what will be used.\nNotice all our p-values are fairly close. P-values obtained using the distributional assumptions match exactly., and that obtained by simulation is very close. It should also be noted that although the p-value obtained by simulation will vary slightly each time, while those obtained using the binomial distribution will stay the same."
  },
  {
    "objectID": "content/Binomial.html#impact-of-sample-size",
    "href": "content/Binomial.html#impact-of-sample-size",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Impact of sample size",
    "text": "Impact of sample size\nNow that we can “easily” run a binomial test, let’s do it a few times to see the impact of sample sizes. For example, we could see the same proportion/signal (50%) of older birds in our sample, but if we only collected 8 individuals we would not be able to reject HO. Note what happens to our simulation outcomes:\n\nnumber_of_draws &lt;- 8\nnumber_of_simulations &lt;- 1000\n\nsampling_experiment&lt;- data.frame(\"observed_proportion\" = rep(NA, number_of_simulations))\nfor(i in 1:number_of_simulations){\nsampling_experiment$observed_proportion[i] = sum(sample(choices,number_of_draws, replace=T))/number_of_draws\n}\nsampling_experiment$Proportion = ifelse(sampling_experiment$observed_proportion &lt;= .5, \n                                  '&lt;= .5', ifelse(sampling_experiment$observed_proportion &gt;= 1, '&gt;.5 & &lt; 1', '&gt;= 1'))\n\n\nggplot(sampling_experiment,\n              aes(x=observed_proportion, fill=Proportion)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed proportions from 1000 random draws\",\n       x= \"Proportion\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSo now, we find that outcomes that are as or more extreme than what we saw in the actual data occur 2 times. So, in taking 1000 random draws from our null population, we only saw what we actually observed (or something more extreme) 0.002% of the time. Similarly,\n\nbinom.test(4,8,p=.75)\n\n\n    Exact binomial test\n\ndata:  4 and 8\nnumber of successes = 4, number of trials = 8, p-value = 0.1138\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.1570128 0.8429872\nsample estimates:\nprobability of success \n                   0.5 \n\n\nleads to a p-value which is &gt;.05, so we fail to reject HO. Again, this relates to how sampling error interacts with sample size, much as we saw when constructing confidence intervals. This means we have to differentiate between statistical significance and importance.\nGiven a large enough sample size, we can detect very small differences from our parameter value under the null hypothesis. For example, what if data from another population of finches showed 780 older birds out of a sample of 1000 birds that collided with windows. If we assume the population distribution in regards to age is the same, we are still testing\n\\[\n\\begin{split}\nH_O: p=.75 \\\\  \nH_A: p \\neq .75\n\\end{split}\n\\]\nIn our sample, we found a signal of 0.78, which is very close to .75. However, we find a p-value of\n\nbinom.test(780,1000, p = .75)\n\n\n    Exact binomial test\n\ndata:  780 and 1000\nnumber of successes = 780, number of trials = 1000, p-value = 0.02843\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.7530202 0.8053200\nsample estimates:\nprobability of success \n                  0.78 \n\n\nIs the slight increase in older birds really important to understanding the population? Maybe, or maybe not. The point is we have to understand the difference between estimates and significance and the more nebulous idea of importance."
  },
  {
    "objectID": "content/Binomial.html#estimates-and-p-values-work-together",
    "href": "content/Binomial.html#estimates-and-p-values-work-together",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Estimates and p-values work together",
    "text": "Estimates and p-values work together\nThis is one way estimates and NHST work together. Estimate focuses on the sample (Given sampling error, where do we think true parameter lies?). Hypothesis testing focuses on the likelihood of the signal given the null distribution (how likely were we to observe data that we did, a la the p-value), but gives no information about the actual difference (which could be important for determining if something really matters!)."
  },
  {
    "objectID": "content/Binomial.html#one-sided-tests",
    "href": "content/Binomial.html#one-sided-tests",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "One-sided tests",
    "text": "One-sided tests\nIn introducing the p-value (and estimation) we focused on two-sided (or two-tailed) tests. This means we considered deviations from our value under the null hypothesis (for p-values) or via sampling error (for confidence intervals) based on their magnitude, and not direction. However, we can instead decide we want to consider differences to one “side” of our value of interest. Following this idea, we have 3 options for our null and alternative hypotheses (note C is a constant value here!):\n\n\n\n\n\n\n\n\n\n\n\n*Two-sided (\ntypical)**\n\nFocused on signals greater than predicted in null\nFocused on signals less than predicted in null\n\n\nHO\np = C\np &lt;= C\np &gt;= C\n\n\nHA\np \\(\\neq\\) C\np &gt; C\np &lt; C\n\n\n\nFor example, Claramunt et al Claramunt, Hong, and Bravo (2022) wished to consider if roads impaired bird movement. To do they considered if banded birds were more likely to be recapture in one of 3 areas across a road from their original location or one of 6 on the same side on which they were captured. They were only interested if roads reduced bird movement, so they were justified in using a sided test. These tests move all the rejection region to one side. You can run these by adding an alternative argument to binom.test\n\nbinom.test(116,641, p=.33, alternative = \"less\")\n\n\n    Exact binomial test\n\ndata:  116 and 641\nnumber of successes = 116, number of trials = 641, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is less than 0.33\n95 percent confidence interval:\n 0.0000000 0.2078378\nsample estimates:\nprobability of success \n             0.1809672 \n\n\nHere we reject HO, where\n\\[\n\\begin{split}\nH_O: p&gt;=.33 \\\\\nH_A: p &lt; .33\n\\end{split}\n\\]\nHowever, sided or tailed tests should be rarely used? Why? Because it can be too tempting to use a sided test after observing the data! A signal that is not significant at the \\(\\alpha\\) =.05 level using two-sided tests can be significant as a one-tailed test.\nIf you do use these, note they correspond to confidence bounds instead of intervals. Again, the full rejection region is placed on one side of the estimate."
  },
  {
    "objectID": "content/Binomial.html#tying-it-all-together",
    "href": "content/Binomial.html#tying-it-all-together",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Tying it all together",
    "text": "Tying it all together\nLet’s return to our bird collision example and connect estimation and p-values (and teach you how to estimate confidence intervals for binomial data).\nRemember, we found 9 older birds in our sample of 18. This means our estimate for the proportion of older birds 0.5. Just like for continuous data, we can consider sampling error in our estimate. Let’s think about how that might happen.\n\n\nIn short, the standard error of p is\n\\[\nSE(p) = \\sqrt{\\frac{p(1-p)}{N}}\n\\]but since we don’t know p, we use our estimate\n\\[\nSE(\\hat{p}) = \\sqrt{\\frac{(\\hat{p}(1-\\hat{p})}{N}}\n\\]\nTo find out a little more, click here.\n\nFor a single draw (Bernoulli data), if we assume a success is equal to 1 and a failure to zero, we could “simply” multiply the likelihood of each our outcomes by their average squared distance from the mean\n\\[\n\\begin{split}\n\\sigma^2 = (0-p)^2(1-p)+(1-p)^2(p)\\\\\nwhich \\ eventually \\ reduces \\  to \\\\\n\\sigma^2 =p(1-p)\n\\end{split}\n\\]\nIf we move to N independent draws, we predict the average observed outcome (or the mean number of successes!) will be\n\\[\n\\mu_S = Np\n\\]\nSince we are assuming each data point is independent, the variance of N draws will be\n\\[\n\\sigma_\\mu^2 =Np(1-p)\n\\]\nso\n\\[\n\\sigma_\\mu =\\sqrt{Np(1-p)}\n\\]\nNotice in doing this we went from a proportion to a number of successes! Now we can use our typical equation for standard error of the means\n\\[\n[\\sigma_{\\mu_s} = \\frac{\\sigma}{\\sqrt{N}} = \\frac{\\sqrt{(Np(1-p)}}{\\sqrt{N}}  = \\sqrt{\\frac{(p(1-p)}{N}} \\ ] \\sim [s_{\\overline{Y}} = \\frac{s}{\\sqrt{N}} =   \\frac{\\sqrt{(N\\hat{p}(1-\\hat{p})}}{\\sqrt{N}} = \\sqrt{\\frac{(\\hat{p}(1-\\hat{p})}{N}}]\n\\]\nThis is actually a bit tricky as it assumes some connections between categorical and continuous data, and many ways have been proposed to do this (Subedi and Issos 2019), but this gets us close.\n\nIt turns out our estimate of the standard error may be biased, especially for small sample sizes or extreme (close to 0 or 1) values of p. For that reason, several ways have been suggested to calculate confidence intervals (Subedi and Issos 2019) . Note, for example, the binom.confint function in the binom package gives multiple outcomes. For the function,\n\nthe first argument is the number of successes\nthe second argument is the number of trials\n\n\nlibrary(binom)\nbinom.confint(9,18)\n\n          method x  n mean     lower     upper\n1  agresti-coull 9 18  0.5 0.2903102 0.7096898\n2     asymptotic 9 18  0.5 0.2690160 0.7309840\n3          bayes 9 18  0.5 0.2835712 0.7164288\n4        cloglog 9 18  0.5 0.2592888 0.7005143\n5          exact 9 18  0.5 0.2601906 0.7398094\n6          logit 9 18  0.5 0.2841566 0.7158434\n7         probit 9 18  0.5 0.2812976 0.7187024\n8        profile 9 18  0.5 0.2808406 0.7191594\n9            lrt 9 18  0.5 0.2808092 0.7191908\n10     prop.test 9 18  0.5 0.2903102 0.7096898\n11        wilson 9 18  0.5 0.2903102 0.7096898\n\n\nFor now, we will use method labelled the Agresti-Coull method, which adjusts for slight bias in other estimates and is useful across sample sizes.\n\nusing_distribution &lt;- dbinom(0:18,18,.75)\nfinches &lt;- data.frame (Number = 0:18, Probability = using_distribution)\nfinches$Proportion &lt;- finches$Number/18\nfinches$criteria &lt;- \"retain\"\nfinches$criteria[pbinom(finches$Number, 18, .75) &lt; .025] &lt;- \"reject\"\nfinches$criteria[(1-pbinom(finches$Number, 18, .75)) &lt; .025] &lt;- \"reject\"\nproportion_observed = data.frame(Proportion = 9/18, Probability = .15)#sets height\nggplot(finches, aes(x = Proportion, y = Probability)) + \n  geom_bar(stat=\"identity\", aes(fill = criteria)) + \n  geom_segment(x = .29031, xend = .70968,y= .15 , yend =.15) +\n  geom_vline(xintercept = .75, color = \"blue\") + geom_vline(xintercept = 9/18, color = \"black\") +\n  geom_point(data= proportion_observed) +\n  ggtitle(\"Comparing p-values and confidence intervals for finch problem\")\n\n\n\n\nNote we see our rejection region in red; it also contains our estimate! Similarly, the 95% confidence interval for our estimate does not contain the paramater value under the null hypothesis!"
  },
  {
    "objectID": "content/Binomial.html#next-steps",
    "href": "content/Binomial.html#next-steps",
    "title": "The one where we introduce hypothesis testing via binomial tests",
    "section": "Next steps",
    "text": "Next steps\nMake sure you understand the above concepts (i.e., how p-values are is related to null hypotheses and how to interpret them!). Our following chapters will extend this idea to different types of data, starting with continuous data from a single sample in the next chapter."
  },
  {
    "objectID": "content/Acquiring_data.html",
    "href": "content/Acquiring_data.html",
    "title": "Acquiring data",
    "section": "",
    "text": "Let’s start our statistics journey by thinking about the simplest scenario: We want to know something about a group. An example might be the average (or mean, we will define later if needed!) value for some trait, the minimum value, or the maximum value. We could also wish to know about the distribution of values for that trait in the group. These traits of the group are called statistics:\n\nthe numerical facts or data themselves - Dictionary.com\n\nThis means we have a target trait we are focused on, and we have defined a group of interest. We can call this group of interest a population. Note that while the term population may have specific meanings in some fields (such as ecology), here population is just the group of interest. It could be a population of Goliath grouper in Florida, a population of flowers in Virginia, or people from a certain country or demographic group. We could want to know something about all of these groups!\nAs we’ve already noted, in a perfect world we know everything (or at least our trait value) for every member of the focal population. However, we often don’t or can’t measure every member of a population. It may be too difficult or expensive to measure every member of the population. In fact, we may not even know how large the population is!\nIn the cases where we can’t measure every member of the population, we collect data on the focal trait(s) from a sample. A sample is the subset of the population of interest. Data can be collected from samples used in experimental studies, where researchers manipulate something to see how it impacts the focal trait. Researchers may expose organisms to different stimuli in a controlled lab, field, or mesocosm study to see what happens. For example, researchers interested in impacts of an invasive crayfish (Pacifastacus leniusculus) on Mazama newts (Taricha granulosa mazamae) collected newts and crayfish.; they then placed either just newts or newts and crayfish in in large tanks to observe interactions Girdner et al. (2018).\n\n\n\nFigure 1: Experimental mesocosms used to evaluate Mazama newt and signal crayfish behavior on Wizard Island, Crater Lake, Oregon. A team of NPS scientists observed the interaction between newts and crayfish in tanks designed to mimic natural habitat.\n\n\nData can also be collected from observational studies, where researchers simply measure outcomes and other traits without manipulating anything. For example, scientists interested in impacts of climate change on species ranges surveyed sites for species presence and abundance and compared it to historical data (Sagarin et al. (1999)).\nDifferent types of studies change what we can use the data for. In general, experimental studies are more commonly used to ascertain causation (something makes something happen), whereas observational studies are used to assess correlation (something happens when something else happens). However, these can be hard to disentangle, especially since studies can only be observational since experiments would be unethical or impossible to carry out. As XKCD puts it\n\n\n\nFigure 2: XKCD: Correlation. Title text (text that pops up when you hover over the comic): Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’.\n\n\n\nCorrelation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’ - XKCD #552\n\nOnce we have the sample, we can measure the trait of interest in it, and use that to estimate the statistic of interest for the actual population. This is the science of statistics, which can actually be defined as\n\nthe practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample. - Oxford English Dictionary\n\nIf the whole idea of statistics is to infer something about the population from our sample, we need to make sure the sample is representative of the population. That means it should not be biased. Bias occurs if the trait values we measure in our sample differ from the population in a consistent way. This can happen with samples of convenience, or when researchers select samples that are easy to measure but may not be representative of the population. Classic examples include estimating the amount of time students spend studying by surveying students at a campus library.\nBias may also be related to issues of independence. In a good sampling design, every member of the population has the same chance of being included in a sample. Samples of convenience violate this premise, and often the underlying issue is that the samples are not independent. A perfect solution is to randomly choose members of the population to be in the sample, but that is often not possible. Again, it requires knowing every member of the population! Indepence also means each data point is not related to any others!\n\n\n\nFigure 3: XKCD: Slope Hypothesis Testing. Don’t worry, we’ll come back to significance - but what is the independence issue?\n\n\nIn some cases linkages among samples are impossible to avoid. We will cover ways to address that using blocking factors or random effects later.\nNotice in discussing bias we are not directly focusing on the quality of the measurements! For that, we could discuss accuracy (how well we measure the underlying trait in regards to its true value, which we typically don’t know) and precision (how repeatable our measurement technique is). Obviously we need good data to make good estimates, but these ideas are different from our current focus on picking a good sample.\nEven if we have a proper way to measure a trait (accurate and precise) in a good sample (not biased), we will still be producing an estimate of the population statistic! This is due to sampling error. Sampling error refers to the fact that every sample will produce a slightly different estimate of the statistic. Imagine this - there a 1000 fish in a lake. We sample 50 of them, measure their length, and use it calculate the average fish length. If we took a different sample, do you think it would have exactly the same average?\nWe can demonstrate this in R - you won’t understand the code below yet, so just trust me for now, but this will let you start seeing code and thinking about how to use it.\nLet’s generate a population of fish. We’ll store their lengths in a vector called lengths.\n\nlengths &lt;- rnorm(n=1000, mean = 10, sd=1)\n\nThe average length of fish in this population is 9.99 cm. We can then simulate a sample from this population. In fact, let’s simulate 2 and compare the means of each.\n\nsample_1 &lt;- sample(lengths,50)\nsample_2 &lt;-sample(lengths, 50)\n\nThe mean length for fish in sample 1 is 10.21 cm, while that in sample 2 is 10 cm (Note: if you view this on the webpage you will see a number, but in the actual qmd file you see R code here - this is an example of merging code and text!). These are both close to the true value, but they are also both slightly different - this is sampling error!\nSampling error always exists, and a major part of statistics is to quantify it. One thing that reduces sampling error is to have large samples! Remember, if we measure every member of the population we don’t even need statistics, so the closer we get to that (implying larger samples) the better!"
  },
  {
    "objectID": "content/Acquiring_data.html#how-do-we-get-data",
    "href": "content/Acquiring_data.html#how-do-we-get-data",
    "title": "Acquiring data",
    "section": "",
    "text": "Let’s start our statistics journey by thinking about the simplest scenario: We want to know something about a group. An example might be the average (or mean, we will define later if needed!) value for some trait, the minimum value, or the maximum value. We could also wish to know about the distribution of values for that trait in the group. These traits of the group are called statistics:\n\nthe numerical facts or data themselves - Dictionary.com\n\nThis means we have a target trait we are focused on, and we have defined a group of interest. We can call this group of interest a population. Note that while the term population may have specific meanings in some fields (such as ecology), here population is just the group of interest. It could be a population of Goliath grouper in Florida, a population of flowers in Virginia, or people from a certain country or demographic group. We could want to know something about all of these groups!\nAs we’ve already noted, in a perfect world we know everything (or at least our trait value) for every member of the focal population. However, we often don’t or can’t measure every member of a population. It may be too difficult or expensive to measure every member of the population. In fact, we may not even know how large the population is!\nIn the cases where we can’t measure every member of the population, we collect data on the focal trait(s) from a sample. A sample is the subset of the population of interest. Data can be collected from samples used in experimental studies, where researchers manipulate something to see how it impacts the focal trait. Researchers may expose organisms to different stimuli in a controlled lab, field, or mesocosm study to see what happens. For example, researchers interested in impacts of an invasive crayfish (Pacifastacus leniusculus) on Mazama newts (Taricha granulosa mazamae) collected newts and crayfish.; they then placed either just newts or newts and crayfish in in large tanks to observe interactions Girdner et al. (2018).\n\n\n\nFigure 1: Experimental mesocosms used to evaluate Mazama newt and signal crayfish behavior on Wizard Island, Crater Lake, Oregon. A team of NPS scientists observed the interaction between newts and crayfish in tanks designed to mimic natural habitat.\n\n\nData can also be collected from observational studies, where researchers simply measure outcomes and other traits without manipulating anything. For example, scientists interested in impacts of climate change on species ranges surveyed sites for species presence and abundance and compared it to historical data (Sagarin et al. (1999)).\nDifferent types of studies change what we can use the data for. In general, experimental studies are more commonly used to ascertain causation (something makes something happen), whereas observational studies are used to assess correlation (something happens when something else happens). However, these can be hard to disentangle, especially since studies can only be observational since experiments would be unethical or impossible to carry out. As XKCD puts it\n\n\n\nFigure 2: XKCD: Correlation. Title text (text that pops up when you hover over the comic): Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’.\n\n\n\nCorrelation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’ - XKCD #552\n\nOnce we have the sample, we can measure the trait of interest in it, and use that to estimate the statistic of interest for the actual population. This is the science of statistics, which can actually be defined as\n\nthe practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample. - Oxford English Dictionary\n\nIf the whole idea of statistics is to infer something about the population from our sample, we need to make sure the sample is representative of the population. That means it should not be biased. Bias occurs if the trait values we measure in our sample differ from the population in a consistent way. This can happen with samples of convenience, or when researchers select samples that are easy to measure but may not be representative of the population. Classic examples include estimating the amount of time students spend studying by surveying students at a campus library.\nBias may also be related to issues of independence. In a good sampling design, every member of the population has the same chance of being included in a sample. Samples of convenience violate this premise, and often the underlying issue is that the samples are not independent. A perfect solution is to randomly choose members of the population to be in the sample, but that is often not possible. Again, it requires knowing every member of the population! Indepence also means each data point is not related to any others!\n\n\n\nFigure 3: XKCD: Slope Hypothesis Testing. Don’t worry, we’ll come back to significance - but what is the independence issue?\n\n\nIn some cases linkages among samples are impossible to avoid. We will cover ways to address that using blocking factors or random effects later.\nNotice in discussing bias we are not directly focusing on the quality of the measurements! For that, we could discuss accuracy (how well we measure the underlying trait in regards to its true value, which we typically don’t know) and precision (how repeatable our measurement technique is). Obviously we need good data to make good estimates, but these ideas are different from our current focus on picking a good sample.\nEven if we have a proper way to measure a trait (accurate and precise) in a good sample (not biased), we will still be producing an estimate of the population statistic! This is due to sampling error. Sampling error refers to the fact that every sample will produce a slightly different estimate of the statistic. Imagine this - there a 1000 fish in a lake. We sample 50 of them, measure their length, and use it calculate the average fish length. If we took a different sample, do you think it would have exactly the same average?\nWe can demonstrate this in R - you won’t understand the code below yet, so just trust me for now, but this will let you start seeing code and thinking about how to use it.\nLet’s generate a population of fish. We’ll store their lengths in a vector called lengths.\n\nlengths &lt;- rnorm(n=1000, mean = 10, sd=1)\n\nThe average length of fish in this population is 9.99 cm. We can then simulate a sample from this population. In fact, let’s simulate 2 and compare the means of each.\n\nsample_1 &lt;- sample(lengths,50)\nsample_2 &lt;-sample(lengths, 50)\n\nThe mean length for fish in sample 1 is 10.21 cm, while that in sample 2 is 10 cm (Note: if you view this on the webpage you will see a number, but in the actual qmd file you see R code here - this is an example of merging code and text!). These are both close to the true value, but they are also both slightly different - this is sampling error!\nSampling error always exists, and a major part of statistics is to quantify it. One thing that reduces sampling error is to have large samples! Remember, if we measure every member of the population we don’t even need statistics, so the closer we get to that (implying larger samples) the better!"
  },
  {
    "objectID": "content/Acquiring_data.html#next-steps",
    "href": "content/Acquiring_data.html#next-steps",
    "title": "Acquiring data",
    "section": "Next steps",
    "text": "Next steps\nNow that we have data, we’ll discuss summarizing it in the next section (and actually define mean and some of the other terms we’ve started to use!)."
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Many thanks to Bill Rice and Steve Gaines at UCSB for encouraging me to continue my interests in statistics.\nMy department at Baruch also supported me when I proposed the Biostatistics (ENV/BIO 2100) course in 2017 and taught for the first time in 2018.\nBaruch College’s Center for Teaching and Learning, as a channel for a statewide funding effort focuse on developed OER (open-educational resources) at CUNY and SUNY campuses, have supported the continued development of the class.\nThe class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!\n\nThis repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/additional_resources.html",
    "href": "content/additional_resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "The class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!"
  },
  {
    "objectID": "content/additional_resources.html#class-related-materials",
    "href": "content/additional_resources.html#class-related-materials",
    "title": "Additional resources",
    "section": "",
    "text": "The class now includes\n\nwebsite (https://sites.google.com/view/biostats/home) housing slides and associated material\ntutorials for many lessons using Swirl\n\ndeveloped with support of a QUBES working group\n\nthis book!"
  },
  {
    "objectID": "content/additional_resources.html#other-resources",
    "href": "content/additional_resources.html#other-resources",
    "title": "Additional resources",
    "section": "Other resources",
    "text": "Other resources\nAs noted in the introduction, there are many, many resources that may assist you in your quest to learn statistics and R. Relevant ones are noted throughout the book and listed here.\n“Introduction to Data Science” (n.d.)\n“Welcome | r for Data Science” (n.d.)"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html",
    "href": "content/chapters/Compare_means_among_populations.html",
    "title": "Comparing means among groups",
    "section": "",
    "text": "In the last chapter we introduced the idea of comparing parameters among populations. Now we will extend those ideas to instances when continuous data is collected."
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#example-back-to-the-iris",
    "href": "content/chapters/Compare_means_among_populations.html#example-back-to-the-iris",
    "title": "Comparing means among groups",
    "section": "Example: Back to the iris",
    "text": "Example: Back to the iris\nWhen we introduced NHST for continuous data, we focused on sepal length from I. virginica.\n\nset.seed(42)\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nand tested if it was equal to a given value (7.0 cm)\n\\[\n\\begin{split}\nH_O: \\mu_{sepal \\ length} = 7 \\ cm \\\\\nH_A: \\mu_{sepal \\ length} \\neq 7 \\ cm\n\\end{split}\n\\]\nWe then considered how to assess these types of hypotheses using z, t, Wilcoxon, and sign tests.\nHowever, as we noted in the last chapter, we often instead have data from multiple populations. For example, we may have data from 3 species of flowers. We commonly see this data plotted as a bar chart with error bars\n\n\nLoading required package: lattice\n\n\nLoading required package: plyr\n\n\n\n\n\nWhat do we test now and how?"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#welcome-to-anovas",
    "href": "content/chapters/Compare_means_among_populations.html#welcome-to-anovas",
    "title": "Comparing means among groups",
    "section": "Welcome to ANOVAs",
    "text": "Welcome to ANOVAs\nAs previously noted, we can’t compare heights among group. Height is a random variable, and it’s highly unlikely they will be exactly the same. Seeing the actual data may help us remember this.\n\nggplot(iris, aes(y=Sepal.Length, x=Species, color=Species)) +\n  geom_jitter() +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")\n\n\n\n\nSo we typically focus on a parameter that describes the distribution of a focal trait, such as the mean. Just like binomial data, our hypotheses are then\n\\[\n\\begin{split}\nH_O: \\mu_{sepal \\ length, \\ setosa} = \\mu_{sepal \\ length, \\ virginica} = \\mu_{sepal \\ length, \\ versicolor}\\\\  \nH_A: \\mu_{sepal \\ length, \\ setosa} \\neq \\mu_{sepal \\ length, \\ virginica} \\neq \\mu_{sepal \\ length, \\ versicolor}\\\\\n\\end{split}\n\\]\nGiven that, our overall idea is to consider if the data are better explained by an overall group average or by species-specific averages. To visualize this, we could use\n\ncolors &lt;- c(\"group means\" = \"black\", \"overall average\" = \"orange\")\nggplot(iris, aes(Species,Sepal.Length)) + \n  geom_jitter(aes(colour=Species), size = 3) +\n  geom_errorbar(aes(ymin=Sepal.Length, ymax=Sepal.Length, color=\"group means\"), \n                data = function_output) +\n    geom_hline(aes(yintercept=mean(Sepal.Length),  color = \"overall average\"))+\n  scale_color_manual(values=colors)+\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\", \n       color= \"Focus\")\n\n\n\n\nLike our earlier considerations of variance and SSE, the data will obviously be fit better by species-specific averages. Its impossible for them to do worse than the overall average, and at worst they all are the group average. However, we should also remember that these are samples, so we know sampling error is an issue. Therefore, we have to consider if the species-specific averages do enough of a better job explaining the data to warrant using them. To put this in our SSE and hypothesis framework, we need to consider if a more complicated view of the world is worth it.\nTo test this, we can (as always) carry out a sampling experiment. The general idea is that species does snot matter (just like we saw in contingency analysis). Given that, we can draw samples that match our respective sample sizes for each population from a single population. The mean can be set as the pooled mean for the data (since under the null tested factors don’t matter). Since we don’t know or set sigma, we can again estimate it from the data. One such sample might look like\n\nvariance_estimate &lt;- sum((function_output$N -1) * (function_output$sd)^2)/(sum(function_output$N)-length(function_output$N))\nmean_sepal &lt;- mean(iris$Sepal.Length)\nsimulated_data &lt;- data.frame(Species=c(rep(\"setosa\", 50), \n                                       rep(\"versicolor\", 50),\n                                       rep(\"virginica\",50)),\n                             Sepal.Length=rnorm(150, mean_sepal, \n                                          sd= sqrt(variance_estimate)))\n\nfunction_output &lt;- summarySE(simulated_data, measurevar=\"Sepal.Length\", groupvars =\n                               c(\"Species\"))\n\nggplot(simulated_data, aes(Species,Sepal.Length)) + \n  geom_jitter(aes(colour=Species), size = 3) +\n  ylab(\"Sepal Length (cm)\")+ggtitle(\"Sepal Length of various iris species\")+\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\")) +\n  geom_errorbar(aes(ymin=Sepal.Length, ymax=Sepal.Length, color=\"group means\"), \n                data = function_output) +\n    geom_hline(aes(yintercept=mean(Sepal.Length),  color = \"overall average\"))+\n  scale_color_manual(values=colors)\n\n\n\n\nAs expected, under the null hypothesis the overall and species-specific averages are closer. Now that we have the data, however, we are stuck with a new question: What is our test statistic?\nIn general, using a difference among means makes sense for cases when we have only 2 populations. This will be used when we introduce t-tests. However, it does not work for 3+ populations, so we will need a different approach.\nANOVAs offer an approach that can be used for any group of 2+ populations when certain assumptions are met. ANOVAs stands for analysis of variance which may seem odd given our hypotheses are focused on means. However, the idea (not fully developed here) is that we can get an overall estimate of variance by\n\ncalculating variance for each point around its respective group-specific mean\n\nthis leads to 3 estimates of variance, which we can multiple by (n-1) to account for differences in sample size and then divide by the number of groups to get an overall estimate of variance\n\nthis is referred to as mean square error\n\n\n\\[\n\\begin{split}\n\\textrm{Remember, }s^2 = \\frac{\\sum_{i=1}^{n} (Y_{i}-\\overline{Y})^2}{n-1} \\sim \\sigma^2\\\\\n\\textrm{So if we have j groups, for each group we can see}\\\\\ns_j^2 = \\frac{\\sum_{i=1}^{n_j} (Y_{ij}-\\overline{Y_j})^2}{n_j-1} \\sim \\sigma_j^2\\\\\n\\textrm{which we can combine to estimate }\\sigma_{overall}\\\\\n\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 ...(n_j-1)s_j^2}{n_1+n_2+...n_j-1} = s_{overall}^2\\\\\n\\end{split}\n\\]\ncalculating variance for each group mean around the overall mean\n\nthis is called mean square treatment\n\n\n\\[\n\\begin{split}\n\\frac{\\sum_{j=1}^{j} (\\overline{Y_j}-Y_{overall}{i})^2}{j-1} = \\frac{s^2}{n} \\\\\n\\textrm{where j is the number of groups. This can be multiplied by n to get }s^2 \\\\\n\\end{split}\n\\]\nIn other words, variance among groups should be equal to variance within groups. You should also note this means we can partition the variance for any given observation as its distance from its group mean and its group means distance from the overall\nUnder the null hypothesis, the ratio of these estimates should tend towards 1. We can see this using a sampling experiment.\n\n#sample\n  ratio &lt;- data.frame(rep = 1:10000, mse = rep(NA,10000), \n                      msg = rep(NA,10000), ratio = rep(NA,10000))\nfor(i in 1:10000){\n    setosa &lt;- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))\n    versicolor &lt;- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))\n    virginica &lt;- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))\n    mean_overall &lt;- mean(c(setosa, versicolor, virginica))\n    ratio$mse[i] &lt;- (49 * var(setosa) + 49 * var(versicolor) + 49 * var(virginica))/(150 - 3)\n    ratio$msg[i] &lt;- (50 * (mean(setosa)-mean_overall)^2 + \n                 50 * (mean(versicolor)-mean_overall)^2 + \n                 50 * (mean (virginica)-mean_overall)^2)/2\n    ratio$ratio[i] &lt;- ratio$msg[i]/ratio$mse[i]\n}\n  \nsummary(lm(Sepal.Length~Species, iris))$fstatistic[1]\n\n   value \n119.2645 \n\nggplot(ratio, aes(ratio)) +\n    geom_histogram(aes(y=..count../sum(..count..)), fill = \"orange\", bins=15)+\n    labs(main = \"Ratio under null hypothesis\", y= \"Probability\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nUsing this approach, we could determine how unusual our data were (get a p-value!). However, finding a distribution that approximates this shape would make future work easier. It turns out all the squared terms above lead to this being a rato of \\(\\chi^2\\) distributions, where the numerator has degrees of freedom j-1 (# of groups - 1) and the numerator had degrees of freedom n-j-1; the “lost” degrees of freedom are used to estimate group and overall means.\n\nggplot(ratio, aes(ratio)) +\n    geom_histogram(aes(y=..count../sum(..count..)), fill = \"orange\", bins = 15) +\n    labs(main = \"Ratio under null hypothesis\", y= \"Probability\")+\n    stat_function(fun = df, args = list(df1 =2, df2 = 147), color = \"green\")   \n\n\n\n\nIn our data, we found a signal of 119.264502184505! This is going to lead to a very low p-value."
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#welcome-to-the-linear-model",
    "href": "content/chapters/Compare_means_among_populations.html#welcome-to-the-linear-model",
    "title": "Comparing means among groups",
    "section": "Welcome to the linear model",
    "text": "Welcome to the linear model\nAn ANOVA is just one case of a linear model. We will fully explore these later, but noting this now is useful in that all linear models have the same sets of assumptions. In general, linear models assume the residuals of the model are are independent, identically distributed, and follow a normal distribution. You’ll sometime see this written as\n\\[\n\\epsilon \\approx i.i.d.\\ N(\\mu,\\sigma)\n\\] But what does this mean?\nResiduals are the distance between a measurement and its model-predicted value. A closely related term, error, is actually the distance between a measurement and the unknown population mean for a group. Linear models assume the residuals are independent of each other (this follows from independent data points) and that their spread (around their predicted values) is normally distributed and similar for all points.\nUnderstanding this explains two key points. The residuals, not the data, need to be normally distributed. Also, we have to build the model to get the residuals, then we check the assumptions.\nWe can do this in R using the lm function. This approach also lets us use a single set of functions to build many model types. As always, there are many ways to do anything in R, so there are specific ANOVA functions that we will not introduce here.\nFor our data, we can build an lm object\n\niris_anova &lt;- lm(Sepal.Length~Species, iris)\n\nthen use plot to check the assumptions.\n\nplot(iris_anova)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese 4 plots focus on the residuals (not the data).\n\nThe Residuals vs fitted plot allows us to see if the residuals are identically distributed - we want to see a flat red line and no structure to the residuals in regards to their spread or location. Note we only have 3 fitted values here (matching our 3 group means), so will see “lines” of data in one-way ANOVAs (another name for what we are doing here).\n\nThe Q-Q Residuals plot allows us to assess normality - points should be on the line\nThe other 2 graphs give show different forms of residuals against the fitted values. We will return to them later.\n\nIf our assumptions are met, we can look at the output. One way to do this is using the summary function.\n\nsummary(iris_anova)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nThis output may be confusing, however. The overall p value shown in the bottom right is for the entire model - that works for now, but soon won’t. We also see individual p values for 2 levels of species, plus an odd intercept term.\nThese are model artifacts and may be confusing. R lets the first factor level (typically alphabetical) be an intercept for the linear model, and then considers the other factor levels as deviations from that. It also shows if all the resulting Estimates are significantly different from 0. Note this means a significant intercept term does not mean your groups actually differ.\nGiven these issues, why use the summary command? It does present some other useful information. For example, the R2 values is a measure of how variation the model explains. It can range from 0 (the model explains nothing) to 1 (all residuals are zero, in this case meaning all members of a given species have the exact same height). The adjusted-R2 value is similar, but it adjusts the measure to account for the fact more complex models will always explain more variation.\nYou can also remove the intercept to get group estimates for all groups\n\nsummary(lm(Sepal.Length~Species-1, iris))\n\n\nCall:\nlm(formula = Sepal.Length ~ Species - 1, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nSpeciessetosa       5.0060     0.0728   68.76   &lt;2e-16 ***\nSpeciesversicolor   5.9360     0.0728   81.54   &lt;2e-16 ***\nSpeciesvirginica    6.5880     0.0728   90.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.9925,    Adjusted R-squared:  0.9924 \nF-statistic:  6522 on 3 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nNote these match our group means, which is good, but the overall p value is now less useful (it compares our data to a null that assumes everything is equal to 0), and the output is still confusing.\nSince we are doing an omnibus test, what we typically want is a single p value associated with our factor of interest (species in this case). To get that, we’ll use the Anova function from the car package.\n\nlibrary(car)\n\nLoading required package: carData\n\nAnova(iris_anova, type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Sepal.Length\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept) 1253.00   1 4728.16 &lt; 2.2e-16 ***\nSpecies       63.21   2  119.26 &lt; 2.2e-16 ***\nResiduals     38.96 147                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWhat does type=“III” mean\n\nResiduals can be calculated in multiple ways. For simple models (those with one variable) most calculations lead to the same answer. When we start adding multiple factors to a model and/or interactions, however, they differ. In short, Type I residuals consider the order in which factors are added to a model, and type 2 do not consider interactions. We will stick with type III for this class.\n&lt;&gt;\nDoing this we see Species has a significant impact on explaining variation in the data (and our very high F value). So we reject the null hypothesis that mean sepal length does not differ among species. Now what?"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#post-hoc-tests",
    "href": "content/chapters/Compare_means_among_populations.html#post-hoc-tests",
    "title": "Comparing means among groups",
    "section": "Post-hoc tests",
    "text": "Post-hoc tests\nJust like for a multi-population \\(\\chi^2\\) tests, we need to do follow-up tests to compare groups while controlling for the FWER. For linear models (and more), we will use the glht function from the multcomp package to conduce these tests.\n\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\ncompare_cont_tukey &lt;- glht(iris_anova, linfct = mcp(Species = \"Tukey\"))\nsummary(compare_cont_tukey)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Sepal.Length ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nversicolor - setosa == 0       0.930      0.103   9.033   &lt;1e-08 ***\nvirginica - setosa == 0        1.582      0.103  15.366   &lt;1e-08 ***\nvirginica - versicolor == 0    0.652      0.103   6.333   &lt;1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nOur first approach uses a new (to us) method. It is not exactly Tukey’s method (very confusing) but is closely related to it. Tukey’s method is focuses on all possible pair-wise comparisons and controls for the FWER using a studentized range approach (not fully developed here, but similar to z-transform but focused on the range of means and using the estimated standard deviation; similar to t-statistic in this aspect and also developed by Student). It is also called Tukey’s Honestly Significant Difference/HSD, Tukey-Kramer method, and many other names. In glht, specifying “Tukey” tells the program to do all possible pairs comparison (like Tukey’s method). The post-hoc control for FWER, however, uses a slightly different approach that can handle interactions (still to be explained) and some other things a little bit easier.\nUsing this approach we see that all species differ significantly from all others; the output also provides estimates of the differences, which match up with our model summary output.\nWe can also use the methods we previously discussed such as Bonferroni and FDR. These requre us to set up the comparisons, which also means we can limit our number of tests if so desired. For example, we could focus only on differences with I. virginica.\n\ncompare_virginica_only &lt;- glht(iris_anova, linfct = mcp(Species = \n                                                                c(\"virginica - versicolor = 0\", \n                                                                  \"virginica - setosa = 0\")))\n\nAfter setting up the comparison, we can specify the method to use to correct for FWER,\n\nsummary(compare_virginica_only, test=adjusted(\"holm\")) \n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: lm(formula = Sepal.Length ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nvirginica - versicolor == 0    0.652      0.103   6.333 2.77e-09 ***\nvirginica - setosa == 0        1.582      0.103  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\nsummary(compare_virginica_only, test=adjusted(\"fdr\")) \n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: lm(formula = Sepal.Length ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nvirginica - versicolor == 0    0.652      0.103   6.333 2.77e-09 ***\nvirginica - setosa == 0        1.582      0.103  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- fdr method)\n\n\nNote for our small number of tests and relatively large differences in means and large sample sizes, differences in p values are minimal.\nThere are instances when FWER are not an issue and thus p values do not need to be adjusted. This occurs when we explore orthogonal contrasts."
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#a-little-deeper-into-linear-models",
    "href": "content/chapters/Compare_means_among_populations.html#a-little-deeper-into-linear-models",
    "title": "Comparing means among groups",
    "section": "A little deeper into linear models",
    "text": "A little deeper into linear models\nLet’s return to linear models to help explain orthogonal contrasts (and some other things). Linear models are a sysem of equations (a matrix), where\n\\[\n\\begin{split}\nY=X\\beta+\\epsilon, \\textrm{ where }\\\\\n\\textrm{Y is our observations (an nx1 matrix)}\\\\\n\\textrm{X is a matrix showing our explanatory variables (an nxk matrix)}\\\\\n\\beta \\textrm{ is our coefficient matrix(an kx1 matrix)}\\\\\n\\epsilon \\textrm{is a matrix (an nx1) of residuals)}\\\\\n\\end{split}\n\\]\nIn our case, \\(\\beta\\) is simply a 3x1 matrix where each entry is a species average (or one is an intercept and other two are distances from it - same thing) and X is a matrix with dummy variables (1 or 0) indicating which group each observation belongs too. X is sometimes called a model or design matrix. We can see this using R. First, we can pull the design matrix from our model object\n\nlibrary(rmarkdown)\npaged_table(data.frame(model.matrix(iris_anova)))\n\n\n\n  \n\n\n\nWe can also pull the model coefficients, which form our \\(\\beta\\) matrix, and place them in the correct orientation.\n\nmatrix(as.numeric(iris_anova$coefficients), ncol=1)\n\n      [,1]\n[1,] 5.006\n[2,] 0.930\n[3,] 1.582\n\n\nSo for our first observation, which is\n\niris[1,]\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n\n\nWe would multiply (remember, rows get multiplied by columns in matrices)\n\n1*5.006 + 0*.930+0*1.58\n\n[1] 5.006\n\n\nThis explains why all our fitted values are one of three values! We can also see our observation minus prediction\n\niris[1,\"Sepal.Length\"]-1*5.006 + 0*.930+0*1.58\n\n[1] 0.094\n\n\nmatches our first model residual\n\niris_anova$residuals[1]\n\n    1 \n0.094 \n\n\nUnderstanding this general setup explains a few things. When models get more complicated you may see errors or warnings related to singularity. This occurs when XTX isn’t invertible(linear algebra!), which it needs to be to find \\(\\beta\\). This occurs if columns in your design matrix are not independent and are actually linear combinations of each other. This happens when you have highly related measurements (we’ll discuss correlation eventually so you can actually measure this!). We will use similar manipulations to eventually find the \\(\\hat{H}\\) matrix when we consider Cook’s Distance (in regression chapter). Degrees of freedom are similarly related to the number of estimated coefficients the model required (the number of rows in the \\(\\beta\\) matrix).\nReturning to our contrasts, note when we do post-hoc tests we are effectively testing for differences in \\(\\beta\\) values. We can put these “tests” in a similar system of equations/matrix. For these tests, the coefficients have to equal 0. For pair comparisons, that means we have 1 for one coeffcient and -1 for the other (for example, (1,-1,0). However, we can also compare one coefficient to the average of the others (2,-1,-1). We could write these two contrasts as\n\\[\n\\begin{bmatrix}\n1 & -1 & 0 \\\\\n2 & 1 & 1 \\\\\n\\end{bmatrix}\n\\]\nA group of contrasts are orthogonal if the sum of the multiplied coefficients from each column equals zero. In this case\n\n1*2 + -1*1 + 0*1\n\n[1] 1\n\n\ndoes not equal 0, so these are not orthogonal contrasts. This is also because I can add add the first and third column and get the second (columns are not independent). However, if we instead carried out these contrasts\n\\[\n\\begin{bmatrix}\n2 & -1 & -1 \\\\\n0 & 1 & -1\\\\\n\\end{bmatrix}\n\\]\nthey would be independent. There are other options as well, as we can always find a number of orthogonal contrasts equal to the number of groups being compared minus one.\nResulting p values would not required correction for FWER. We can specify contrasts like this using glht. Below I do the same matrix, but note I set the maximum entry to 1 so that estimate of mean differences aren’t doubled.\n\ncontr &lt;- rbind(\"setosa - versicolor - virginica\" = c(1, -.5,-.5),\n               \"versicolor - virginica\" = c(0,1,-1))\nsummary(glht(iris_anova, linfct = mcp(Species = contr)), test=adjusted(\"none\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: lm(formula = Sepal.Length ~ Species, data = iris)\n\nLinear Hypotheses:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \nsetosa - versicolor - virginica == 0 -1.25600    0.08916 -14.086  &lt; 2e-16 ***\nversicolor - virginica == 0          -0.65200    0.10296  -6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#displaying-output-of-post-hoc-tests",
    "href": "content/chapters/Compare_means_among_populations.html#displaying-output-of-post-hoc-tests",
    "title": "Comparing means among groups",
    "section": "Displaying output of post-hoc tests",
    "text": "Displaying output of post-hoc tests\nOutput from post-hoc tests is often displayed using compact letter display. Groups that are not significantly differently share the same letter (so in this case they all have different letters).\n\ncld_output &lt;- fortify(cld(compare_cont_tukey))\ncld_output$Species &lt;- cld_output$lhs\n\nfunction_output &lt;- summarySE(iris, measurevar=\"Sepal.Length\", groupvars =\n                               c(\"Species\"))\n\nfunction_output &lt;- merge(function_output, cld_output)\n\nggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +\n  geom_col(aes(fill=Species)) +\n    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. species\"))),\n       y= \"Sepal length (cm)\",\n       x= \"Species\")+\n  geom_text(aes(label=letters,y=Sepal.Length+3*ci))\n\n\n\n\nOther options include plotting the differences in means\n\nplot(compare_cont_tukey)\n\n\n\n\n\nemmeans: another option\nAnother popular package for conducting posthoc comparison in R is emmeans. The package has a great starters guide here"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#t-test-connections",
    "href": "content/chapters/Compare_means_among_populations.html#t-test-connections",
    "title": "Comparing means among groups",
    "section": "T-test connections",
    "text": "T-test connections\nSo far we have focused on comparing means among multiple groups. This can include include comparing means between only 2 groups (which we already do for the post-hoc tests). In doing this we also introduced new post-hoc tests and the ideas of a linear model.\nThe linear model framework will unify most of the remaining tests we learn in class. In fact, several tests we’ve already learned can be formulated this way. This is extremely useful given we want statistics to be a related set of tests in a comprehensive framework.\nThere are many ways to teach statistics, however, and a long history of tests. Many textbooks and approaches build up from one sample tests by moving to two-sample t-tests. These tests bridge the logic noted above and the approach we used for single-sample t-test. This is because the t-distribution is a special case of the F distribution. It occurs when the square root of an F distribution with 1 degree of freedom in the numerator is considered. Thus the degrees of freedom associated with a t-test will be equal to the degrees of freedom associated with the denominator of the associated F-test.\n2-sample t-tests may also an easier approach to first considering differences among groups since with only 2 populations the difference in means may be considered. However, it can be shown (not here) this is simply a rearrangement of our exploration of variances.\nTo demonstrate this, let’s only consider two species. Note\n\ntwo_species_subset &lt;- iris[iris$Species!=\"setosa\",]\nt.test(Sepal.Length ~ Species, two_species_subset, var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 98, p-value = 1.725e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8818516 -0.4221484\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nyields the same p-value as\n\nAnova(lm(Sepal.Length ~ Species, two_species_subset), type=\"III\")\n\nAnova Table (Type III tests)\n\nResponse: Sepal.Length\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 1761.80  1 5253.038 &lt; 2.2e-16 ***\nSpecies       10.63  1   31.688 1.725e-07 ***\nResiduals     32.87 98                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlso note the t statistic is the square root of the F statistic.\n\n-5.6292^2\n\n[1] -31.68789\n\n\nNote the t.test function can also use columns holding data from each population as arguments as opposed to the formula interface, but we will not use that approach here.\nThe var.equal=T argument, however, is not the default in R, and this assumption is one of the major differences in 2-sample t-tests and F tests. Remember, ANOVAs and t-tests both require estimates for sigma. If we do not assume the variances are equal for each group, then the best way to estimate the variance is to calculate the variance for each group and take a weighted (by sample size mean). This approach is known as the Behren-Fisher or Welsh t-test.\n\nt.test(Sepal.Length ~ Species, two_species_subset)\n\n\n    Welch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nThe resulting statistics has a distrubtion that can be approximated by a t-distribution, but the associated degrees of freedom can can be non-integer (decimal) and less than (n1+ n2 - 2).\nThis means the basic assumptions for 2-sample t-tests are independent data points, groups show the same variance, and means are normally distributed. Much like the one-sample t-test, the central limit theorem implies assumptions about the mean distribution are commonly met. However, if they are not met, we have a few common options."
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#non-parametric-connections",
    "href": "content/chapters/Compare_means_among_populations.html#non-parametric-connections",
    "title": "Comparing means among groups",
    "section": "Non-parametric connections",
    "text": "Non-parametric connections\nOptions for when assumptions of the t- and F-tests (ANOVAs) are presented below. Note given the history of t-tests being considered apart from ANOVAs, some functions only work with less than 2 populations while others work with three or more. However, the overall approaches are similar.\n\nRanks: Wilcoxon/Mann-Whitney U and Kruskal-Wallis test\nWe can extend the Wilcoxon test to 2-samples. To do so, we rank the data points from smallest to largest. The ranks are then used to calculate a U statistic. The statistic sums the ranks for each group (r), then uses them to calculate\n\\[\nU_1 = n_1n_2+\\frac{n_1(n_1+1)}{2}-r_1\n\\] The U statistics is calculated for each group. The larger U value is then taken and used to compute a p value. We can calculate this using the wilcox.test function,\n\nwilcox.test(Sepal.Length ~ Species, two_species_subset)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Sepal.Length by Species\nW = 526, p-value = 5.869e-07\nalternative hypothesis: true location shift is not equal to 0\n\n\nThis test assumes the two distributions being considered have similar shape (not that the resulting means are normally-distributed). If you remove the default continuity correction (applied as we approximate discrete data with a continuous distribution)\n\nwilcox.test(Sepal.Length ~ Species, two_species_subset, correct=F)\n\n\n    Wilcoxon rank sum test\n\ndata:  Sepal.Length by Species\nW = 526, p-value = 5.765e-07\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe get the same result as the Kruskal-Wallis test\n\nkruskal.test(Sepal.Length ~ Species, two_species_subset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Sepal.Length by Species\nKruskal-Wallis chi-squared = 24.989, df = 1, p-value = 5.765e-07\n\n\nwhich is a rank-based test that can be applied to 3+ populations.\n\nkruskal.test(Sepal.Length ~ Species, data = iris)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Sepal.Length by Species\nKruskal-Wallis chi-squared = 96.937, df = 2, p-value &lt; 2.2e-16\n\n\nIf we have more than three populations and this omnibus test reveals a significant p-value, we can follow it up with appropriate post-hoc tests.\n\npairwise.wilcox.test(iris$Sepal.Length, \n                          iris$Species, \n                          p.adjust.method=\"holm\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  iris$Sepal.Length and iris$Species \n\n           setosa  versicolor\nversicolor 1.7e-13 -         \nvirginica  &lt; 2e-16 5.9e-07   \n\nP value adjustment method: holm \n\n\n\n\nSign/Binary approach\nFor a single sample, we also considered the sign/binary test. We will return to this test in the next chapter, as it does not work for data from independent samples.\n\n\nBootstrapping\nAnother option is to extend the bootstrapping option. Although we could again develop a simulation using the boot function again, here we again use the MKinfer package.\n\nlibrary(MKinfer)\n\nWarning: package 'MKinfer' was built under R version 4.2.3\n\nboot.t.test(Sepal.Length ~ Species, two_species_subset)\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  Sepal.Length by Species\nbootstrap p-value &lt; 2.2e-16 \nbootstrap difference of means (SE) = -0.6487257 (0.1144229) \n95 percent bootstrap percentile confidence interval:\n -0.8760 -0.4239\n\nResults without bootstrap:\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nnote we can also run this test without assuming variances are differenct.\n\nboot.t.test(Sepal.Length ~ Species, two_species_subset, var.equal=T)\n\n\n    Bootstrap Two Sample t-test\n\ndata:  Sepal.Length by Species\nbootstrap p-value &lt; 2.2e-16 \nbootstrap difference of means (SE) = -0.652117 (0.1316271) \n95 percent bootstrap percentile confidence interval:\n -0.910 -0.396\n\nResults without bootstrap:\nt = -5.6292, df = 98, p-value = 1.725e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8818516 -0.4221484\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n\n\nBoth these approaches also show the corresponding t-test results, but note you should choose which test you plan to use before seeing the results!\nFor more than 2 groups, the t1waybt function in the WRS2 package can allow comparison.\n\nlibrary(WRS2)\nt1waybt(Sepal.Length~Species, iris)\n\nCall:\nt1waybt(formula = Sepal.Length ~ Species, data = iris)\n\nEffective number of bootstrap samples was 599.\n\nTest statistic: 111.9502 \np-value: 0 \nVariance explained: 0.716 \nEffect size: 0.846 \n\n\nIf needed, the mcppb20 package allows for appropriate post-hoc comparisons.\n\nbootstrap_post_hoc &lt;- mcppb20(Sepal.Length~Species, iris)\nbootstrap_post_hoc_df &lt;-data.frame(bootstrap_post_hoc$comp)\nbootstrap_post_hoc_df$adjusted_p &lt;- p.adjust(as.numeric(bootstrap_post_hoc$comp[,6]), \"holm\")\nbootstrap_post_hoc_df$Group &lt;- factor(bootstrap_post_hoc_df$Group)\nlibrary(plyr)\nbootstrap_post_hoc_df$Group &lt;- revalue(bootstrap_post_hoc_df$Group,\n                                       setNames(                                      bootstrap_post_hoc$fnames,as.character(1:length(bootstrap_post_hoc$fnames))))\n\nThe following `from` values were not present in `x`: 3\n\nbootstrap_post_hoc_df$Group.1 &lt;- factor(bootstrap_post_hoc_df$Group.1)\n\nbootstrap_post_hoc_df$Group.1 &lt;- revalue(bootstrap_post_hoc_df$Group.1,\n                                       setNames(                                      bootstrap_post_hoc$fnames,as.character(1:length(bootstrap_post_hoc$fnames))))\n\nThe following `from` values were not present in `x`: 1\n\nbootstrap_post_hoc_df\n\n       Group    Group.1     psihat  ci.lower   ci.upper p.value adjusted_p\n1     setosa versicolor -0.9100000 -1.143333 -0.7266667       0          0\n2     setosa  virginica -1.5466667 -1.836667 -1.3066667       0          0\n3 versicolor  virginica -0.6366667 -0.930000 -0.3666667       0          0\n\n\n\n\nPermutation\nA new option when comparing groups (2 or more) is known as the permutation test. We encountered a similar approach when we learned about the Fisher’s test for binomial data. Using this approach, we can move the measurements between measured populations, calculate test statistics, and consider how unusual our observed statistic was (a p value!). We can do this for 2\n\nlibrary(coin)\nindependence_test(Sepal.Length ~ Species, data =  two_species_subset)\n\n\n    Asymptotic General Independence Test\n\ndata:  Sepal.Length by Species (versicolor, virginica)\nZ = -4.9183, p-value = 8.731e-07\nalternative hypothesis: two.sided\n\n\nor 3+ populations\n\nindependence_test(Sepal.Length ~ Species, data =  iris)\n\n\n    Asymptotic General Independence Test\n\ndata:  Sepal.Length by Species (setosa, versicolor, virginica)\nmaxT = 8.7572, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nPost-hoc test options are available in the rcompanion package.\n\nlibrary(rcompanion)\n\nWarning: package 'rcompanion' was built under R version 4.2.3\n\n\n\nAttaching package: 'rcompanion'\n\n\nThe following object is masked from 'package:MKinfer':\n\n    quantileCI\n\npairwisePermutationTest(Sepal.Length ~ Species,\n                             data = iris,\n                             method=\"holm\")\n\n                  Comparison   Stat   p.value  p.adjust\n1    setosa - versicolor = 0 -7.246  4.28e-13 8.560e-13\n2     setosa - virginica = 0 -8.368 5.883e-17 1.765e-16\n3 versicolor - virginica = 0 -4.918 8.731e-07 8.731e-07"
  },
  {
    "objectID": "content/chapters/Compare_means_among_populations.html#next-steps",
    "href": "content/chapters/Compare_means_among_populations.html#next-steps",
    "title": "Comparing means among groups",
    "section": "Next steps",
    "text": "Next steps\nOur following chapters will extend ANOVAs to consider the impact of multiple measured categories. In doing so, we will also explain paired t-tests and sign tests for paired data."
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html",
    "href": "content/Compare_proportions_among_populations.html",
    "title": "Comparing proportions among groups",
    "section": "",
    "text": "Now that we’ve covered hypothesis testing for both discrete and continous data, we’ll extend these ideas to compare differences among groups. In addition considering these differences, the same test we’ll let us consider if proportions follow a given ratio."
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#example-back-to-the-birds",
    "href": "content/Compare_proportions_among_populations.html#example-back-to-the-birds",
    "title": "Comparing proportions among groups",
    "section": "Example: Back to the birds",
    "text": "Example: Back to the birds\nLet’s return to our bird example Klem (1989). We previously found that purple finches did not strike windows at proportions that might be predicted by population demographics using a binomial test. However, what if instead we wanted to compare the collision rate of old vs young birds among several species?\n\n\n\nCephas, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\n\n\nLet’s start simple and just compare purple finches and dark-eyed juncos.\n[Becky Matsubara from El Sobrante, California, CC BY 2.0 &lt;https://creativecommons.org/licenses/by/2.0&gt;, via Wikimedia Commons&gt;, via Wikimedia Commons] (/images/Dark-eyed_Junco_(Oregon)_(39651044095).jpg){fig-alt=“Dark-eyed Junco (Junco hyemalis), Sobrante Ridge Regional Reserve, Richmond, California.”}\nKlem’s sample of finches totaled 18, with 9 being older (after hatching year). For juncos, 4 of 11 sampled birds were older. We could put this data in a table.\n\n\n\n\nObserved\n\n\n\n\n\n\n\nFinch\nJunco\nRow Totals\n\n\n\n9\n4\n13\n\n\n\n9\n7\n16\n\n\nColumn Totals\n9\n9\nN = 29\n\n\n\nFirst, we could plot our data\n\nbirds_original &lt;- data.frame(Age = c(\"Old\",\"Young\",\"Old\", \"Young\"),\n                      Species = c(\"Junco\", \"Junco\", \"Finch\", \"Finch\"),\n                      Number = c(4, 7, 9, 9))\nlibrary(ggplot2)\nggplot(birds_original, aes(x= Species, y = Number)) +\n  geom_col(aes(fill = Age)) + \n  labs(x=\"Species\", \n       y=\"Frequency\", \n       main =\"Age at collision for juncos and finches\")\n\n\n\n\nGiven the different sampling sizes, a mosaic plot might help in visually comparing ratios.\n\nggplot(birds_original, aes(x= Species, y = Number)) +\n  geom_col(aes(fill = Age), position = \"fill\")+\n    labs(x=\"Species\", \n       y=\"Proportion\", \n       main =\"Age at collision for juncos and finches\")\n\n\n\n\nBefore we test this, we need to decide on an hypothesis. Although both species were predicted to occur at 3:1 ratios in the wild, that’s not what we are considering here. Instead, we want to know if the likelihood of old vs young birds being in our samples differed for the species. Put another way, we are asking if the proportion of young vs old is contingent on species. These tests are often called contingency analysis, and the table we started with may be referred to as a to as a contingency table.\nIf the proportion of old vs young birds differ among species, it could be because the age structure of the focal populations are different or because the birds differ in their relationship to glass at different ages. However, we are still testing a distribution-based parameter.\n\\[\n\\begin{split}\nH_O: p_{finches} = p_{juncos} \\\\  \nH_A: p_{finches} \\neq p_{juncos} \\\\\n\\textrm{ where p is likelihood of sampled bird being older}\\\\\n\\end{split}\n\\]\nPut another way,we want to test if p is independent of species.\n\\[\n\\begin{split}\nH_O: \\textrm{Probability of older bird hitting window is independent of species} \\\\  \nH_A: \\textrm{Probability of older bird hitting window is dependent of species} \\\\  \n\\end{split}\n\\]\nThis formulation is important, because it helps form our predictions under the null hypothesis. What would we expect if p did not differ among species? If age and species were independent, we could expect\n\\[\n\\textrm{Pr[ Old AND Given speciesuse] = Pr[given species] * Pr[Old]}\n\\]\nSince we have 13/29 birds are old, we should expect\n\n\n\n\nObserved\n\n\n\n\n\n\n\nFinch\nJunco\nRow Totals\n\n\n\n18 * 13/29\n11 * 13/29\n13\n\n\n\n18 * 16/29\n11 * 16/29\n16\n\n\nColumn Totals\n9\n9\nN = 29\n\n\n\nIn order to carry out a sampling experiment to consider noise from this expected outcome, we have to determine the p parameter to use for our population. This is because under the null hypothesis, there is only one population- any observed difference is just due to chance!\nHowever, we have an issue - we don’t know p. In our binomial experiment it was set by our null hypothesis. Now we are comparing p among species, but that doesn’t set a population distribution.\nTo fix this, we go back to our normal approximations. We have shown for large sample sizes the binomial distribution follows the central limit theorem, with \\(Np\\) and \\(p\\) both showing a normal distribution.\n\nsample_size=c(\"1\",\"5\",\"10\", \"20\", \"40\", \"80\")\nnumber_of_simulations &lt;- 1000\nsampling_experiment &lt;- setNames(data.frame(matrix(ncol = length(sample_size), nrow = number_of_simulations)), sample_size)\n\nfor(k in 1:length(sample_size)){\nfor(i in 1:number_of_simulations){\nsampling_experiment[i,k] = rbinom(n=1, size=as.numeric(sample_size[k]), prob=.7)\n}\n}\nlibrary(reshape2)\nsampling_experiment_long &lt;- melt(sampling_experiment, variable.name = \"Sample_size\", value.name = \"mean\")\n\nNo id variables; using all as measure variables\n\nsampling_experiment_long$Sample_size &lt;- factor(sampling_experiment_long$Sample_size, levels =c(\"1\",\"5\",\"10\", \"20\", \"40\", \"80\"))\nlevels(sampling_experiment_long$Sample_size) &lt;- paste(\"Sample size of \", levels(sampling_experiment_long$Sample_size))\n\nggplot(sampling_experiment_long,aes(x=mean)) +\n  geom_histogram(color=\"black\") +\n  labs(title=paste(\"Observed number of successes from \", number_of_simulations, \" random draws\"),\n       subtitle = \"Binomial distribution, p=.7\",  \n       x= \"Mean\",\n       y= \"Frequency\")+\n    facet_wrap(~Sample_size, nrow = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSo, we can replace the binomial distribution in our sampling experiment with a normal population. To use this approach, we estimate a value for p, \\(\\hat{p}\\), from the data, and let\n\\[\n\\begin{split}\n\\mu = Np \\\\\n\\sigma_\\mu =\\sqrt{Np(1-p)}\n\\end{split}\n\\]\nWe then draw only 1 draws this distribution. Why only 1? Because we need to keep the sample sizes the same, so the row and column totals are set! Remember this for a moment. After we draw 1 number, we fill in the rest.\n\nnum_of_simulations &lt;- 10000\nsimulations &lt;- r2dtable(num_of_simulations,c(13,16),c(18,11))\nchi_sq_stats &lt;- data.frame(chisq = rep(NA, num_of_simulations))\nfor(i in 1:num_of_simulations){\nchi_sq_stats$chisq[i] &lt;- chisq.test(matrix(unlist(simulations[i]),nrow=2, byrow=T))$statistic\n}\n\ncolors &lt;- c(\"distribution\" = \"green\", \"simulated data\" = \"orange\")\nggplot(chi_sq_stats, aes(x=chisq)) +\n  geom_histogram(aes(y=..count../sum(..count..), color = \"simulated data\", fill=\"simulated data\")) +\n  labs(y=paste(\"Probability under \", num_of_simulations, \" simulations\", \nsep =\"\"), x=expression(chi^2), color=\"Source\")+guides(fill=F)+\n  scale_color_manual(values = colors)+\n  scale_fill_manual(values=colors)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOnce we carry out the sampling experiment, we can Z-transform our cell data(because they are normal now!). The results for a single cell would follow a N(0,1) distribution (the Z). If we wanted, we could square these outcomes (which would then follow a \\(\\chi^2\\) distribution, by definition), and, since we have 4 cells, add them. The resulting variate would follow a \\(\\chi^2\\) distribution with 1 degree of freedom (since we drew 1 numbers for the free “cell” in our table). Finally, because of all the p’s above, we could actually rewrite all of this as\n\\[\nV=\\sum_{i=1}^{n}{\\frac{{(Observed-Expected)}^2}{Expected}} \\textrm{where n is number of cells}\n\\]"
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#contingency-analysis-using-the-chi2-test",
    "href": "content/Compare_proportions_among_populations.html#contingency-analysis-using-the-chi2-test",
    "title": "Comparing proportions among groups",
    "section": "Contingency analysis using the \\(\\chi^2\\) test",
    "text": "Contingency analysis using the \\(\\chi^2\\) test\nThe resulting test is called a \\(\\chi^2\\) test. Note this takes count-based data and uses a continuous distribution to describe it, so it’s an approximate test.\n\nggplot(chi_sq_stats, aes(x=chisq)) +\n  geom_histogram(aes(y=..count../sum(..count..), color = \"simulated data\", fill=\"simulated data\")) +\n  stat_function(fun = dchisq, args = list(df = 1),aes(color =\"distribution\")) +\n  labs(y=paste(\"Probability under \", num_of_simulations, \" simulations\", \nsep =\"\"), x=expression(chi^2), color=\"Source\")+guides(fill=F)+\n  scale_color_manual(values = colors)+\n  scale_fill_manual(values=colors)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can carry out this test in R using the chisq.test function.\nThis function requires a matrix of aggregated values (counts for each cell). Currently we have a data frame (birds). To make this work, we have a few options.\nWe can input the data directly as a matrix, specifying the string, the number of rows and columns, and how we entered the data in regards to rows (remember ?chisq.test)\n\nchisq.test(matrix(c(9,4,9,7), 2, 2, byrow=T))\n\nWarning in chisq.test(matrix(c(9, 4, 9, 7), 2, 2, byrow = T)): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matrix(c(9, 4, 9, 7), 2, 2, byrow = T)\nX-squared = 0.11002, df = 1, p-value = 0.7401\n\n\nIf the data is in wide data frame (meaning more than one measured outcome per row, so measuring the young and old as columns in the data frame) or we make it look like that, we can use the data directly from the data frame. Consider the difference in format. This is long data (one measure per row):\n\nbirds_original\n\n    Age Species Number\n1   Old   Junco      4\n2 Young   Junco      7\n3   Old   Finch      9\n4 Young   Finch      9\n\n\nand this wide\n\nlibrary(reshape2) \nbirds_wide &lt;- dcast(birds_original, Age~Species) \n\nUsing Number as value column: use value.var to override.\n\nbirds_wide\n\n    Age Finch Junco\n1   Old     9     4\n2 Young     9     7\n\n\nWe can make the matrix with wide data\n\nchisq.test(matrix(c(birds_wide$Junco, birds_wide$Finch), nrow = 2))\n\nWarning in chisq.test(matrix(c(birds_wide$Junco, birds_wide$Finch), nrow = 2)):\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matrix(c(birds_wide$Junco, birds_wide$Finch), nrow = 2)\nX-squared = 0.11002, df = 1, p-value = 0.7401\n\n\nFor our 2x2 table, notice the order of input does not matter (because it wouldn’t impact the expected values. Consider\n\nchisq.test(matrix(c(birds_wide$Finch, birds_wide$Junco), nrow = 2))\n\nWarning in chisq.test(matrix(c(birds_wide$Finch, birds_wide$Junco), nrow = 2)):\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matrix(c(birds_wide$Finch, birds_wide$Junco), nrow = 2)\nX-squared = 0.11002, df = 1, p-value = 0.7401\n\n\nUsing cbind is also an option.\n\nchisq.test(cbind(birds_wide$Finch, birds_wide$Junco))\n\nWarning in chisq.test(cbind(birds_wide$Finch, birds_wide$Junco)): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  cbind(birds_wide$Finch, birds_wide$Junco)\nX-squared = 0.11002, df = 1, p-value = 0.7401\n\n\nAs long as we specify the table, we are ok. Note each of these tests notes 1 df. The degrees of freedom associated with this test are based on the number of free cells (or, alternatively, the number of cells minus the parameters you had to fill in!). This typically can be calculated as (# of columns -1)*(# of rows -1). They each also note a p-value greater than .05. This would suggest we should fail to reject the null hypothesis.\nEach result also tells you this is an approximation (as we already noted!). Since the test is approximate, by default R applies Yate’s continuity correction to data focused on 2x2 tables. Some argue this correction is too strict, and you can turn it off in R (correct=F argument). You can also choose to simulate the outcome instead (simulate.p.value = T), but note this will still be an approximate answer since we can’t do every single sample.\nThe output also indicate the results may be incorrect? Why? In order for our normal approximation to work, we need large samples and a \\(\\hat{p}\\) that is not near 0 or 1. Together, these mean our expected values for most cells (actually, &gt;80%) can not be less than 5., and no cell can have an expected value of less than 1. If these assumption are not met(or are close), R will warn us. We can check expected values using\n\nchisq.test(matrix(c(9,4,9,7), 2, 2, byrow=T))$expected\n\nWarning in chisq.test(matrix(c(9, 4, 9, 7), 2, 2, byrow = T)): Chi-squared\napproximation may be incorrect\n\n\n         [,1]     [,2]\n[1,] 8.068966 4.931034\n[2,] 9.931034 6.068966\n\n\nIn this case, 25% of the cells (1/4) has an expected value of less than 5."
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#other-options",
    "href": "content/Compare_proportions_among_populations.html#other-options",
    "title": "Comparing proportions among groups",
    "section": "Other options",
    "text": "Other options\n\nFisher’s test\nIf this is the case for a 2x2 table, we can use a Fisher’s test instead\n\nfisher.test(matrix(c(9,4,9,7), 2, 2, byrow=T))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  matrix(c(9, 4, 9, 7), 2, 2, byrow = T)\np-value = 0.7021\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.2997173 11.0799590\nsample estimates:\nodds ratio \n  1.716435 \n\n\nAs opposed to resampling from a null distribution, Fisher’s test considers all (or lots) of ways the data could be re-arranged (or permuted) and then computes a p-value using that approach. This means Fisher’s test works for any sample size. It is an exact test if all possible combinations are considered (but they rarely are).\nNotice the output reports the odds ratio. This ratio is found by dividing odds in one group by odds in another (thus a ratio). Odds are the probability of one outcome over another. For our data, this could be considered (old/young(finches)) divided by (old/young(juncos)), (9/9)/(4/7)=63/36. This is close to what we saw in the output; slight differences occur since the fisher.test function returns a conditional estimate of the odds ratio.\nNote odds differ from relative risks, which compare the probability of an event occurring (or not) among 2 groups. The results are similar for rare events (think about why!) but not for common events. For more, see Altman, Deeks, and Sackett (1998) Davies, Crombie, and Tavakoli (1998) or this [video and paper]{https://www.bmj.com/content/348/bmj.f7450}(target=“_blank”) Grant (2014).\n\n\nG test\nAnother option, the G test, uses a slightly different approach as well. Instead of resampling, the test uses likelihood to compare outcomes. Likelihood asks how likely we were to observed a given set of data given parameter values.\nFor an easy example of likelihood, let’s go back to a one-sample example and focus on just our new finch data. We have 9 old and 9 young birds, so we have a signal of .5 for p. We can use likelihood to calculate how likely our data was under multiple values of p (ranging from 0 - 1, the only options here) and compare the likelihood of those outcomes White (n.d.).\n\n\n\n\n\nSimilar to calculating sum square errors from models, what is most likely is what we saw, but we know there is always noise in the data. Thankfully, it turns out the ratio of likelihood values follow a \\(\\chi^2\\) distribution and can thus provide a p-value to compare possible models. We will return to likelihood-based approaches later, in fact, as they can be used for any dataset we can generate a model for and can be used to compare multiple models.\nFor our current contingency analysis, we can develop a model where a species parameter impacts the likelihood and one where it does not (not fully shown here, but shown by Patrone (2022)).\nThe GTest function from the DescTools package to employ this test.\n\nlibrary(DescTools)\nGTest(x = matrix(c(9,7,9,4), 2, 2, byrow = T))\n\n\n    Log likelihood ratio (G-test) test of independence without correction\n\ndata:  matrix(c(9, 7, 9, 4), 2, 2, byrow = T)\nG = 0.51774, X-squared df = 1, p-value = 0.4718"
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#what-about-more-than-2-groups",
    "href": "content/Compare_proportions_among_populations.html#what-about-more-than-2-groups",
    "title": "Comparing proportions among groups",
    "section": "What about more than 2 groups?",
    "text": "What about more than 2 groups?\nThese ideas can be extended to compare more than 2 groups with a few important caveats.\n\nThe Fisher test is even less exact since all permutations of the data can seldom be explored\nMore importantly, if we reject the null hypothesis we need to do follow-up tests.\n\nLet’s explore this idea with the Klem data. In addition to considering impacts of age, Klem also recorded the sex (coded as male/female) for each bird. He had data on 4 species.\n\n\n\n\nObserved\n\n\n\n\n\n\n\n\nFinch\nJunco\nRobin\nCardinal\n\n\n\n6\n5\n7\n7\n\n\n\n12\n7\n11\n3\n\n\n\nWe can use the same approaches to test this.\n\nchisq.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))\n\nWarning in chisq.test(matrix(c(6, 5, 7, 7, 12, 7, 11, 3), nrow = 2, byrow =\nT)): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  matrix(c(6, 5, 7, 7, 12, 7, 11, 3), nrow = 2, byrow = T)\nX-squared = 3.7909, df = 3, p-value = 0.2849\n\n\nNote our sample size is still a potential issue, but only 1/8 cells has a predicted value less than 1.\n\nchisq.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))$expected\n\nWarning in chisq.test(matrix(c(6, 5, 7, 7, 12, 7, 11, 3), nrow = 2, byrow =\nT)): Chi-squared approximation may be incorrect\n\n\n          [,1]     [,2]      [,3]     [,4]\n[1,]  7.758621 5.172414  7.758621 4.310345\n[2,] 10.241379 6.827586 10.241379 5.689655\n\n\nwhich means the approximation is fine. We could use the other tests if we preferred.\n\nfisher.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  matrix(c(6, 5, 7, 7, 12, 7, 11, 3), nrow = 2, byrow = T)\np-value = 0.2941\nalternative hypothesis: two.sided\n\nGTest(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))\n\n\n    Log likelihood ratio (G-test) test of independence without correction\n\ndata:  matrix(c(6, 5, 7, 7, 12, 7, 11, 3), nrow = 2, byrow = T)\nG = 3.8087, X-squared df = 3, p-value = 0.2829\n\n\nRegardless, we see all p&gt;.05. What does this mean?\n\nPost-hoc comparisons: Controlling for the FWER\nWhen we compared one group to a set value or two groups to each other, this was easy: it meant our focal parameter was the same between the groups or between expected and observed values. For more than 2 groups, it means the parameters is also means the parameter of interest does not differ among the groups. In other words, our null hypothesis is\n\\[\nH_O: p_{finch} = p_{junco} = p_{cardinal} = p_{robin}\n\\]\nwhere p is the proportion of the population that are males. Here, this means all species have similar male/female ratios (at least given our samples). This is an example of a very useful insignificant result. This study would have been interesting regardless of outcome.\nHowever, let’s imagine we had data on another species (catbirds in the table below).\n\n\n\n\nObserved\n\n\n\n\n\n\n\n\n\nFinch\nJunco\nRobin\nCardinal\nCatbird\n\n\nMale\n6\n5\n7\n7\n30\n\n\nFemale\n12\n7\n11\n3\n7\n\n\n\nWhen we run the test (notice I immediately checked expected values, or assumptions, which is a good habit to get into)\n\nchisq.test(matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T))\n\nWarning in chisq.test(matrix(c(6, 5, 7, 7, 30, 12, 7, 11, 3, 7), nrow = 2, :\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  matrix(c(6, 5, 7, 7, 30, 12, 7, 11, 3, 7), nrow = 2, byrow = T)\nX-squared = 17.179, df = 4, p-value = 0.001784\n\nchisq.test(matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T))$expected\n\nWarning in chisq.test(matrix(c(6, 5, 7, 7, 30, 12, 7, 11, 3, 7), nrow = 2, :\nChi-squared approximation may be incorrect\n\n\n          [,1]     [,2]      [,3]     [,4]     [,5]\n[1,] 10.421053 6.947368 10.421053 5.789474 21.42105\n[2,]  7.578947 5.052632  7.578947 4.210526 15.57895\n\n\nWe now have a significant p-value (.002). What does this mean now?\nA significant p-value from a multi-population test means the parameter is not the same for all groups. However, it does not necessarily mean the parameter is different for every group. Remember, our null hypothesis is (now)\n\\[\nH_O: p_{finch} = p_{junco} = p_{cardinal} = p_{robin} = p_{catbird}\n\\]\nWe would reject this if we have evidence any of these qualities are not true. For example, focusing on catbird comparisons for now, we may find\n\\[\n\\begin{split}\np_{catbird} \\neq p_{junco} \\\\\np_{catbird} \\neq p_{cardinal} \\\\\np_{catbird} \\neq p_{robin} \\\\\np_{catbird} \\neq p_{finch} \\\\\n\\end{split}\n\\]\nor we may find\n\\[\n\\begin{split}\np_{catbird} \\neq p_{junco} \\\\\np_{catbird} \\neq p_{cardinal} \\\\\np_{catbird} = p_{robin} \\\\\np_{catbird} = p_{finch} \\\\\n\\end{split}\n\\]\nEither of these outcomes would reject the null hypothesis that proportions were the same for all species, but they mean different things.\nIn general, after we show using an overall, or omnibus, test that there is a difference among populations, we need to determine which ones actually differ from the others. We do this using post-hoc comparisons to compare specific groups.\nWe can technically choose which comparisons to focus on. For example, you can do compare all possible pairs or just certain combinations. Why would this matter?\nThe answer has to do with family-wise error rate (FWER). Remember, for every test we run we have an \\(\\alpha\\)% chance of a type 1 error. If we run many tests, the likelihood of making a type 1 error increases (the rate of increase depends on how independent the tests are, but we need to control for it.\n\n\n\nXKCD 882: Significant\n\n\nFor this reason, we modify our “used” \\(\\alpha\\) for our post-hoc tests. There are many approaches to doing this, but they all depend on how many tests we run - so the more post-hoc comparisons we include, the harder it may be to find a significant difference among focal pairs.\nTo illustrate this, we will first use a very simple method that is no longer recommended but is useful as a starting point. One options is to control the FWER by dividing \\(\\alpha\\) by the number of post-hoc tests we intend to run. For the above example, if we do all pairs comparisons we would be running 10 comparison (4+3+2+1…). So instead of using .05 as a cutoff, we would use .005.\nFirst, it helps to make a table with row and column names (which are slightly different than headers and very different than a column of names in R).\n\nbird_ratio_table &lt;- matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T)\ncolnames(bird_ratio_table) &lt;- c(\"Finch\", \"Junco\", \"Cardinal\", \"Robin\", \"Catbird\")\nrownames(bird_ratio_table) &lt;- c(\"Male\", \"Female\")\n\nThen we can run the test with the pairwiseNominalIndependence function from the rcompanion package. Note the function needs a table or matrix and to know which method to use to compare rows or columns. Looking at the table\n\nbird_ratio_table\n\n       Finch Junco Cardinal Robin Catbird\nMale       6     5        7     7      30\nFemale    12     7       11     3       7\n\n\nLet’ s us see we want to compare columns.\n\nlibrary(rcompanion)\n\nWarning: package 'rcompanion' was built under R version 4.2.3\n\nbonf_correct &lt;- pairwiseNominalIndependence(bird_ratio_table, compare=\"col\", method = \"bonf\")\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nbonf_correct\n\n           Comparison p.Fisher p.adj.Fisher  p.Gtest p.adj.Gtest p.Chisq\n1       Finch : Junco  0.71200       1.0000 0.643000     1.00000 0.93800\n2    Finch : Cardinal  1.00000       1.0000 0.729000     1.00000 1.00000\n3       Finch : Robin  0.11400       1.0000 0.059900     0.59900 0.14200\n4     Finch : Catbird  0.00082       0.0082 0.000505     0.00505 0.00141\n5    Junco : Cardinal  1.00000       1.0000 0.879000     1.00000 1.00000\n6       Junco : Robin  0.23100       1.0000 0.180000     1.00000 0.36900\n7     Junco : Catbird  0.02300       0.2300 0.011200     0.11200 0.02390\n8    Cardinal : Robin  0.23600       1.0000 0.111000     1.00000 0.23700\n9  Cardinal : Catbird  0.00471       0.0471 0.001950     0.01950 0.00476\n10    Robin : Catbird  0.42400       1.0000 0.461000     1.00000 0.74600\n   p.adj.Chisq\n1       1.0000\n2       1.0000\n3       1.0000\n4       0.0141\n5       1.0000\n6       1.0000\n7       0.2390\n8       1.0000\n9       0.0476\n10      1.0000\n\n\nThe test then shows all-pair comparisons with regular (what we should compare to .005 now, but we don’t usually know that!) and adjusted p-values (which have compensated for multiple tests so we can use our normal .05 cutoff- use this one!) for each test we have covered (you should use a post-hoc that matches what you did for the overall, or omnibus, comparison).\nYou can order and display these differently if it helps. For example, if we used the \\(\\chi^2\\) test.\n\nbonf_correct[order(bonf_correct$p.adj.Chisq), c(\"Comparison\", \"p.adj.Chisq\")]\n\n           Comparison p.adj.Chisq\n4     Finch : Catbird      0.0141\n9  Cardinal : Catbird      0.0476\n7     Junco : Catbird      0.2390\n1       Finch : Junco      1.0000\n2    Finch : Cardinal      1.0000\n3       Finch : Robin      1.0000\n5    Junco : Cardinal      1.0000\n6       Junco : Robin      1.0000\n8    Cardinal : Robin      1.0000\n10    Robin : Catbird      1.0000\n\n\nWe see that catbirds different from finches and cardinals in the proportion of males and females, while all other species do not differ. Note the un-adjusted p-value for this comparison pair is the same we would have found from just comparing the two groups\n\nchisq.test(matrix(c(6,30,12,7), nrow=2, byrow=T))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matrix(c(6, 30, 12, 7), nrow = 2, byrow = T)\nX-squared = 10.189, df = 1, p-value = 0.001413\n\nbonf_correct[bonf_correct$Comparison==\"Finch : Catbird\", \"p.Chisq\"]\n\n[1] 0.00141\n\n\nHowever, this comparison is very conservative. Many other options exist, and we will explore two here.\nThe sequential Bonferroni, or Holm’s, method, allocates your \\(\\alpha\\) to accept as many tests as signficant as possible while still controlling for the FWER. To do this, it orders the post-hoc tests by p-value, smallest to largest. It then rejects the null hypothesis attached to the smallest p-value and subtracts that p-value from \\(\\alpha\\). It continues to do this until \\(\\alpha\\) is too small to reject the next smallest p-value. I think of it as buying p-values with \\(\\alpha\\).\nWe can use this approach by simply changing the method. Let’s also order our results again for viewing.\n\nholm_correct &lt;- pairwiseNominalIndependence(bird_ratio_table, compare=\"col\", method = \"holm\")\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nholm_correct[order(holm_correct$p.adj.Chisq), c(\"Comparison\", \"p.adj.Chisq\")]\n\n           Comparison p.adj.Chisq\n4     Finch : Catbird      0.0141\n9  Cardinal : Catbird      0.0428\n7     Junco : Catbird      0.1910\n3       Finch : Robin      0.9940\n1       Finch : Junco      1.0000\n2    Finch : Cardinal      1.0000\n5    Junco : Cardinal      1.0000\n6       Junco : Robin      1.0000\n8    Cardinal : Robin      1.0000\n10    Robin : Catbird      1.0000\n\n\nAlthough we still reject the same null hypotheses, notice the adjusted p-value for the junco:catbird comparison is slightly lower. This is due to the “holm” approach.\nA final approach we will demonstrate is called the False Discovery Rate, or FDR, approach. It’s similar to the Holm’s approach in that it starts by ordering the post-hoc tests by p-value, smallest to largest. However, it then rejects the null hypothesis attached to the largest p-value that is &lt; \\(\\alpha\\) and subsequently rejects all null hypotheses attached to smaller p-values. It is thus less conservative but basically attempts to maximize the number of hypotheses you reject given a set \\(\\alpha\\).\n\nfdr_correct &lt;- pairwiseNominalIndependence(bird_ratio_table, compare=\"col\", method = \"fdr\")\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(Dataz, ...): Chi-squared approximation may be incorrect\n\nfdr_correct[order(fdr_correct$p.adj.Chisq), c(\"Comparison\", \"p.adj.Chisq\")]\n\n           Comparison p.adj.Chisq\n4     Finch : Catbird      0.0141\n9  Cardinal : Catbird      0.0238\n7     Junco : Catbird      0.0797\n3       Finch : Robin      0.3550\n8    Cardinal : Robin      0.4740\n6       Junco : Robin      0.6150\n1       Finch : Junco      1.0000\n2    Finch : Cardinal      1.0000\n5    Junco : Cardinal      1.0000\n10    Robin : Catbird      1.0000\n\n\nAgain, we reject the same null hypotheses, but again you also observe a slight change in p-values. This indicates the important point here is controlling for the FWER. In later chapters we will expand this idea further by focusing on specific subsets of tests or comparisons, although these approaches are less commonly used."
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#goodness-of-fit-tests",
    "href": "content/Compare_proportions_among_populations.html#goodness-of-fit-tests",
    "title": "Comparing proportions among groups",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nAbove we focused on comparing proportions among multiple groups using the \\(\\chi^2\\) test. This same approach (comparing expected vs observed values) can also be used to see if a single sample follows a specific distribution. In general, these tests are used to test hypotheses in the form of:\n\\[\n\\begin{split}\nH_O: \\textrm{data come from a particular discrete probability distribution} \\\\\nH_A: \\textrm{data do not come from a particular discrete probability distribution} \\\\\n\\end{split}\n\\]\nFor example, we used a binomial test in earlier chapters to see if our observed number of old (9) and young (9) finches matched a probability of .75.\n\nbinom.test(x=9, n=18, p=.75)\n\n\n    Exact binomial test\n\ndata:  9 and 18\nnumber of successes = 9, number of trials = 18, p-value = 0.02499\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.2601906 0.7398094\nsample estimates:\nprobability of success \n                   0.5 \n\n\nNote we instead consider what we observed vs what we expected using a \\(\\chi^2\\) test.\n\nchisq.test(c(9,9), p=c(.75,.25))\n\nWarning in chisq.test(c(9, 9), p = c(0.75, 0.25)): Chi-squared approximation\nmay be incorrect\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(9, 9)\nX-squared = 6, df = 1, p-value = 0.01431\n\n\nBoth p values are &lt; .05, so we reject the null hypothesis, \\(H_O: p=.75\\) . However, the p values are different? Do you remember why?\nThe \\(\\chi^2\\) test is an approximation! Thus, we prefer the binomial test to the \\(\\chi^2\\) test when we only have 2 categories (binomial data) but for more groups we can use the \\(\\chi^2\\) test. Since we are still using a \\(\\chi^2\\) test, these goodness-of-fit tests have the same assumptions as contingency analysis. If this isn’t true, we can combine categories as needed (note the binomial distribution is the extreme form of combining categories!), or we can use a G-test.\nThe main issue with goodness-of-fit tests is understanding how many degrees of freedom should be used for the null distribution. It usually depends on how many parameters you are estimating. Note, if parameters are determined outside of R, the software may not give appropriate answers.\nFor example, Klem wanted to know if bird strikes differed among months. The null hypothesis was no difference among months, which he tested by assuming a single probability described the likelihood of strikes regardless of month. An alternative (attached to a full model) would have assumed different probabilities for different months.\nIn this case, we estimated one parameter (p), or the number of collisions in one month is determined by the others due to our sample size, so we have 12-1=11 degrees of freedom. If we instead compared our data to a binomial distribution, we might need to estimate p and have one value set by the others. Another common use of goodness-of-fit tests is to determine if the number of offspring (or seeds) match ratios predicted by Punnet square crosses.\nAnother common distribution that data are tested against is the Poisson distribution. It describes the probability that a certain number of events occur in a block of time (or space), when those events happen independently of each other and occur with equal probability at every point in time or space. The Poisson distribution thus matches a null hypothesis that incidents are randomly distributed in space or time. The Poisson distribution is an extension of binomial that occurs when p is very low and n is large. When this occurs, note N-S ~ N (and a few other things happen), which lead to the entire distribution being described by a single parameter \\(\\mu \\approx Np\\), which is the mean and variance.\nThese traits also allow you to determine if data are random, clumped, or uniform. If the mean of a dataset is approximately the same as the variance, the points may be randomly distributed. If the variance is much greater than the mean, the data may be clumped. Alternatively, if the variance is much less than the mean, the data may be uniformly distributed."
  },
  {
    "objectID": "content/Compare_proportions_among_populations.html#next-steps",
    "href": "content/Compare_proportions_among_populations.html#next-steps",
    "title": "Comparing proportions among groups",
    "section": "Next steps",
    "text": "Next steps\nOur following chapters will extend ideas about testing differences among populations, including post-hoc tests, to continuous data."
  },
  {
    "objectID": "content/getting_started.html",
    "href": "content/getting_started.html",
    "title": "Before the first class",
    "section": "",
    "text": "Over the course of the semester/reading this book, our (ambitious) goals are to\nTo prepare for our first few lessons"
  },
  {
    "objectID": "content/getting_started.html#concept-stuff",
    "href": "content/getting_started.html#concept-stuff",
    "title": "Before the first class",
    "section": "Concept stuff",
    "text": "Concept stuff\n\nCheck out the class website\nWatch this video"
  },
  {
    "objectID": "content/getting_started.html#tech-stuff",
    "href": "content/getting_started.html#tech-stuff",
    "title": "Before the first class",
    "section": "Tech stuff",
    "text": "Tech stuff\n\nGet access to R!. You can make an account at Rstudio cloud (https://rstudio.cloud/). You can also install R (https://cran.r-project.org/) and Rstudio (https://www.rstudio.com/) on your machine, but I strongly recommend starting with Rstudio cloud.\nRstudio cloud is free for up to 25 hours/month, you don’t have to maintain it, and it gives gives a standard install (same on all machines, so your intro/ our training may be smoother). You can also do both. If you need help, videos are at :\n\nDownloading R\nDownloading Rstudio\nMaking a Rstudio cloud account\n\nJoin the github classroom we’ll be using for our sessions\n\nlook for email from Blackboard! \nWhen you visit the page it will ask you to connect or create a github repository. You can use any name (be anonymous or not) that you want. This is a free process.\n\n\n\nOptional (get a head start if you want)\nIt may be easier to open these intructions in a browser so you can follow along there while working in Rstudio!\nAfter you join the github classroom, you’ll make a clone of the repository onto your machine. First, find your copy of the repository. You can follow the github classroom link again, or log into github and then visit https://github.com/settings/repositories. Find the repository called data_science_intro_YOURGITHUBUSERNAME, and click on it. Then follow along below - find instructions for Rstudio cloud or Rstudio desktop depending on your setup.\n\nIf you are using Rstudio cloud…\nVideo at Accepting your first github repository (from github classroom) and cloning to Rstudio cloud\nLog into your Rstudio cloud account. You’ll see something like this:\n\n\n\nRstudio cloud home screen\n\n\nTo copy a repository, select New Project, New Project from Github repo. Next you’ll need to enter the url for your repository. To find this, click on the Code button from the github page for your repository (instructions above!)\n\n\n\nClick on Code to get repository url\n\n\nCopy the web url (or click the copy icon). Input that into the field asking for the URL of your github repository.\nNote you may need to enter your github username and password to create the repository.\nThe next screen will bring you to a “normal” RStudio screen. We’ll come back to this in the first class or two!\n\n\nIf you are using RStudio on your desktop (or via a server…anywhere that\nlooks like an RStudio screen)\nVideo at Accepting your first github repository (from github classroom) and cloning to Rstudio desktop\nTo start working on an assignment, open RStudio.\n\n\n\nSelect File &gt; New Project in Rstudio\n\n\nSelect file, new project, Version control. On the next screen select git. If this isn’t available, you may need to install git (free) on your system. You can download it at https://git-scm.com/download/.\nNext you’ll need to enter the url for your repository. To find this, click on the Code button from the github page for your repository (instructions above!).\n\n\n\nClick on Code to get repository url\n\n\nCopy the web url (or click the copy icon). Input that into the Rstudio Repository URL space. You can select/edit what you want the repository to be called and where its stored (its just a folder on your computer). For example, I have a Repositories folder in my main hard drive where I save all of these. Then select Create project. Whatever you choose, the project will be saved in new folder in that location using the name you chose. Note you may need to enter your github username and password to create the repository.\nYou also may get an error/warning about personal access token! this happens at different points on different machines (thus why Rstudio cloud is nice). If you see this now, don’t worry. We’ll cover it (a known issue) in class.\nThe next screen will bring you to a “normal” RStudio screen. We’ll come back to this in the first class or two!"
  },
  {
    "objectID": "content/Intro_to_ggplot2.html",
    "href": "content/Intro_to_ggplot2.html",
    "title": "Intro to ggplot2!",
    "section": "",
    "text": "Before doing this, review the Estimation and Probability lecture set slides from https://sites.google.com/view/biostats/lessons/estimation-and-probablity and the 2_estimation_lecture.R script in the lecture_files folder of the CUNY-BioStats github repository.\nRemember you should"
  },
  {
    "objectID": "content/Intro_to_ggplot2.html#overview",
    "href": "content/Intro_to_ggplot2.html#overview",
    "title": "Intro to ggplot2!",
    "section": "Overview",
    "text": "Overview\n\nggplot2 basics\nggplot2 is a great plotting package that allows a lot of control over your output. Let’s do some examples using the sleep dataset that we left off with last week. Load the dataset\n\nsleep &lt;- read.csv(\"https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/sleep.csv\", stringsAsFactors = T)\n#need to use stringsAsFactors to make characters read in as factors\n\nggplot2 works in layers so you can or subtract as needed. Provided code is verbose here so you can see what its doing. First, install and call the package.\n\nlibrary(ggplot2)\n\nTo make a plot, first set a base layer using the ggplot function.\n\ndreaming_sleep_relationship &lt;- ggplot(sleep, aes(x=TotalSleep, y = Dreaming))\n\nHere we are naming a dataframe to use (first argument), then noting which columns to use for the x and y axis (under the aes argument, stands for aesthetics).\nNote when we do this we get a blank graph (if we name the ggplot output, we have to call it to see it!)\n\ndreaming_sleep_relationship\n\n\n\n\nNext we add data layers using geom_ commands. Let’s start with a scatter plot, which we make using the geom_point command.\n\ndreaming_sleep_relationship_scatter &lt;- ggplot(sleep, aes(x=TotalSleep, y = Dreaming)) + \n  geom_point()\n\nAgain, nothing is shown, but not the object is saved! We can call it\n\ndreaming_sleep_relationship_scatter\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe can also just call it directly, but when/if we do this the object is not saved in the environment.\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point()\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIf nothing extra is given, the geom_commands inherit everything from the ggplot command. So here we get a scatter plot of the relationship between TotalSleep and Dreaming. Note the axis labels are the column titles, which may not be what we want in the end in regards to readability.\nHowever, now you have a basic plot. You can also use other arguments in geom_layer commands to add to it. For example, let’s color these by primate\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Primate))\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we’ve added information on primates. Since that require us to get more data from the dataset, we had to add another aes argument. Note this is different from (not evaluated in code, as it causes an error!)\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(colour=\"Primate\")\n\nor\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(colour=\"blue\")\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe first causes an error as primate isn’t a color. The second makes all points blue! Also note the 2nd method loses the legend as color now conveys no information.\nIn general, you have to put things you want to plot in the aes argument area and anything outside of that changes the entire plot. For example, we can change the size of all points using\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(size = 4)\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis is also a good time to talk about renaming factor labels. You may want to change Primate levels to Yes and No for your graph. Lots of ways to do this, but the revalue function in the plyr package is nice (and we’ll use this suite of packages often, same person developed ggplot2, plyr, and reshape)\n\nlibrary(plyr)\nsleep$Taxa &lt;- revalue(sleep$Primate, c(Y = \"Primate\", N = \"Non-primate\"))\n\nNotice what I did above. I made a new column from an existing one using a name I might want on a legend. Now I can use it in a graph.\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa))\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nI can also just change the legend title directly or change legend text, but often workign with the dataframe is easier for me.\nIf we wanted the levels of Primate in a different order, we can use the relevel function in the plyr package to set one as the “first” level (and then do this sequentially to get them in the right order if needed). You can also change level orders using the factor or ordered functions for multiple levels at once.\n\nsleep$Taxa &lt;- relevel(sleep$Taxa, \"Primate\" )\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa))\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFinally, we can use the theme or related functions (like xlab, ylab, ggtitle) to change how the graph looks. Note, all the code here is verbose so you can change as needed, but you rarely need all this.\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa), size = 4) +\n  #below here is ylabel, xlabel, and main title\n  ylab(\"Average hours spent dreaming daily\") +\n  xlab(\"Average hours spent sleeping daily\") +\n  ggtitle(\"Time spent dreaming increases with total sleeping time\") +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\"))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYou can also directly change legend title and colours with the scale_ commands\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa), size = 4) +\n  #below here is ylabel, xlabel, and main title\n  ylab(\"Average hours spent dreaming daily\") +\n  xlab(\"Average hours spent sleeping daily\") +\n  ggtitle(\"Time spent dreaming increases with total sleeping time\") +\n  #scale commands help with legends\n  scale_colour_manual(name=\"Type of mammal\",values = c(\"#FFA373\",\"#50486D\")) +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\"))\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIn general scale_[whatever you had aes commands]_manual lets you set colors or codes. To see color codes go to this chart\nYou can also facet a graph by another column. For example, I can split the graph I already made by Taxa\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa), size = 4) +\n  #below here is ylabel, xlabel, and main title\n  ylab(\"Average hours spent dreaming daily\") +\n  xlab(\"Average hours spent sleeping daily\") +\n  ggtitle(\"Time spent dreaming increases with total sleeping time\") +\n  #scale commands help with legends\n  scale_colour_manual(name=\"Type of mammal\",values = c(\"#FFA373\",\"#50486D\")) +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\")) +\n  facet_wrap(~Taxa, ncol = 1)\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice doing this and having legend may be redundant, so I can remove the legend\n\nggplot(sleep, aes(x=TotalSleep, y = Dreaming)) +\n  geom_point(aes(colour=Taxa), size = 4) +\n  #below here is ylabel, xlabel, and main title\n  ylab(\"Average hours spent dreaming daily\") +\n  xlab(\"Average hours spent sleeping daily\") +\n  ggtitle(\"Time spent dreaming increases with total sleeping time\") +\n  #scale commands help with legends\n  scale_colour_manual(name=\"Type of mammal\",values = c(\"#FFA373\",\"#50486D\")) +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\"),\n        strip.text.x = element_text(size = 18, colour = \"purple\")) +\n  facet_wrap(~Taxa, ncol = 1) +\n  guides(colour=FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\n\n\n\nI also added a theme section to change the facet label. All this shows how you are focused on adding or layering levels in ggplot2.\nYou can save the most recent plot directly to your working directory using\n\nggsave(\"Fig1.jpg\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 14 rows containing missing values (`geom_point()`).\n\n\nThis is useful when we need to send just an image to someone (or add it to a document). You can also just save using rstudio functionality.\nggplot2 is a great example of needing to undertand basic functionality without having to remember everything. The intro class lecture and accompanying code should help you get started. A few other points that often come up are noted below.\n\n\nHistograms\nFor histograms, you only need one axis (frequency is calculated automatically)\n\nggplot(sleep, aes(x=Dreaming)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 12 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nNote we can just copy our theme info from above and modify as needed (or ggplot2 will largely skip un-needed info). You can also save and name a theme so you don’t have to do all this everytime.\n\nggplot(sleep, aes(x=Dreaming)) +\n  geom_histogram() + \n  #below here is ylabel, xlabel, and main title\n  ylab(\"Frequency\") +\n  xlab(\"Average hours spent dreaming daily\") +\n  ggtitle(\"Distribution of hours spent dreaming\") +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\"),\n        strip.text.x = element_text(size = 18, colour = \"purple\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 12 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nFinally, remember you can subset the dataframes you feed to the ggplot functions (or any other function for that matter). For example, let’s just do a histogram of just primate sleep.\n\nggplot(sleep[sleep$Taxa == \"Primate\",], aes(x=Dreaming)) +\n  geom_histogram() + \n  #below here is ylabel, xlabel, and main title\n  ylab(\"Frequency\") +\n  xlab(\"Average hours spent dreaming daily\") +\n  ggtitle(\"Distribution of hours spent dreaming\") +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill=\"white\"),\n        panel.grid.minor.y = element_line(size=3),\n        panel.grid.major = element_line(colour = \"black\"),\n        plot.background = element_rect(fill=\"gray\"),\n        legend.background = element_rect(fill=\"gray\"),\n        strip.text.x = element_text(size = 18, colour = \"purple\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nNot interesting, but you get the idea.\n\n\nBarcharts and confidence intervals\nEstimating is a key part of statistics and should include the value you are estimating and an estimate of uncertainty. Graphs typically show this using confidence intervals, which rely on samples of means following a normal distribution that we can describe. If we assume the estimate (not the data!) is normally distributed, we can assume things about uncertainty. Namely, we can build a 95% confidence interval around our estimate (meaning the true mean is in the range 95 out of 100 times we create a sample).\nNow’s let do these in R. Confidence intervals are often tied to barcharts. Although these are common in practice, they are not easy by default in R as statisticians don’t love them. That’s because they use a lot of wasted color. I’ll show this in a moments. However, since they are common I’ll show you how to build them.\nLet’s go back to the sleep dataset and consider the average total sleep time speed for each exposure level. First, lets change exposure to factors and label them\n\nstr(sleep) #just a reminder\n\n'data.frame':   62 obs. of  13 variables:\n $ Species    : Factor w/ 62 levels \"Africanelephant\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ BodyWt     : num  6654 1 3.38 0.92 2547 ...\n $ BrainWt    : num  5712 6.6 44.5 5.7 4603 ...\n $ NonDreaming: num  NA 6.3 NA NA 2.1 9.1 15.8 5.2 10.9 8.3 ...\n $ Dreaming   : num  NA 2 NA NA 1.8 0.7 3.9 1 3.6 1.4 ...\n $ TotalSleep : num  3.3 8.3 12.5 16.5 3.9 9.8 19.7 6.2 14.5 9.7 ...\n $ LifeSpan   : num  38.6 4.5 14 NA 69 27 19 30.4 28 50 ...\n $ Gestation  : num  645 42 60 25 624 180 35 392 63 230 ...\n $ Predation  : int  3 3 1 5 3 4 1 4 1 1 ...\n $ Exposure   : int  5 1 1 2 5 4 1 5 2 1 ...\n $ Danger     : int  3 3 1 3 4 4 1 4 1 1 ...\n $ Primate    : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 1 1 1 1 1 2 ...\n $ Taxa       : Factor w/ 2 levels \"Primate\",\"Non-primate\": 2 2 2 2 2 2 2 2 2 1 ...\n\nsleep$Exposure &lt;- factor(sleep$Exposure)\n\nCheck levels\n\nlevels(sleep$Exposure)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nand relabel if you want (just for example here)\n\nlevels(sleep$Exposure)&lt;- c(\"Least\",\"Less\", \"Average\", \"More\", \"Most\") \n\nNext, we need to get the average and standard deviation for each group (remember this is tied to the normal distribution!). If we wanted to this by hand, we could do something like thi (let’s just focus on least for an example, and note we have to remove NA data)\n\nmean(sleep[sleep$Exposure == \"Least\", \"TotalSleep\"], na.rm = T)\n\n[1] 12.94615\n\n\nThis is our estimate. The standard deviation of this estimate is\n\nsd(sleep[sleep$Exposure == \"Least\", \"TotalSleep\"], na.rm = T) / \n  sqrt(length(sleep[sleep$Exposure == \"Least\" & is.na(sleep$TotalSleep) == F, \"TotalSleep\"]))\n\n[1] 0.7833111\n\n\nwhich is equivalent to\n\nsd(sleep[sleep$Exposure == \"Least\", \"TotalSleep\"], na.rm = T) / \n  sqrt(length(na.omit(sleep[sleep$Exposure == \"Least\", \"TotalSleep\"])))\n\n[1] 0.7833111\n\n\nWe also call this the standard error of the mean.\nFortunately, we can also do this using a function from the Rmisc package in R, as ggplot2 doesn’t have it built in (maybe because bar charts are a bad idea?).\n\nlibrary(Rmisc)\n\nLoading required package: lattice\n\nsleep_by_exposure &lt;- summarySE(sleep, measurevar = \"TotalSleep\", groupvars = \"Exposure\", na.rm = T)\n\nInspect the table\n\nsleep_by_exposure\n\n  Exposure  N TotalSleep       sd        se       ci\n1    Least 26   12.94615 3.994119 0.7833111 1.613259\n2     Less 13   11.11538 3.957029 1.0974823 2.391209\n3  Average  4    8.57500 1.808084 0.9040419 2.877065\n4     More  5   10.72000 1.663430 0.7439086 2.065421\n5     Most 10    4.19000 1.776670 0.5618323 1.270953\n\n\nNow we can use this summarized data to make a graph that shows uncertainty (95% confidence intervals)\n\nggplot(sleep_by_exposure\n       , aes(x=Exposure, y=TotalSleep)) +\n  geom_col(size = 3) +\n  geom_errorbar(aes(ymin=TotalSleep-ci, ymax=TotalSleep+ci), size=1.5) +\n  ylab(\"Total sleep (hours per day\")+ggtitle(\"Sleep across different taxa\")+\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nNow to show why barplots waste ink. Note we can show the same information with\n\nggplot(sleep_by_exposure\n       , aes(x=Exposure, y=TotalSleep)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin=TotalSleep-ci, ymax=TotalSleep+ci), size=1.5) +\n  ylab(\"Total sleep (hours per day\")+ggtitle(\"Sleep across different taxa\")+\n  theme(axis.title.x = element_text(face=\"bold\", size=28), \n        axis.title.y = element_text(face=\"bold\", size=28), \n        axis.text.y  = element_text(size=20),\n        axis.text.x  = element_text(size=20), \n        legend.text =element_text(size=20),\n        legend.title = element_text(size=20, face=\"bold\"),\n        plot.title = element_text(hjust = 0.5, face=\"bold\", size=32))\n\n\n\n\nAll the exta color is nice, but its not really adding anything!"
  },
  {
    "objectID": "content/Intro_to_ggplot2.html#lets-practice",
    "href": "content/Intro_to_ggplot2.html#lets-practice",
    "title": "Intro to ggplot2!",
    "section": "Let’s practice!",
    "text": "Let’s practice!\nLet’s return to the mammal sleep dataset that we left off with last week (Make sure you did the first assignment!).\nLoad the dataset\n\nsleep &lt;- read.csv(\"https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/sleep.csv\", stringsAsFactors = T)\n#need to use stringsAsFactors to make characters read in as factors\n\nLast time you used the built-in plot functions to do some plots. Let’s replace those with ggplot2 and do some more.\n\n1\n\nFirst plot how TotalSleep is explained by BrainWt (remember the issues with the data). Use ggplot2 to plot the relationship.\n\n\n\n2\n\nNext color code each plot point by whether or not its a primate. In order to do this you can use the Primate column or (following class code) make a new column called Taxa to represent the information (hint:search for “ revalue”). Make sure axes are well-labeled.\n\n\n\n3\n\nLet’s work with histograms.\n\n\nWhat type of variation do we see in total time spent sleeping? Create a histogram to explore this issue.\nFacet the graph you created based on whether or not the animal is a primate (Primate column).\nNow only graph the data for primates.\n\n\n\n4\n\nDevelop a properly-labeled bar graph with error bars to explore how total sleep changes with\n\n\nPrimate (relabeled as yes/no as Primate/Non-Primate; note there are multiple ways to do this!) – use a 95% confidence interval for the bar\nPredation risk (as a factor!) – use 1 standard error for the bar. Note the difference!"
  },
  {
    "objectID": "content/Intro_to_ggplot2.html#estimates-and-certainty-concepts",
    "href": "content/Intro_to_ggplot2.html#estimates-and-certainty-concepts",
    "title": "Intro to ggplot2!",
    "section": "Estimates and Certainty Concepts",
    "text": "Estimates and Certainty Concepts\n\n5\n\nWhat does a 95% confidence interval mean?\n\n\n\n6\n\nTo make sure you understand the ideas of sampling, confidence intervals, and the central limit theorem, review the visualizations produced by UBC:\n\n\nhttps://www.zoology.ubc.ca/~whitlock/Kingfisher/SamplingNormal.htm\nhttps://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm\nhttps://www.zoology.ubc.ca/~whitlock/Kingfisher/CLT.htm\n\n\n\n7\n\nFor this question you’ll need the central_limit_theorem.R script from the code_examples folder. Download it to your computer and open it. Alternatively, go ahead and make a copy of the CUNY-Biostats repository. You won’t have write access but can keep one up-to-date on your machine/cloud (pull occassionally!).\n\nOnce you get the script, open it in Rstudio (it will be in another tab!). Make sure you have the VGAM library installed (if you open the script n Rstudio, it will likely prompt you at the top). Then use the Source button (next to the Run command we’ve been using for lines or segments). Source runs the entire code at once (similar to knitting an Rmd file) without showing any console output, but graphs and objects are still produced!\nYou can also do this from the web (included here). When you knit the file, output will appear in your final file. However, its nice to know what Source does in general.\n\nlibrary(VGAM)\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nsource(\"https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/code_examples/central_limit_theorem.R\")\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\nThis script follows the UBC tutorial to show you how well the CLT (central limit theorem) works (and how it functions). This will be useful in coming to understand when you can trust tests based on the normality of means. The script produces output (graphs) that allow you to examine 6 distributions that differ in shape (skewness and kurtosis) and how those traits interact with sample size to influence the normality of means.\nSource it (or look for the graphs produced in your knitted file) and and then review the plots and consider how sample size interacts with the shape of underlying distributions to influence how quickly sample means approach normality. The noted distributions are:\n\nNormal(Z) (0,1) {no Kurtosis / no skewness / no truncation}\nDouble exponential (0,2) {high Kurtosis / no skewness / no truncation}\nUniform(0,1) {moderate Kurtosis / no skewness / double truncation}\nExponential(1,1) {high asymmetric Kurtosis / high skewness / single truncation}\nChi-square(df=4) {low Kurtosis / moderate skewness / single truncation}\nBinomial distribution (p=.7) {discrete distribution]"
  },
  {
    "objectID": "content/Intro_to_Rmd.html",
    "href": "content/Intro_to_Rmd.html",
    "title": "Intro to Rmd",
    "section": "",
    "text": "Overview\n\nRmd basics\nRmd files differ from R files in that they combine regular text with code chunks. This is a code chunk\n\nprint(\"this is a chunk\")\n\n[1] \"this is a chunk\"\n\n\nCode chunks combine code with output. When combined with regular text/prose, this makes it easier to produce a range of documents. You set the output in the YAML header (the stuff between the 3 dashes you see at top of this document).\nAfter you write the file, you Knit it to turn the Rmd file into the selected output. Try it now. Note the first time you do this in a project you may be prompted to install a number of packages! If you are using a webservice you may also need to allow pop-ups in your browser. Don’t be surprised if a new window pops up (it should).\n\n\n\nThe knit button turns your .rmd file into other products\n\n\nThe Knit button saves the .Rmd file and renders a new version whose output depends on what you selected in the header. Here we have html_document, so if everything works a preview of a webpage like document should appear. The file also produces a github friendly .md file. This means you should only edit the Rmd file (leave the md and output files alone! They are automatically produced any changes you make there will be overwritten by your next knit).\nWhen you Knit a file, it runs in a totally new R instance. this means anything you only added in your instance (like working in the console) won’t be available. In other words, its the best way to see what a “new” user gets when they use your code.\nhowever, you don’t have to knit the file every time. if you just want to see output, note you can press the green button next to an R chunk.\n\n\n\nThe green arrows just runs the chunk in the console and shows the output\n\n\n\nprint(\"this is a chunk\")\n\n[1] \"this is a chunk\"\n\n\nNow we’ll start changing the file to show you how rmarkdown works. First, amend the file by replacing the NAME and DATE spots in the header (top of the file between the — markers) with your name and the real date. Then Knit the file again. You should see your name in the new preview.\nRstudio has a Markdown Quick Reference guide (look under the help tab), but some general notes.\n\nPound/Hashtag signs denote headers\nyou can surround something double asterisks for bold or single asterisks for italics\nlists are denoted by numbers or asterisks at beginning of line (followed by space!)\n\nand can be indented for sublevels\n\nR code can be done inline, but is generally placed in stand-alone chunks\n\nthese will, by default, show the code and output\n\nlots of other options exist!\n\nThe main idea is Rmd files allow you to combine code, text, graphs, etc into multiple outputs that you can share (including with coding illiterate colleagues who just want output).\nTo practice working with Rmd files and R, work through the questions below. You can also get more help with this video\n\n\n\nPractice in R\n\n1\n\nLet x be defined by\n\n\nx &lt;- 5:15\n\nTry executing this chunk (in R studio, not the webview) by clicking the Run button within the chunk or by placing your cursor inside it and pressing Ctrl+Shift+Enter.\nThis will run the code in the Console. You may need to switch to Console (from Rmarkdown) in the lower right window area to see this. The executed code is also displayed in your processed file (hit Knit again to see this!).\nNote running this chunk has added an object named x to the Environment tab area (top right area of screen). But nothing was “returned” in the console. You prove this by typing x in the console. What does it return?\nDetermine what the “:” does! Complete the following sentence:\nThe : means FILL THIS IN.\n\n\n2\n\nNow try to guess the output of these commands\n\n\nlength(x)\nmax(x)\nx[x &lt; 10]\nx^2\nx[ x &lt; 12 & x &gt; 7]\n\nINSERT AN R CHUNK HERE AND RUN EACH OF THESE COMMANDS. Add a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Ctrl+Alt+I. Then state what each of these does.\n\n\n3\n\nIs -1:2 the same as (-1):2 or -(1:2)? INSERT AN R CHUNK HERE AND RUN EACH OF THESE COMMANDS. Then state what each of these does.\n\n\n\n\nData input, plotting, and tests\nYou can read in a dataset from the internet following this protocol.\n\nsleep &lt;- read.csv(\"http://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/sleep.csv\", stringsAsFactors = T)\n\nRun this chunk and note it has added an object named sleep to the environment.\nInfo on the dataset is viewable @ http://www.statsci.org/data/general/sleep.html.\n\n4\n\nHow many rows does the sleep data set have (hint: ?dim)? What kind of data is stored in each variable?\n\nENTER ANSWERS HERE. ADD ANY R CHUNKS YOU USED TO FIND THE ANSWER.\n\n\n5\n\nChange the column named BodyWt to Body_weight”* in the sleep dataset.\n\nADD ANY R CHUNKS YOU USED TO COMPLETE THE TASK.\n\n\n6\n\nProduce a plot of how TotalSleep differs between primates and other species. What is this plot showing?\n\nNote, as of early 2020 R no longer reads in strings as factors! This means the Primate column, which is full of “Yes”s and “No”s, reads in as words and R doesn’t know how to plot them. There are many ways to handle this. You can modify the read.csv command (add stringsAsFactors = T option), eg\n\nsleep &lt;- read.csv(\"http://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/sleep.csv\", stringsAsFactors = T)\n\nIf you do this, you’ll need to rechange anything you previously updated to the object (like renaming the BodyWt column).\nYou can also modify a single column for the actual object\n\nsleep$Primate &lt;- factor (sleep$Primate)\n\nor for a single command, eg (plot not actually shown!)\n\nplot(BodyWt ~ factor(Primate), data = sleep)\n\nNOTE YOU CAN ADD A PLOT TO THE DOCUMENT TOO! AMEND THE BELOW AS NEEDED.\n\nplot(cars)\n\n\n\n\n\n\n7\n\nThe sleep dataset begs to have a linear model fit for it. Let’s consider. First plot how TotalSleep is explained by BrainWt. Are there any issues with the data? Exclude any outlier and fit a linear model to obtain the p-value for the model (hint: summary()). What does this imply?\n\nENTER ANSWERS HERE. ADD ANY R CHUNKS YOU USED TO FIND THE ANSWER.\n\n\n\nEXTRA QUESTIONS\nnot required\n Dow Puffin Matthew Zalewski / CC BY (https://creativecommons.org/licenses/by/3.0)\n\n8\n\nSometimes data doesn’t have headers (column names),so you have to add them. Download a dataset on alcids (birds like puffins and auklets) from https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/alcids55.csv.\nYou’ll need to modify the read.csv function by specifying header = False, then use the names function to name the columns [“year”, “a1_abund”, “NAO”, “a2_abund”, “a3_abund”, “a4_abund”, “a5_abund”, “a6_abund”]. Try it and check your input using the head command.\n\nENTER ANSWERS HERE. ADD ANY R CHUNKS YOU USED TO FIND THE ANSWER.\n\n\n9\n\nHere’s a sample dataset:\n\n\n\n\nDate\ngreenness\nRichness\nhabitat\n\n\n\n\n12-25-2009\n13766\n46\nforest\n\n\n01-01-2010\n50513\n60\nforest\n\n\n01-15-2010\n25084\n60\ngrassland\n\n\n\nEnter it into R (manually or via a .csv). (Hint: you have a piece of this in the code already). Check your input using the head() command.\nENTER ANSWERS HERE. ADD ANY R CHUNKS YOU USED TO FIND THE ANSWER."
  },
  {
    "objectID": "content/Probability.html",
    "href": "content/Probability.html",
    "title": "Probability",
    "section": "",
    "text": "We’ve already address probability. When we stated the 95% confidence interval means that if we make these intervals from 100 samples that we expect 95 of them to contain the true mean, we are discussing probability. An even easier example is flipping a coin. You probably know there is a 50% chance of a coin landing on heads. That doesn’t mean given flip will be half heads and half tails. Both of these statements refer to likely outcomes if we do something many, many times!"
  },
  {
    "objectID": "content/Probability.html#what-is-probability",
    "href": "content/Probability.html#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability?\nTo put it specifically, the probability of an outcome is its true relative frequency, the proportion of times the event would occur if we repeated the same process over and over again. We can describe the probability if an outcome by considering all the potential outcomes and how likely each is. If we describe all the outcomes, the total probability must be equal to 1 (since frequency is typically measured as a fraction!). Some probability distributions can be described mathematically, others are a list of possible outcomes, and others are almost impossible to solve. The “impossible” ones require simulations, and we will return to this for our introduction to Bayesian analysis.\nThe simplest case is when we focus on outcomes for a single trait that falls into specific categories. These outcomes are often mutually exclusive, meaning only one can happen (like our heads and tails example!), and lead to discrete probability distributions (meaning each outcome has to be a specific value). Another example is rolling a 6-sided die. The die can only land on numbers 1 to 6 (so 2.57 is not an option!).\n\ndie_roll &lt;- data.frame(Number = 1:6, Probability = rep(1/6,6))\ndie_roll$Number &lt;- factor(die_roll$Number)\n\nlibrary(ggplot2)\nggplot(die_roll, aes(x=Number, y= Probability, fill=Number)) +\n  geom_col() \n\n\n\n\nin this example, the probability of rolling a 6 is .167. You may see this written as\n\\[\nP[roll=6]= \\frac{1}{6} \\sim .167\n\\]Obviously when you roll a die you don’t roll .167 of a 6. You roll a 1, 2, 3, 4, 5, or 6. Again, probability refers to the expected outcome over multiple attempts.\nCompare this to a continuous probability distribution, where the outcome can take any value in a given range.\n\nggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = \"orange\")+\n  labs(y=\"Probability\", x=\"Outcome\")\n\n\n\n\nHere’s an odd outcome: Since the outcome can take on any value in a given range, the chance of it being a specific value is 0. Think about it this way - for any value you mention, I can zoom in more. For example, if you ask the probability of x in the above graph being equal to 0, we could zoom in to 0.0, or 0.00, or 0.000. At some limit of resolution, the area under the curve (which denotes the probability and would be found using integral calculus, which we won’t do here) would be equal to 0!\nThis may seem like an odd aside, but it is actually very important. It explains why when we will discuss probabilities the probability of an outcome being less than, more than, or between two values in upcoming chapters. For example, we can note (again) that for a normal distribution (what we see above and will (still eventually) define more appropriately) that 67% of the data falls within one standard devation for perfectly (very rare!) normally-distributed data.\n\n#function from https://dkmathstats.com/plotting-normal-distributions-in-r-using-ggplot2/\ndnorm_one_sd &lt;- function(x){\n  norm_one_sd &lt;- dnorm(x)\n  # Have NA values outside interval x in [-1, 1]:\n  norm_one_sd[x &lt;= -1 | x &gt;= 1] &lt;- NA\n  return(norm_one_sd)\n}\n\nggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = \"orange\")+\n  labs(y=\"Probability\", x=\"Outcome\") + \n  stat_function(fun = dnorm_one_sd, geom = \"area\", fill = \"purple\")\n\n\n\n\n\nWhat if more than one thing is of importance?\nSometimes we focus on the probability of more than one outcome for a given event. This requires adding or combining probabilities. The first step in doing this is deciding if the outcomes are mutually exclusive. This means they can not occur in the same unit of focus. For example, we could ask the probability of rolling a 1 or a 6, or of being &gt;2 and &lt;-2. In both cases, a single outcome can’t be both of these things, so the outcomes are mutually exclusive. When this is the case, we simply add the probabilities. This is sometimes called the union of two outcomes.\nContrast this with when we want to know the probability of two things occurring that may occur in the same unit. For example, assume our die not only had dots on it, but these dots were a different color. For example, odd numbers were blue and even numbers were red.\n\ncolors &lt;- c(\"blue\" = \"blue\", \"red\" = \"red\")\ndie_roll$Color &lt;- NA\ndie_roll[as.numeric(as.character(die_roll$Number)) %% 2 == 0, \"Color\"] &lt;- \"blue\"\ndie_roll[as.numeric(as.character(die_roll$Number)) %% 2 != 0, \"Color\"] &lt;- \"red\"\nggplot(die_roll, aes(x=Number, y= Probability, fill=Color)) +\n  geom_col() +\n  scale_fill_manual(values = colors)\n\n\n\n\nNow, the probability of rolling any given group of numbers can be found by adding probabilities since the outcomes are mutually exclusive. Same for die color. However, what about the probability of rolling a blue (even) outcome or a 6? Note we can’t simply add these. Why not?\nBecause a single roll can result in a 6 and blue dots! So adding the probability of getting blue dots (.5) and of getting a 6 (.167) will double-count the 6. In other words, there is an an intersection of the possible outcomes. So, the probability of rolling a 6 or blue is equal to\n\\[\nP[roll=blue] + P[roll=6] - P[roll=6 \\ and \\ blue]= \\frac{1}{2} + \\frac{1}{6} -\\frac{1}{6}\n\\]\nThis is sometimes called the general addition principle.\nIf we are measuring multiple outcomes (note this slightly different than the probability of two or more outcomes for a specific event), we need to consider if the events are independent. This means the outcome of one does not influence the outcome of the other. If this is true, the probability of both events occurring can be found by simply multiplying the probability of each (the multiplication rule). For example, consider the probability of flipping a coin twice and seeing a heads followed by a tails. We can write out all the options (T, HH, TH, TT); assuming independence, the probability for each is \\(\\frac{1}{4}\\). We can also say the probability of heads on the first flip is \\(\\frac{1}{2}\\) and the probability of tails on the second flip is \\(\\frac{1}{2}\\); multiplying these yields \\(\\frac{1}{2}\\).\nConsider instead that we roll 2 dice and measure the sum of the rolls. Since one roll does not influence the other, these are independent events. As noted above, we can work out the probability distribution by writing out all possible outcomes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*Fi\n\nrst\nDic\ne\n**\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n* Sec\n\nond\nDic\ne\n**\n1\n( 1 ,1)\n( 1 ,2)\n( 1 ,3)\n\n\n*(1\n,\n4\n\n)\n**\n( 1 ,5)\n( 1 ,6)\n\n\n\n2\n( 2 ,1)\n( 2 ,2)\n\n\n*(2\n,\n3\n\n)\n**\n( 2 ,4)\n( 2 ,5)\n( 2 ,6)\n\n\n\n3\n( 3 ,1)\n\n\n*(3\n,\n2\n\n)\n**\n( 3 ,3)\n( 3 ,4)\n( 3 ,5)\n( 3 ,6)\n\n\n\n4\n\n\n*(4\n,\n1\n\n)\n**\n( 4 ,2)\n( 4 ,3)\n( 4 ,4)\n( 4 ,5)\n( 4 ,6)\n\n\n\n5\n( 5 ,1)\n( 5 ,2)\n( 5 ,3)\n( 5 ,4)\n( 5 ,5)\n( 5 ,6)\n\n\n\n6\n( 6 ,1)\n( 6 ,2)\n( 6 ,3)\n( 6 ,4)\n( 6 ,5)\n( 6 ,6)\n\n\n\nNow assume we want to know the probability of the sum of the dice being equal to 5. If we assume independence, the the probability of rolling a sum of 5 (highlighted cells above) is \\(\\frac{4}{36}\\).\nWe could also simulate the outcome\n\nlibrary(reshape2)\nnumber_of_rolls &lt;- 100000\nsum_of_rolls &lt;- data.frame(index = 1:number_of_rolls, Sum = NA)\nfor (i in 1:number_of_rolls){\n  dice_roll_trial &lt;- sample(1:6, size = 2, replace = TRUE)\n  sum_of_rolls$Sum[i] &lt;- sum(dice_roll_trial)\n}\nsum_of_rolls_df &lt;- dcast(sum_of_rolls, Sum ~ \"Probability\", length )\n\nUsing Sum as value column: use value.var to override.\n\nsum_of_rolls_df$Probability &lt;- sum_of_rolls_df$Probability/number_of_rolls\nggplot(sum_of_rolls_df, aes(x=Sum, y=Probability)) +\n  geom_col(fill=\"orange\", color=\"black\") +\n  labs(y=\"Probability\")+\n    scale_x_continuous(breaks = c(2:12))\n\n\n\n\nNotice here we find the probability of rolling a sum of 5 is 0.10995 which is very close to \\(\\frac{4}{36}\\).\nTo find the probability using math, we have to note that the dice rolls are independent. For example, even though we only want a 4 on the second dice if we roll a 1 on the first dice, the roll of the first die does not influence the roll of the second. We should also note the desired outcomes are mutually exclusive. So we can find the probability of each happening and then add them. It’s easy to see how probability can get complicated!"
  },
  {
    "objectID": "content/Probability.html#conditional-probability",
    "href": "content/Probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nUnlike our coin example, sometimes a first event occurring does influence the probability of a second event.\n\n\n\nXKCD Conditional Risk: The annual death rate among people who know that statistics is one is six.\n\n\nIn a similar vein, although the risk of shark attack is low, it increases dramatically if you swim in the ocean.\nUnlike our previous examples, we now have 2 events (lets call them A and B), and the probability of both occurring is equal to\n\\[\nP[A \\ and \\ B] = P[A] \\ P[B|A]\n\\]\nwhich can be read as “the probability of A and B occurring is equal to the probability of A multiplied by the probability of B given A occurs”. Note if A and B are independent, this reduces to the multiplication rule.\nWe can extend this by noting\n\\[\nP[A \\ and \\ B] = P[A] \\ P[B|A] \\\\\nP[A \\ and \\ B] = P[B] \\ P[A|B] \\\\\nP[A] \\ P[B|A] = P[B] \\ P[A|B] \\\\\nP[A|B] = \\frac{P[B|A]*P[A]}{P[B]}\n\\]\nThis rule is known as Bayes’ Theorem. We will return to this when we discuss Bayesian analysis, but we can use it here for demonstration.\nFor our lightning example, we could use some (pretend) numbers to understand the risk and Bayes’ Theorem. First, let A be the probability of being outside in a lightning storm. B is then the probability of getting struck by lightning, , and P[B|A] is the probabiity of getting struck by lightning given that you are outside in a lightning storm (hint: it’s much higher than the P[B]).\nHere’s anoher similar (in concept) example. Medical trials are designed to test the effectiveness of drugs or treatments. In these trials, drug efficacy is considered by comparing outcomes in people who receive the drug or treatment compared to those who receive a placebo (such as a sugar pill). Note this only works if participants do not know which group (drug vs placebo) they are in (why?). In a given trial, people who receive the drug recover 60% of the time (or avoid some other adverse outcome). This may seem good, but it’s only relevant when compared to the placebo group. What if people receiving the placebo recovered 80% of the time? Also, if we know the probability of recovering without the drug, we can consider the total probability of recovery. For now, let’s assume that 20% of people who receive the placebo recover.\nWe could use a tree diagram to consider possible options:\n\nlibrary(DiagrammeR)\n\nWarning: package 'DiagrammeR' was built under R version 4.2.3\n\nbayes_probability_tree &lt;- function(prior, true_positive, true_negative, label1 = \"Prior\", \n                                   label2 = \"Complimentary Prior\", label3 = \"True Positive\",\n                                   label4 = \"False Negative\", label5 = \"False Positive\",\n                                   label6 = \"True Negative\") {\n  \n  if (!all(c(prior, true_positive, true_negative) &gt; 0) && !all(c(prior, true_positive, true_negative) &lt; 1)) {\n    stop(\"probabilities must be greater than 0 and less than 1.\",\n         call. = FALSE)\n  }\n  c_prior &lt;- 1 - prior\n  c_tp &lt;- 1 - true_positive\n  c_tn &lt;- 1 - true_negative\n  \n  round4 &lt;- purrr::partial(round, digits = 5)\n  \n  b1 &lt;- round4(prior * true_positive)\n  b2 &lt;- round4(prior * c_tp)\n  b3 &lt;- round4(c_prior * c_tn)\n  b4 &lt;- round4(c_prior * true_negative)\n  \n  bp &lt;-  round4(b1/(b1 + b3))\n  \n  labs &lt;- c(\"X\", prior, c_prior, true_positive, c_tp, true_negative, c_tn, b1, b2, b4, b3)\n  \n  tree &lt;-\n    create_graph() %&gt;%\n    add_n_nodes(\n      n = 11,\n      type = \"path\",\n      label = labs,\n      node_aes = node_aes(\n        shape = \"circle\",\n        height = 1,\n        width = 1,\n        x = c(0, 3, 3, 6, 6, 6, 6, 8, 8, 8, 8),\n        y = c(0, 2, -2, 3, 1, -3, -1, 3, 1, -3, -1))) %&gt;% \n    add_edge(\n      from = 1,\n      to = 2,\n      edge_aes = edge_aes(\n        label = label1\n      )\n    ) %&gt;% \n    add_edge(\n      from = 1, \n      to = 3,\n      edge_aes = edge_aes(\n        label = label2\n      )\n    ) %&gt;% \n    add_edge(\n      from = 2,\n      to = 4,\n      edge_aes = edge_aes(\n        label = label3\n      )\n    ) %&gt;% \n    add_edge(\n      from = 2,\n      to = 5,\n      edge_aes = edge_aes(\n        label = label4\n      )\n    ) %&gt;% \n    add_edge(\n      from = 3,\n      to = 7,\n      edge_aes = edge_aes(\n        label = label5\n      )\n    ) %&gt;% \n    add_edge(\n      from = 3,\n      to = 6,\n      edge_aes = edge_aes(\n        label = label6\n      )\n    ) %&gt;% \n    add_edge(\n      from = 4,\n      to = 8,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 5,\n      to = 9,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 7,\n      to = 11,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) %&gt;% \n    add_edge(\n      from = 6,\n      to = 10,\n      edge_aes = edge_aes(\n        label = \"=\"\n      )\n    ) \n  message(glue::glue(\"The probability of having {label1} after testing {label3} is {bp}\"))\n  print(render_graph(tree))\n  invisible(tree)\n}\n\n#first example\nbayes_probability_tree(prior = 0.5, true_positive = 0.8, true_negative = 0.8, label1 = \"medicine\", label2 = \"placebo\",\n                       label3 = \"cured\", label4 = \"not cured\",\n                       label5 = \"cured\", label6 = \"not cured\")\n\nThe probability of having medicine after testing cured is 0.8\n\n\nThis tree let’s us consider multiple things. We can see the probability of being cured is .5, but 80% of those come from the group receiving medicine (again, the P[being cured|given medicine]). WE could also derive this using Bayes Theorem.\n$$ P[being  cured|medicine] = \\ P[being  cured|medicine] = \\ P[being  cured|medicine] = .8\n$$\nOne more example. Instead of medical trials, let’s focus on medical screenings. These are used to identify patients who have a condition, but there are no perfect tests. A test may give a false positive, meaning it says a condition exists when it does not. A test can also give a false negative, meaning it finds a condition does not exist when it really does. Both of these present issues for patients and explain why series of tests are often used before more invasive procedures are employed.\nFor example, assume a procedure is used to assess skin cancer. This cancer occurs at a frequency of .00021 in the general population. The test is fairly accurate; if a patient has cancer, the screening will correctly identify if 95% of the time. However, the probability of a false positive is .01. Given these numbers, how worried should a person be about a positive test?\nAlthough the test seems to be good, note the prevalence of a false positive is far higher than the prevalence of cancer! This means most positives will likely be false. To quantify this, let A be the probability of cancer and B be the probability of a positive screening. So,\n\\[\nP[A|B] = \\frac{.95 \\ * .00021}{.95 \\ * \\ .0021+.01*.9779} \\\\\nP[A|B] = .169\n\\]\nIn other words, only 17% of people with positive screenings actually have cancer.\nTo show this in a probability tree:\n\nbayes_probability_tree(prior = 0.0021, true_positive = 0.95, true_negative = 0.99, label1 = \"cancer\", \n                       label2 = \"not cancer\",\n                       label3 = \"positive\", \n                       label4 = \"negative\",\n                       label5 = \"positive\", \n                       label6 = \"negative\")\n\nThe probability of having cancer after testing positive is 0.16625"
  },
  {
    "objectID": "content/Probability.html#related-ideas",
    "href": "content/Probability.html#related-ideas",
    "title": "Probability",
    "section": "Related Ideas",
    "text": "Related Ideas\nFalse results are integral to the ideas of test specificity and sensitivity Lalkhen and McCluskey (2008). A specific test will yield few false negatives, while a sensitive test will yield few false positives. Put another way (from Lalken & McCluskey (2008): “A test with 100% sensitivity correctly identifies all patients with the disease”, and ” a test with 100% specificity correctly identifies all patients without the disease”. True and false results also impact predictive values.\n\n\n\n\n\n\n\n\n\nCondition\n\n\n\n\n\n\nTest Outcome\nPresent\nAbsent\n\n\nPositive\nTrue Positive\nFalse positive\n\n\nNegative\nFalse negative\nTrue Negative\n\n\n\nSensitivity=\nTrue Positive/ Condition present\nSpecificity =\nTrue negative/\nCondition negative\n\n\n\nThese ideas can also be related to power. We will return to this visualization in future chapters.\n\n\n\nA 3D visualisaion of PPV, NPV, Sensitivity and Specificity. Luigi Albert Maria, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons"
  },
  {
    "objectID": "content/Probability.html#next-steps",
    "href": "content/Probability.html#next-steps",
    "title": "Probability",
    "section": "Next steps",
    "text": "Next steps\nNow that we’ve discussed probability, we can move into the wild world of p-values and discuss how they relate to estimation!"
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html",
    "href": "content/Tests_for_continuous_data_from_one_sample.html",
    "title": "One sample tests for continuous data",
    "section": "",
    "text": "In this chapter will build on our introduction to significance testing by considering tests for continuous data collected on one trait from a single population. This will also allow/require us to more fully define normal distributions, which we have already started to discuss."
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#example",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#example",
    "title": "One sample tests for continuous data",
    "section": "Example",
    "text": "Example\nLet’s return to our iris data and focus on sepal lengths of I. viriginica.\n\nset.seed(42)\nlibrary(ggplot2)\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram( fill=\"blue\", color=\"black\") +\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWhat if we wanted to test if the height was equal to a certain value like 7 cm?\nWe can’t, and it’s not.\nHeight is a random variable. It differs among individuals (see above), so it isn’t equal to any specific value. This may seem obvious, but it’s an important step in understanding hypothesis testing. Many students also struggle with this, but most are with the fact we learned about hypothesis testing focusing on proportions (remember the last chapter?). When we focused on binomial data, it was obvious a single draw could not be 2 things - it was a success or failure, and we focused on the relative occurrence of those.\nSimilarly, for continuous numeric data (remember: data that can on any value in a given range), we need to focus on describing the distribution of the data. If we do that, we might want (and be able to test) if, for example, the mean height of I. virginica is equal to 7 cm. In fact, we typically focus on the mean of the distribution (one of measures of central tendency)\nTo do this, we need to do what we did with binomial data: develop a null hypotheses, use it to construct a null distribution, and compare our data to it see how unusual it is (and get a p-value).\n\n\nWhat is our null hypothesis for this example?\n\nFor this example, we are focused on a two-tailed test (we are asking if the mean is equal to a certain value), so we have\n\\[\n\\begin{split}\nH_O: \\mu_{sepal \\ length} = 7 \\ cm \\\\\nH_A: \\mu_{sepal \\ length} \\neq 7 \\ cm\n\\end{split}\n\\]\n\nNow that we have a null hypothesis, we need to test it. We can do this by simulation. Let’s make a distribution where the \\(\\mu_{sepal \\ length} =7 \\  cm\\), then draw samples from it and see how rare it is to get what we actually observed in the data…which was\n\nmean(iris[iris$Species == \"virginica\",\"Sepal.Length\"])\n\n[1] 6.588\n\n\nSeems easy enough, but what distribution do we draw from? For our binomial data we knew exactly what to parameterize - that’s because the entire distribution that fits the null hypothesis is described by the parameter p that is set by the null hypothesis (go back to the last chapter and note we can find the spread using this one variable as well!).\nIf you remember the central limit theorem, you might realize the distribution of the data does not matter in some ways. No matter what it looks like, the means of the data will tend towards normality. However, we still need to describe the data itself it to simulate our draws. We’ll discuss an alternative where you sample from the data itself at the end of this chapter.\nIt turns out the shape of the data itself appears fairly normal. So far we’ve said normal distributions\n\nare roughly symmetric, with tails on either side. Values near the middle of the range are more common, with the chance of getting smaller or larger values declining at an increasing rate\n\n95% of the sample is within \\~2 standard deviations of the mean (and for our mean of means, 95% of the data is within 2 standard errors)\nLook at a density function fit to the data:\n\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram(aes(y = ..density..),fill=\"blue\", color=\"black\") +\n  geom_density()+\n  labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nIt does appear to be fairly symmetric and peaked in the middle. It turns out normal distributions (finally defined below!) are very common in nature.\nJust like the binomial data, a normal distribution can be described using a formula. The formula has 2 parameters that define the shape of the distribution. \\(\\mu\\) defines the center of the distribution, and \\(\\sigma^2\\) describes its spread. The formula for the probabilty density function is\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\ne^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\n\\]\nThis looks complicated, but remember: this is just an equation for describing the likelihood of outcomes in a probability space! The first part \\(\\sqrt{2\\pi\\sigma^2}\\) arises from trying to work with a curve. To understand the rest, lets take the ln of both sides\n\\[\n\\begin{split}\n\\ln(f(x)) = \\ln(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) - \\frac{1}{2\\sigma^2}*(x-\\mu)^2 \\\\\n\\ln(f(x)) = -\\ln(\\sqrt{2\\pi\\sigma^2}) - \\frac{1}{2\\sigma^2}*(x-\\mu)^2\n\\end{split}\n\\]\nThis may not look like it helps much, but we now have the formula for a straight line, \\(y=mx+b\\), where\n\\[\n\\begin{split}\ny= \\ln(f(x)) \\\\\nb = - \\frac{1}{2\\sigma^2}\\\\\nm = -\\ln(\\sqrt{2\\pi\\sigma^2})\\\\\nx =  (x-\\mu)^2\n\\end{split}\n\\]\nIn other words, our independent variable is the squared distance from the mean (so all positive)! Note both the y-intercept (the amplitude) and slope (shape) depends on how spread out the data is (\\(\\sigma^2\\)). Note in general when ln(y) decreases linearly with x, y decreases at a constant proportional rate with x. So we can say a normal random variate is any random variable in which the probability of an observation declines in proportion to its squared deviation from the mean (µ).\nLet’s fit a normal distribution to our data:\n\ncolors &lt;- c(\"PDF from data\" = \"black\", \"normal curve\" = \"red\")\nggplot(iris[iris$Species == \"virginica\",],\n              aes(x=Sepal.Length)) +\n  geom_histogram(aes(y = ..density..),fill=\"blue\", color=\"black\") +\n  geom_density(aes(color=\"PDF from data\"))+\n    labs(title=expression(paste(\"Sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Sepal length (cm)\",\n       y= \"Density\",\n       color=\"Source\" ) +\nstat_function(fun = dnorm, args = list(mean = mean(iris[iris$Species == \"virginica\",\"Sepal.Length\"]), sd = sd(iris[iris$Species == \"virginica\",\"Sepal.Length\"])), aes(color=\"normal curve\"))+\n      scale_color_manual(values = colors)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote it fits fairly well, so we’ll use it for our sampling experiment. To do so, we’ll take 50 draws (since we had a sample size of 50) from a normal distribution, find means for each draw, then consider their distribution.\n\nnumber_of_draws &lt;- 50\nnumber_of_simulations &lt;- 1000\n\nsampling_experiment&lt;- data.frame(\"observed_mean\" = rep(NA, number_of_simulations))\nfor(i in 1:number_of_simulations){\nsampling_experiment$observed_mean[i] = mean(rnorm(50, 7, sd = sd(iris[iris$Species == \"virginica\",\"Sepal.Length\"])))\n}\n\nLet’s check out the first few outcomes\n\nhead(sampling_experiment)\n\n  observed_mean\n1      6.977317\n2      7.064034\n3      6.903823\n4      6.984919\n5      7.005049\n6      6.981765\n\n\nand plot them\n\nggplot(sampling_experiment,\n              aes(x=observed_mean)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed means from 1000 random draws\",\n       x= \"Mean\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow let’s see how that compares to what we actually saw.\n\nsampling_experiment$compare =   ifelse(abs(sampling_experiment$observed_mean-7) &gt;= abs(mean(iris[iris$Species == \"virginica\",\"Sepal.Length\"])-7), 'as or more extreme', 'not as or more extreme')\n\nsampling_experiment$compare &lt;- factor(sampling_experiment$compare)\nlevels(sampling_experiment$compare) &lt;- c(levels(sampling_experiment$compare), \"as or more extreme\")\n\nggplot(sampling_experiment,\n              aes(x=observed_mean, fill=compare)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed means from 1000 random draws\",\n       x= \"Mean\",\n       y= \"Frequency\", \n       fill = \"Sampled mean is ...\") +\n    scale_fill_discrete(drop = FALSE)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSo in our example simulation, no observed means were as far from the value from the null hypothesis as our sample mean was. This would leave to a p-value of 0 - unusual in some regards, but possible here."
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#another-way-to-compare",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#another-way-to-compare",
    "title": "One sample tests for continuous data",
    "section": "Another way to compare",
    "text": "Another way to compare\nJust like we saw for binomial data, we can always carry out a sampling experiment to find a p-value. However, that’s mainly because we have computers. Even with computers, it would be cumbersome to set up a new sampling experiment for every dataset (which we would need to do for any change in sample size, mean, or standard deviation).\nInstead it would be nice to find a way to replicate the distribution we see above for similar experiments using an equation (just like we did for the binomial data!). One step in doing this relates to how we examine our predictions given our hypotheses. It turns out, we can map our hypotheses to models that explain the variation we see in the data. Our hypotheses (remember\n\\[\n\\begin{split}\nH_O: \\mu_{sepal \\ length} = 7 \\ cm \\\\\nH_A: \\mu_{sepal \\ length} \\neq 7 \\ cm\n\\end{split}\n\\]\nfocus on the mean. They are also stating a prediction for every outcome! Under the null hypothesis, if we were asked to guess the length of a sepal from I. virginica, we would guess 7 cm. Under the alternative hypothesis, we would guess it’s something different than 7. Using our sample, we might instead guess it’s equal to 7 + \\(\\delta\\), where \\(\\delta\\) is estimated from our sample and 7+ \\(\\delta\\) is the mean of our sample.\nFor each hypothesis, we could calculate a measure of related mode fit called the sum squared error from our model, or SSE, where\n\\[\nSSE=sum \\ squared \\ error = \\sum_{i=1}^{n}(Y_i-\\hat{Y_i})^2\n\\]\nHere, \\(Y_i\\) are the data points, and \\(\\hat{Y_i}\\) is our predicted value.\nThis is basically saying we compare the predicted to observed value for each data point for each model (the square exists so things don’t cancel out!). Our null hypothesis corresponds to a simpler view of the world (a reduced or null model), where \\(\\hat{Y_i}\\) is equal to a given value (in our case, 7 cm), whereas under the alternative hypothesis (which corresponds to an alternative or full model), \\(\\hat{Y_i}\\) is equal to a different value (such as 7 + \\(\\delta\\)). The value for \\(\\delta\\) is estimated from our sample and makes the full model larger than the reduced in regard to the number of parameters included in the model.\nWe can then compare the SSE of the 2 models by finding their difference. This is our signal from the data. If we take multiple samples from a known population that is defined by the null hypothesis, we can carry out a very similar sampling to what we did originally.\n\nfor(i in 1:number_of_simulations){\na &lt;- rnorm(50, 7, sd = sd(iris[iris$Species == \"virginica\",\"Sepal.Length\"]))\nsampling_experiment$observed_mean[i] &lt;- mean(a)\n\nsampling_experiment$SSE_null[i] &lt;-  sum((a-7)^2)\n\nsampling_experiment$SSE_full[i] &lt;- sum((a-mean(a))^2)\n}\n\nsampling_experiment$SSE_difference &lt;- sampling_experiment$SSE_null - sampling_experiment$SSE_full\n\nggplot(sampling_experiment,\n              aes(x=SSE_difference)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed difference in model fit from 1000 draws\",\n       x= \"Mean\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ndifference_SSE_observed &lt;- sum((iris[iris$Species == \"virginica\",\"Sepal.Length\"]-7)^2)-sum((iris[iris$Species == \"virginica\",\"Sepal.Length\"]-mean(iris[iris$Species == \"virginica\",\"Sepal.Length\"]))^2)\n\nggplot(sampling_experiment,\n              aes(x=SSE_difference)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Observed difference in model fit from 1000 draws\",\n       x= \"Mean\",\n       y= \"Frequency\")+\n  geom_vline(aes(xintercept=difference_SSE_observed))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote we never saw as large a difference in the signal our sampling experiment as we did in the data!\n\nnrow(sampling_experiment[sampling_experiment$SSE_difference &gt;=  difference_SSE_observed,])\n\n[1] 0\n\n\nNote this distribution is skewed for a few reasons. Given the square term, it must positive. We also unlikely (given sampling error) that signal will be exactly zero. In fact,since we estimate \\(\\delta\\) from the data itself, the alternative model is almost always a better fit (remember our bias in variance estimates?).\nThis approach accounts for the noise that we see in our data (variation we expect to see in signal values even if the null hypothesis is true) through sampling. However, if we could estimate the noise, we could also divide our signal by it. Accounting for signal and noise let’s us take very different questions in terms of data and use a standardized approach to analyze them. This is because a signal to noise ratio typically follows some distribution."
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#z-test-a-distribution-based-approach",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#z-test-a-distribution-based-approach",
    "title": "One sample tests for continuous data",
    "section": "Z-test: a distribution based approach",
    "text": "Z-test: a distribution based approach\nIn our case, the signal to noise ratio is approximated by the normal distribution! This is an approximate solution to the signal to noise ratio because our means approach a normal distribution as sample size increases - so for non-infinity sample sizes, they may not be perfectly normal!\nWe can show that our signal is simply the mean from our data minus the mean under the null hypothesis, and including \\(\\sigma\\) in the denominator accounts for noise. We can make this more generalizable if we z-transform the data using the formula\n\\[\nz=\\frac{Y - \\mu}{\\sigma}\n\\]\nAfter this transformation, the data is centered at 0 (think about it - if you subtract the mean from all data points…) and has a standard deviation of 1 (because you divided by the standard deviation!). This also makes the mean of transformed data equal to \\(\\delta\\) and the mean under the null hypothesis equal to zero. This also means ~68% of the data points lie between -1 and 1, while ~95% lie between -2 and 2 (since the standard deviation is 1!).\n\n\nLoading required package: viridisLite\n\n\nWarning: package 'viridisLite' was built under R version 4.2.3\n\n\n\n\n\nFigure 1: ~65% of the data lies with 1 standard deviation of the mean, ~95% lies within 2 standard deviations, and ~99% lies within 3 standard deviations of the mean\n\n\n\n\nWe call this specific form of the normal distribution (N(0,1), showing the mean and standard deviation parameters) the Z distribution. Extending on this with some algebra, we can get a Z-score from any data using the equation\n\\[\nz=\\frac{\\bar{x} - \\mu}{\\sigma}\n\\]\nThis transforms a given data point into a z-score on the z, or standard normal, distribution. We can then use the Z distribution to consider how unusual our z-scores are (i.e., get a p-value!). We call the approach that uses this distribution the z-test, and it can be carried out using the z.test function in the BDSA package.\n\nlibrary(BSDA)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'BSDA'\n\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\nz.test(iris[iris$Species == \"virginica\",\"Sepal.Length\"], mu = 7, \n             sigma.x= sd(iris[iris$Species == \"virginica\",\"Sepal.Length\"]))\n\n\n    One-sample z-Test\n\ndata:  iris[iris$Species == \"virginica\", \"Sepal.Length\"]\nz = -4.5815, p-value = 4.617e-06\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.411746 6.764254\nsample estimates:\nmean of x \n    6.588 \n\n\nUsing this approach we get a p-value of .000005 - not 0, but very close!\n\nA little history\nAlthough software now provides us exact (approximate) p-values, historically this was far more difficult. For this reason, people took advantage of a transformation so they could use a standardized table like this one.\n\n\n\nExample of z-table. Jsmura, Creative Commons Attribution-Share Alike 4.0 International licence\n\n\nThese tables showed how Z-scores related to p-values. Note these often showed the area to the left of the value, so two-tailed tests required one to multiple the given p-value by 2 (or, if focused on the upper tail, multiply (1-given p-value) by 2 since the distribution is symmetric. Similarly, some tables only had values &lt;0; for those you could find the score whose absolute value corresponed to the observed z-score and multiply the noted p-value by 2 for two-tailed tests .\nFor our example, we got a z score of -4.8515. The table doesn’t even go that low!"
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#does-the-distribution-of-the-data-matter",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#does-the-distribution-of-the-data-matter",
    "title": "One sample tests for continuous data",
    "section": "Does the distribution of the data matter?",
    "text": "Does the distribution of the data matter?\nRemember we are focusing on the distribution of the means (both our sampling experiment and SSE calculations include the means of the sample and data under the null hyothesis!). Given that and the central limit theorem, does the distribution of the data matter? Yes, but only in regards to the relationship between sample size and normality of the sample means. If the underlying data is normal, then the sampled means are distributed normally for almost any sample size, although sample size impacts the spread of the sample means.\n\n\nNo id variables; using all as measure variables\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 2: Means drawn from a normal distribution are normal regardless of sample size\n\n\n\n\nFor other distributions, larger sample sizes are required to approximate normality. For example, consider a highly-peaked (double-exponential) distribution\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nNo id variables; using all as measure variables\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nor as skewed \\(\\chi^2\\) distribution (here with a df =4, to be explained later!):\n\n\nNo id variables; using all as measure variables\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEven for these very non-normal distributions, the means approach a normal distribution at fairly low sample sizes. This is even true for binomial data, especially when p is not very close to 0 or 1. Consider\n\n\nNo id variables; using all as measure variables\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is why it used to be more common to use a normal approximation to the binomial distribution - even though the binomial distribution is easy to compute,the z is even easier! For example, Sandidge (Sandidge 2003) noted that brown recluse spiders chose dead prey items (as opposed to live - 2 categories!) when offered choices. This data could be assessed using a binomial test\n\n binom.test(119,(59+41+41), p=.5)\n\n\n    Exact binomial test\n\ndata:  119 and (59 + 41 + 41)\nnumber of successes = 119, number of trials = 141, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.7733542 0.8995595\nsample estimates:\nprobability of success \n             0.8439716 \n\n\nor a z-test approach by finding a z-sore\n\n p=.5\n n=41+41+59\n z_score &lt;- (119-p*n)/sqrt(n*p*(1-p))\n pnorm(z_score, lower.tail = F) *2\n\n[1] 3.11282e-16\n\n\nor a \\(\\chi^2\\) test, which is similar to a z test but focuses on a sum of independent squared Z variates. The \\(\\chi^2\\) distribution is defined by a degrees of freedom, or df, parameter, which in this case isequal to the number of categories -1 (or 2-1=1 in this case). We will return to df multiple times! Note the p-values obtained from the Z and \\(\\chi^2\\) tests are the same!\n\nchisq.test(c(119,n-119))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(119, n - 119)\nX-squared = 66.73, df = 1, p-value = 3.113e-16\n\n\nThis may seem esoteric, but understanding these issues may help you interpret older papers while also choosing to employ more modern statistical methods.\n\nQQ norm plots - commonly used, but not needed, at this point!\nIn addition to considering the sample size and underlying distribution, quantile-quantile (Q-Q) plots are sometimes used to assess normality. These plots plot quantiles in one data set against quantiles from another to determine if they come from similar distribution. Remember, quantiles just order data; percentiles are example where you have 100 cut points. Q-Q norm plots consider if a given dataset are similar to a normal distribution. If so, then the dots should match the straight line produced by the qqline function.\n\nqqnorm(iris[iris$Species == \"virginica\",\"Sepal.Length\"])\nqqline(iris[iris$Species == \"virginica\",\"Sepal.Length\"])\n\n\n\n\nWhile we introduce Q-Q plots here, and they are often used to assess normality, remember our tests (so far) are relying on the means of the data being normally-distributed and not the data itself!"
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#what-if-we-dont-know-the-variance",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#what-if-we-dont-know-the-variance",
    "title": "One sample tests for continuous data",
    "section": "What if we don’t know the variance?",
    "text": "What if we don’t know the variance?\nAssuming everything above makes sense, we are left with one issue: the variance of the underlying population is rarely known!\nIn the above examples, we actually used our estimate of variance from the sample to run our simulation experiment and z-test! While this works ok for large sample sizes (yay for central limit theorem!) and is what statisticians relied upon historically, it doesn’t work for well for smaller sample sizes (unless we somehow know the population variance). Our estimates for the population variance are less precise and potentially biased at small sample sizes.\nTo address this issue, statisticians developed the t-distribution. Unlike the normal distribution, its shape depends on the sample size. This parameter is coded as degrees of freedom, commonly denoted as df, and is equal to n - 1 (we’ll come back to df later!). The major breakthrough, however, was that df was the only sample-specific parameter. The same distribution works regardless of the estimated population variance, as a t statistic/score is created that functions like a z score.\n\\[\nt=\\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\n\\]\nIt can be shown (though not here!) that the t-distribution is actually a specific form of the F distribution (which we’ll see in ANOVAs). An F-distribution is the ration of two \\(\\chi^2\\) distributions, which (as noted above) are sums of squared Z distributions. The t distribution is the special case where you can take the square of an F distribution where the numerator (top) \\(\\chi^2\\) distribution has 1 degree of freedom and the denomintor (bottom) has n-1 degrees of freedom. In general, variance follows a \\(\\chi^2\\) distribution with n-1 degrees of freedom.\nBecause it directly uses the estimate of the population variance, smaller sample sizes show more spread (and thus make null hypotheses more difficult to reject!). For example, note how small sample sizes (remember, df=3 means n=4!) are notably different from the normal distribution, while larger sample sizes become very hard to distinguish!\n\n\n\n\n\nNote this means the t-distribution is actually a class of distributions.\nIn older text books, you would have a t-table that showed t scores corresponding to commonly used values of \\(\\alpha\\) for multiple degrees of freedom. These published t scores are sometimes called critical values. Users would have compared their calculated t-score (or its absolute value in the case of negative values) to the appropriate critical values (often \\(\\alpha/2\\) for 2-sided tests) to determine if a finding was significant. We can actually produce a t table in R.\n\ndf &lt;- 1:100\ncut_offs &lt;- c(\".1\", \".05\", \".025\", \".01\")\nt_table &lt;- setNames(data.frame(matrix(ncol = length(cut_offs)+1, nrow = length(df))), c(\"df\",cut_offs))\nt_table$df &lt;- df\nfor (i in 1:length(df)){\n  for (j in 2:ncol(t_table))\n  t_table[i,j] &lt;- round(abs(qt(as.numeric(colnames(t_table)[j]), df[i])),3)\n}\nlibrary(rmarkdown)\npaged_table(t_table)\n\n\n\n\n\n\nFigure 3: Critical values\n\n\n\nTo calculate the test statistic in R, we can instead (thankfully!) use the t.test function.\n\nt.test(iris[iris$Species == \"virginica\",\"Sepal.Length\"], mu = 7, \n             sigma.x= sd(iris[iris$Species == \"virginica\",\"Sepal.Length\"]))\n\n\n    One Sample t-test\n\ndata:  iris[iris$Species == \"virginica\", \"Sepal.Length\"]\nt = -4.5815, df = 49, p-value = 3.195e-05\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588 \n\n\nNote the t-test also provides confidence intervals. Note these consider the spread of the data given the t-distribution so they are always wider than those predicted using a normal distribution, though the difference is small at large sample sizes.\nThis explains why I have noted ~95% of the data lies within 2 standard errors of the mean! For truly normal data, it’s actually within 1.96 standard errors. For data where we estimate the population variance, it depend on the sample size, but even at n=21 (and thus df = 20!) the number is 2.09. Note\n\nt_critical &lt;- setNames(data.frame(matrix(ncol = 2, nrow = length(sample_size))), c(\"n\", \"95% of the data is within this many standard deviations of the mean\"))\nfor (i in 1:length(sample_size)){\nt_critical$n[i] &lt;- sample_size[i]\nt_critical[i,2] &lt;-qt(.025,as.numeric(sample_size[i]))\n}"
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#what-if-we-dont-trust-the-normal-approximation",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#what-if-we-dont-trust-the-normal-approximation",
    "title": "One sample tests for continuous data",
    "section": "What if we don’t trust the normal approximation?",
    "text": "What if we don’t trust the normal approximation?\nDespite the central limit theorem (or because of it), we may not think our sample size is sufficient given the distribution the data to assume that \\(\\hat{Y}\\) really follows a normal distribution. In that case we a few options.\n\nWilcoxon test (aka, signed binary-transform, Fisher’s sign test)\nIf the distribution of the data is symmetric, a Wilcoxon test may be appropriate. Note it is rare to have data that are symmetrically distributed but that for which you don’t think th means will be normally distributed). We introduce the test here as it will come back up (and be more useful) in later chapters.\nThis test employs a strategy we will see again and again: it ranks the data, in this case based on the distance from the mean under the null hypothesis. A sign is also assigned to each rank, with those originating from data points that were lower than the proposed mean becoming negative. In theory, the sum of the signed ranks should be ~0 if HO is true. We can carry out this test in R using the wilcox.test function.\n\nwilcox.test(iris[iris$Species == \"virginica\",\"Sepal.Length\"], mu = 7)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  iris[iris$Species == \"virginica\", \"Sepal.Length\"]\nV = 241.5, p-value = 0.0001312\nalternative hypothesis: true location is not equal to 7\n\n\n\n\nSign test (aka the median test)\nIf the data are not symmetrically distributed, the sign test actually just counts those below the proposed value (which is the median here instead of the mean, since we are concerned about normality). In theory, approximately half the values should be under the proposed mean if HO is true. The proportion below is compared to .5 using a binomial test.\n\nSIGN.test(iris[iris$Species == \"virginica\",\"Sepal.Length\"], md = 7)\n\n\n    One-sample Sign-Test\n\ndata:  iris[iris$Species == \"virginica\", \"Sepal.Length\"]\ns = 12, p-value = 0.0003059\nalternative hypothesis: true median is not equal to 7\n95 percent confidence interval:\n 6.3 6.7\nsample estimates:\nmedian of x \n        6.5 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level L.E.pt U.E.pt\nLower Achieved CI     0.9351    6.3    6.7\nInterpolated CI       0.9500    6.3    6.7\nUpper Achieved CI     0.9672    6.3    6.7\n\n\n\n\nBootstrapping\nThe final option we will review is a little different. For all our other hypothesis tests we’ve been resampling from a distribution that fits the parameters from the null hypothesis. However, its turns out we can resample from the actual data we collected to approximate the distribution of the sample (or the signal in most cases). You can then use that distribution to develop confidence intervals or hypothesis testing. The only requirement here is we have a large enough sample size to actually appropriately sample from (and that we have the means to do it!). This approach was developed in the 1990s given the increase in computing power and availability.\nWe can demonstate this with an imaginary population (and also demonstrate the central limit theorem). Let’s make a population whose trait value of focus falls between 60 and 80 in a uniform manner.\n\npopulation_unif &lt;- data.frame(id = 1:10000, \n                              value = runif(10000, 60, 80))\nggplot(population_unif, aes(x=value)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Our imaginary population\",\n       x= \"Trait value\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow’s let’s take samples of 50 from it a lot of times (1000 here) and plot the means of these samples.\n\nsampling_experiment&lt;- data.frame(\"observed_mean\" = rep(NA, number_of_simulations))\nfor(i in 1:number_of_simulations){\nsampling_experiment$observed_mean[i] &lt;- mean(sample(population_unif$value,50, replace = F))\n}\nggplot(sampling_experiment, aes(x=observed_mean)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Means from imaginary samples of size 50 from our imaginary population\",\n       x= \"Trait value\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow let’s instead think of what might happen - we get a single sample of 50.\n\nactual_sample &lt;- data.frame(sample = sample(population_unif$value,50, replace = F))\nggplot(actual_sample, aes(x=sample)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Actual sample of size 50 from our imaginary population\",\n       x= \"Trait value\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow (here’s the odd part): Let’s sample from that sample with replacement to get “new” samples, then get those means, again, 1000 times.\n\nbootstrap_outcomes &lt;- data.frame(mean = rep(NA, number_of_simulations), sd = NA)\nfor (i in 1:number_of_simulations){\nexample_bootstrap &lt;-sample(actual_sample$sample, length(actual_sample$sample), replace = T)\nbootstrap_outcomes$mean[i] &lt;- mean(example_bootstrap)\nbootstrap_outcomes$sd[i] &lt;- sd(example_bootstrap)\n}\n\nWhen we plot them we see it looks very much like the distribution of means we obtained by re-sampling!\n\nggplot(bootstrap_outcomes, aes(x=mean)) +\n  geom_histogram(color=\"black\") +\n  labs(title=\"Bootstrapped means of size 50 from our imaginary population\",\n       x= \"Trait value\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOverall, this means we can use our sample to recreate our underlying distribution and explore its properties!\nLet’s demonstrate this approach with our iris data. First, we can make “new” datasets from our original data by sampling (with replacement) samples of the same size from our original dataset.\n\nbootstrap_data&lt;- iris[iris$Species == \"virginica\",\"Sepal.Length\"]\nbootstrap_outcomes &lt;- data.frame(mean = rep(NA, number_of_simulations), sd = NA)\nfor (i in 1:number_of_simulations){\niris_bootstrap &lt;-sample(bootstrap_data, length(bootstrap_data), replace = T)\nbootstrap_outcomes$mean[i] &lt;- mean(iris_bootstrap)\nbootstrap_outcomes$sd[i] &lt;- sd(iris_bootstrap)\n}\nggplot(bootstrap_outcomes, aes(x=mean)) +\n  geom_histogram(color=\"black\") +\n  labs(title=expression(paste(\"Bootstrapped means of sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Mean sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe again (thanks to the central limit theorem) see the means follow a normal distribution.\nWe can also carry this out using the boot function (in the boot package), though the functions may look a little odd.\n\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    logit, simplex\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\nresults &lt;- boot(data=bootstrap_data, statistic = function(x, inds) mean(x[inds]),\n   R=number_of_simulations)\nggplot(data.frame(results$t), aes(x=results.t)) +\n  geom_histogram(color=\"black\") +\n  labs(title=expression(paste(\"Bootstrapped means of sepal lengths of \",italic(\"I. virginica\"))),\n       x= \"Mean sepal length (cm)\",\n       y= \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nand then find the 95% confidence interval. Like binomial data, there are a few ways to do this. One is to use the percentile, or quantile, method. This is intuitive. We rank the bootstrapped values from smallest to largest and then find points that cut off the bottom and top 2.5%.\n\nquantile( results$t, probs=c(.025, .975) ) \n\n  2.5%  97.5% \n6.4219 6.7781 \n\n\nThough simple, these findings may also be biased. More advanced intervals are provided the boot.ci function.\n\nboot.ci(results)\n\nWarning in boot.ci(results): bootstrap variances needed for studentized\nintervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 6.410,  6.761 )   ( 6.394,  6.758 )  \n\nLevel     Percentile            BCa          \n95%   ( 6.418,  6.782 )   ( 6.416,  6.778 )  \nCalculations and Intervals on Original Scale\n\n\nThe boot.t.test function in the MKinfer package offers another way to calculate bootstrap statistics for single-sample continuous data. It returns the percentile confidence intervals and also offers a p value.\n\nlibrary(MKinfer)\n\nWarning: package 'MKinfer' was built under R version 4.2.3\n\nboot.t.test(iris[iris$Species == \"virginica\",\"Sepal.Length\"], mu = 7)\n\n\n    Bootstrap One Sample t-test\n\ndata:  iris[iris$Species == \"virginica\", \"Sepal.Length\"]\nbootstrap p-value &lt; 2.2e-16 \nbootstrap mean of x (SE) = 6.589584 (0.08854959) \n95 percent bootstrap percentile confidence interval:\n 6.418 6.764\n\nResults without bootstrap:\nt = -4.5815, df = 49, p-value = 3.195e-05\nalternative hypothesis: true mean is not equal to 7\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588"
  },
  {
    "objectID": "content/Tests_for_continuous_data_from_one_sample.html#next-steps",
    "href": "content/Tests_for_continuous_data_from_one_sample.html#next-steps",
    "title": "One sample tests for continuous data",
    "section": "Next steps",
    "text": "Next steps\nNow that we’ve covered dealing with categorical and continuous data, we will move to comparing populations to each other."
  }
]